\documentclass[10pt,a4paper]{article}
\input{../../../../../../latex-templates/gtu-solutions/preamble.tex}
\input{../../../../../../latex-templates/gtu-solutions/english-boxes.tex}

\begin{document}

\begin{center}
{\Huge\bfseries\color{headcolor} Subject Name Solutions}\\[5pt]
{\LARGE 4341603 -- Winter 2024}\\[3pt]
{\large Semester 1 Study Material}\\[3pt]
{\normalsize\textit{Detailed Solutions and Explanations}}
\end{center}

\vspace{10pt}

\subsection*{Question 1(a) [3 marks]}\label{q1a}

\textbf{Describe human learning in brief.}

\begin{solutionbox}

Human learning is the process by which humans acquire knowledge, skills,
and behaviors through experience, practice, and instruction.


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Human Learning Process}
\vspace{-10pt}
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Aspect & Description \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Observation} & Gathering information from environment \\
\textbf{Experience} & Learning through trial and error \\
\textbf{Practice} & Repetition to improve skills \\
\textbf{Memory} & Storing and retrieving information \\
\end{longtable}
}

\begin{itemize}
\tightlist
\item
  \textbf{Learning Types}: Visual, auditory, kinesthetic learning styles
\item
  \textbf{Feedback Loop}: Humans learn from mistakes and successes
\item
  \textbf{Adaptation}: Ability to apply knowledge to new situations
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``OEPMA'' - Observe, Experience, Practice, Memory,
Adapt

\end{mnemonicbox}
\subsection*{Question 1(b) [4 marks]}\label{q1b}

\textbf{Differentiate: Supervised Learning v/s Unsupervised Learning}

\begin{solutionbox}

\textbf{Comparison Table: Supervised vs Unsupervised Learning}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2157}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3922}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3922}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Parameter
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Supervised Learning
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Unsupervised Learning
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Training Data} & Labeled data (input-output pairs) & Unlabeled
data (only inputs) \\
\textbf{Goal} & Predict output for new inputs & Find hidden patterns \\
\textbf{Examples} & Classification, Regression & Clustering,
Association \\
\textbf{Feedback} & Direct feedback available & No direct feedback \\
\end{longtable}
}

\begin{itemize}
\tightlist
\item
  \textbf{Supervised}: Teacher guides learning with correct answers
\item
  \textbf{Unsupervised}: Self-discovery of patterns without guidance
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``SL-Labels, UL-Unknown'' patterns

\end{mnemonicbox}
\subsection*{Question 1(c) [7 marks]}\label{q1c}

\textbf{List out machine learning activities. Explain each in detail.}

\begin{solutionbox}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Machine Learning Activities}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3125}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2812}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4062}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Activity
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Data Collection} & Gather raw data & Collecting relevant data
from various sources \\
\textbf{Data Preprocessing} & Clean and prepare data & Handling missing
values, normalization \\
\textbf{Feature Selection} & Choose important features & Selecting
relevant attributes for learning \\
\textbf{Model Training} & Build learning model & Training algorithm on
prepared dataset \\
\textbf{Model Evaluation} & Assess performance & Testing model accuracy
and effectiveness \\
\textbf{Model Deployment} & Put model to use & Implementing model in
real-world applications \\
\end{longtable}
}

\begin{verbatim}
flowchart LR
    A[Data Collection] {-{-} B[Data Preprocessing]}
    B {-{-} C[Feature Selection]}
    C {-{-} D[Model Training]}
    D {-{-} E[Model Evaluation]}
    E {-{-} F[Model Deployment]}
    F {-{-} G[Model Monitoring]}
\end{verbatim}

\begin{itemize}
\tightlist
\item
  \textbf{Iterative Process}: Activities repeat for model improvement
\item
  \textbf{Quality Control}: Each step ensures better model performance
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``CPFTEDM'' - Collect, Preprocess, Feature, Train,
Evaluate, Deploy, Monitor

\end{mnemonicbox}
\subsection*{Question 1(c OR) [7
marks]}\label{question-1c-or-7-marks}

\textbf{Find mean, median, and mode for the following data: 1, 1, 1, 2,
4, 5, 5, 6, 6, 7, 7, 7, 7, 8, 9, 10, 11}

\begin{solutionbox}

\textbf{Data Analysis Table}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2683}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2195}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3171}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1951}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Statistic
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formula
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Calculation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Result
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Mean} & Sum/Count & (1+1+1+2+4+5+5+6+6+7+7+7+7+8+9+10+11)/17 &
5.88 \\
\textbf{Median} & Middle value & 7th position in sorted data & 6 \\
\textbf{Mode} & Most frequent & Value appearing 4 times & 7 \\
\end{longtable}
}

\textbf{Step-by-step calculation:}

\begin{itemize}
\tightlist
\item
  \textbf{Count}: 17 values
\item
  \textbf{Sum}: 100
\item
  \textbf{Mean}: 100/17 = 5.88
\item
  \textbf{Median}: Middle position (9th) = 6
\item
  \textbf{Mode}: 7 appears 4 times (highest frequency)
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``MMM'' - Mean=Average, Median=Middle, Mode=Most
frequent

\end{mnemonicbox}
\subsection*{Question 2(a) [3 marks]}\label{q2a}

\textbf{Write down steps to use hold out method for model training.}

\begin{solutionbox}

\textbf{Hold Out Method Steps}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2609}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3478}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3913}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Step
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Action
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{1} & Split dataset (70-80\% training, 20-30\% testing) &
Separate data for training and evaluation \\
\textbf{2} & Train model on training set & Build learning algorithm \\
\textbf{3} & Test model on testing set & Evaluate model performance \\
\end{longtable}
}

\begin{itemize}
\tightlist
\item
  \textbf{Random Split}: Ensure representative distribution in both sets
\item
  \textbf{No Overlap}: Testing data never used in training
\item
  \textbf{Single Split}: One-time division of data
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``STT'' - Split, Train, Test

\end{mnemonicbox}
\subsection*{Question 2(b) [4 marks]}\label{q2b}

\textbf{Explain structure of confusion matrix.}

\begin{solutionbox}

\textbf{Confusion Matrix Structure}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
& Predicted Positive & Predicted Negative \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Actual Positive} & True Positive (TP) & False Negative (FN) \\
\textbf{Actual Negative} & False Positive (FP) & True Negative (TN) \\
\end{longtable}
}

\textbf{Components Explanation:}

\begin{itemize}
\tightlist
\item
  \textbf{TP}: Correctly predicted positive cases
\item
  \textbf{TN}: Correctly predicted negative cases\\
\item
  \textbf{FP}: Incorrectly predicted as positive (Type I error)
\item
  \textbf{FN}: Incorrectly predicted as negative (Type II error)
\end{itemize}

\textbf{Performance Metrics:}

\begin{itemize}
\tightlist
\item
  \textbf{Accuracy} = (TP+TN)/(TP+TN+FP+FN)
\item
  \textbf{Precision} = TP/(TP+FP)
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``TPFN-FPTN'' for matrix positions

\end{mnemonicbox}
\subsection*{Question 2(c) [7 marks]}\label{q2c}

\textbf{Define data pre-processing. Explain various methods used in data
pre-processing.}

\begin{solutionbox}

Data pre-processing is the technique of preparing raw data by cleaning,
transforming, and organizing it for machine learning algorithms.

\textbf{Data Pre-processing Methods Table}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2759}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3103}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4138}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Techniques
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Data Cleaning} & Remove noise and inconsistencies & Handle
missing values, remove duplicates \\
\textbf{Data Transformation} & Convert data format & Normalization,
standardization \\
\textbf{Data Reduction} & Reduce dataset size & Feature selection,
dimensionality reduction \\
\textbf{Data Integration} & Combine multiple sources & Merge datasets,
resolve conflicts \\
\end{longtable}
}

\begin{verbatim}
flowchart LR
    A[Raw Data] {-{-} B[Data Cleaning]}
    B {-{-} C[Data Transformation]}
    C {-{-} D[Data Reduction]}
    D {-{-} E[Clean Data]}
\end{verbatim}

\begin{itemize}
\tightlist
\item
  \textbf{Missing Values}: Use mean, median, or mode for imputation
\item
  \textbf{Outliers}: Detect and handle extreme values
\item
  \textbf{Feature Scaling}: Normalize data to same scale
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``CTRI'' - Clean, Transform, Reduce, Integrate

\end{mnemonicbox}
\subsection*{Question 2(a OR) [3
marks]}\label{question-2a-or-3-marks}

\textbf{Explain histogram with suitable example.}

\begin{solutionbox}

A histogram is a graphical representation showing the frequency
distribution of numerical data by dividing it into bins.

\textbf{Histogram Components Table}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Component & Description \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{X-axis} & Data ranges (bins) \\
\textbf{Y-axis} & Frequency of occurrence \\
\textbf{Bars} & Height represents frequency \\
\end{longtable}
}

\textbf{Example}: Student marks distribution:

\begin{itemize}
\tightlist
\item
  Bins: 0-20, 21-40, 41-60, 61-80, 81-100
\item
  Heights show number of students in each range
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``BAR'' - Bins, Axes, Range

\end{mnemonicbox}
\subsection*{Question 2(b OR) [4
marks]}\label{question-2b-or-4-marks}

\textbf{Relate the appropriate data type of following examples:}
\textbf{i) Gender of a person ii) Rank of students iii) Price of a home
iv) Color of a flower}

\begin{solutionbox}

\textbf{Data Types Classification Table}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2432}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2973}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4595}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Data Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Characteristics
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Gender of person} & Nominal Categorical & No natural order
(Male/Female) \\
\textbf{Rank of students} & Ordinal Categorical & Has meaningful order
(1st, 2nd, 3rd) \\
\textbf{Price of home} & Continuous Numerical & Can take any value
within range \\
\textbf{Color of flower} & Nominal Categorical & No natural order (Red,
Blue, Yellow) \\
\end{longtable}
}

\begin{itemize}
\tightlist
\item
  \textbf{Categorical Data}: Limited set of distinct categories
\item
  \textbf{Numerical Data}: Mathematical operations possible
\item
  \textbf{Ordinal}: Categories with meaningful sequence
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``NOCO'' - Nominal, Ordinal, Continuous

\end{mnemonicbox}
\subsection*{Question 2(c OR) [7
marks]}\label{question-2c-or-7-marks}

\textbf{Describe K-fold cross validation in details.}

\begin{solutionbox}

K-fold cross validation is a model evaluation technique that divides
dataset into K equal parts for robust performance assessment.

\textbf{K-fold Process Table}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Step & Action & Purpose \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{1} & Divide data into K equal folds & Create K subsets \\
\textbf{2} & Use K-1 folds for training & Train model \\
\textbf{3} & Use 1 fold for testing & Evaluate performance \\
\textbf{4} & Repeat K times & Each fold serves as test set once \\
\textbf{5} & Average all results & Get final performance metric \\
\end{longtable}
}

\begin{verbatim}
flowchart LR
    A[Original Dataset] {-{-} B[Divide into K folds]}
    B {-{-} C[Iteration 1: Train on folds 2{-}K, Test on fold 1]}
    C {-{-} D[Iteration 2: Train on folds 1,3{-}K, Test on fold 2]}
    D {-{-} E[... Continue for K iterations]}
    E {-{-} F[Average all K results]}
\end{verbatim}

\textbf{Advantages:}

\begin{itemize}
\tightlist
\item
  \textbf{Robust Evaluation}: Every data point used for both training
  and testing
\item
  \textbf{Reduced Overfitting}: Multiple validation rounds
\item
  \textbf{Better Generalization}: More reliable performance estimate
\end{itemize}

\textbf{Common Values}: K=5 or K=10 typically used

\end{solutionbox}
\begin{mnemonicbox}
``DURAT'' - Divide, Use, Repeat, Average, Test

\end{mnemonicbox}
\subsection*{Question 3(a) [3 marks]}\label{q3a}

\textbf{List out applications of regression.}

\begin{solutionbox}

\textbf{Regression Applications Table}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2667}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Domain
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Application
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Finance} & Stock price prediction & Forecast market trends \\
\textbf{Healthcare} & Drug dosage calculation & Determine optimal
treatment \\
\textbf{Marketing} & Sales forecasting & Predict revenue \\
\textbf{Real Estate} & Property valuation & Estimate house prices \\
\end{longtable}
}

\begin{itemize}
\tightlist
\item
  \textbf{Predictive Modeling}: Forecasting continuous values
\item
  \textbf{Trend Analysis}: Understanding relationships between variables
\item
  \textbf{Risk Assessment}: Evaluating future outcomes
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``FHMR'' - Finance, Healthcare, Marketing, Real
estate

\end{mnemonicbox}
\subsection*{Question 3(b) [4 marks]}\label{q3b}

\textbf{Write a short note on single linear regression.}

\begin{solutionbox}

Single linear regression models the relationship between one independent
variable (X) and one dependent variable (Y) using a straight line.

\textbf{Linear Regression Components}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Component & Formula & Description \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Equation} & Y = a + bX & Linear relationship \\
\textbf{Slope (b)} & Change in Y / Change in X & Rate of change \\
\textbf{Intercept (a)} & Y-value when X=0 & Starting point \\
\textbf{Error} & Actual - Predicted & Difference from line \\
\end{longtable}
}

\begin{itemize}
\tightlist
\item
  \textbf{Goal}: Find best-fit line minimizing errors
\item
  \textbf{Method}: Least squares optimization
\item
  \textbf{Assumption}: Linear relationship exists between variables
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``YABX'' - Y equals a plus b times X

\end{mnemonicbox}
\subsection*{Question 3(c) [7 marks]}\label{q3c}

\textbf{Write and discuss K-NN algorithm.}

\begin{solutionbox}

K-Nearest Neighbors (K-NN) is a lazy learning algorithm that classifies
data points based on the majority class of their K nearest neighbors.

\textbf{K-NN Algorithm Steps}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Step & Action & Description \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{1} & Choose K value & Select number of neighbors \\
\textbf{2} & Calculate distances & Find distance to all training
points \\
\textbf{3} & Sort distances & Arrange in ascending order \\
\textbf{4} & Select K nearest & Choose K closest points \\
\textbf{5} & Majority voting & Assign most common class \\
\end{longtable}
}

\begin{verbatim}
flowchart LR
    A[New Data Point] {-{-} B[Calculate Distance to All Training Points]}
    B {-{-} C[Sort Distances]}
    C {-{-} D[Select K Nearest Neighbors]}
    D {-{-} E[Majority Vote]}
    E {-{-} F[Assign Class Label]}
\end{verbatim}

\textbf{Distance Metrics:}

\begin{itemize}
\tightlist
\item
  \textbf{Euclidean}: Most common distance measure
\item
  \textbf{Manhattan}: Sum of absolute differences
\item
  \textbf{Minkowski}: Generalized distance metric
\end{itemize}

\textbf{Advantages:}

\begin{itemize}
\tightlist
\item
  \textbf{Simple}: Easy to understand and implement
\item
  \textbf{No Training}: Stores all data, no model building
\end{itemize}

\textbf{Disadvantages:}

\begin{itemize}
\tightlist
\item
  \textbf{Computationally Expensive}: Must check all points
\item
  \textbf{Sensitive to K}: Performance depends on K value
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``CCSM'' - Choose, Calculate, Sort, Majority vote

\end{mnemonicbox}
\subsection*{Question 3(a OR) [3
marks]}\label{question-3a-or-3-marks}

\textbf{Write any three examples of supervised learning in the field of
healthcare}

\begin{solutionbox}

\textbf{Healthcare Supervised Learning Examples}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3514}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1892}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2162}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2432}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Application
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Input
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Output
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Disease Diagnosis} & Symptoms, test results & Disease type &
Identify medical conditions \\
\textbf{Drug Response Prediction} & Patient data, genetics & Drug
effectiveness & Personalized medicine \\
\textbf{Medical Image Analysis} & X-rays, MRI scans & Tumor detection &
Early disease detection \\
\end{longtable}
}

\begin{itemize}
\tightlist
\item
  \textbf{Pattern Recognition}: Learning from labeled medical data
\item
  \textbf{Clinical Decision Support}: Assisting doctors in diagnosis
\item
  \textbf{Predictive Medicine}: Forecasting health outcomes
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``DDM'' - Diagnosis, Drug response, Medical imaging

\end{mnemonicbox}
\subsection*{Question 3(b OR) [4
marks]}\label{question-3b-or-4-marks}

\textbf{Differentiate: Classification v/s Regression.}

\begin{solutionbox}

\textbf{Classification vs Regression Comparison}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2286}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4286}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3429}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Classification
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Regression
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Output Type} & Discrete categories/classes & Continuous
numerical values \\
\textbf{Goal} & Predict class labels & Predict numerical values \\
\textbf{Examples} & Email spam/not spam & House price prediction \\
\textbf{Evaluation} & Accuracy, Precision, Recall & MAE, MSE,
R-squared \\
\end{longtable}
}

\begin{itemize}
\tightlist
\item
  \textbf{Classification}: Predicts categories (Yes/No, Red/Blue/Green)
\item
  \textbf{Regression}: Predicts quantities (Price, Temperature, Weight)
\item
  \textbf{Algorithms}: Some work for both, others specialized
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``CLASS-Categories, REG-Real numbers''

\end{mnemonicbox}
\subsection*{Question 3(c OR) [7
marks]}\label{question-3c-or-7-marks}

\textbf{Explain classification learning steps in details.}

\begin{solutionbox}

Classification learning involves training a model to assign input data
to predefined categories or classes.

\textbf{Classification Learning Steps}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Step & Process & Description \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{1} & Data Collection & Gather labeled training examples \\
\textbf{2} & Data Preprocessing & Clean and prepare data \\
\textbf{3} & Feature Selection & Choose relevant attributes \\
\textbf{4} & Model Selection** & Choose classification algorithm \\
\textbf{5} & Training & Learn from labeled data \\
\textbf{6} & Evaluation & Test model performance \\
\textbf{7} & Deployment & Use model for predictions \\
\end{longtable}
}

\begin{verbatim}
flowchart LR
    A[Labeled Training Data] {-{-} B[Preprocessing]}
    B {-{-} C[Feature Selection]}
    C {-{-} D[Choose Algorithm]}
    D {-{-} E[Train Model]}
    E {-{-} F[Evaluate Performance]}
    F {-{-} G\{Good Performance?\}}
    G {-{-}|No| D}
    G {-{-}|Yes| H[Deploy Model]}
\end{verbatim}

\textbf{Key Concepts:}

\begin{itemize}
\tightlist
\item
  \textbf{Supervised Learning}: Requires labeled training data
\item
  \textbf{Feature Engineering}: Transform raw data into useful features
\item
  \textbf{Cross-validation}: Ensure model generalizes well
\item
  \textbf{Performance Metrics}: Accuracy, precision, recall, F1-score
\end{itemize}

\textbf{Common Algorithms:}

\begin{itemize}
\tightlist
\item
  \textbf{Decision Trees}: Easy to interpret rules
\item
  \textbf{SVM}: Effective for high-dimensional data
\item
  \textbf{Neural Networks}: Handle complex patterns
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``DCFMTED'' - Data, Clean, Features, Model, Train,
Evaluate, Deploy

\end{mnemonicbox}
\subsection*{Question 4(a) [3 marks]}\label{q4a}

\textbf{Differentiate: Clustering v/s Classification.}

\begin{solutionbox}

\textbf{Clustering vs Classification Comparison}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Aspect & Clustering & Classification \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Learning Type} & Unsupervised & Supervised \\
\textbf{Training Data} & Unlabeled data & Labeled data \\
\textbf{Goal} & Find hidden groups & Predict known classes \\
\textbf{Output} & Group assignments & Class predictions \\
\end{longtable}
}

\begin{itemize}
\tightlist
\item
  \textbf{Clustering}: Discovers unknown patterns in data
\item
  \textbf{Classification}: Learns from known examples to predict new
  ones
\item
  \textbf{Evaluation}: Clustering harder to evaluate than classification
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``CL-Unknown groups, CLASS-Known categories''

\end{mnemonicbox}
\subsection*{Question 4(b) [4 marks]}\label{q4b}

\textbf{List out advantages and disadvantages of apriori algorithm.}

\begin{solutionbox}

\textbf{Apriori Algorithm Pros and Cons}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Advantages & Disadvantages \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Easy to understand} & \textbf{Computationally expensive} \\
\textbf{Finds all frequent itemsets} & \textbf{Multiple database
scans} \\
\textbf{Well-established algorithm} & \textbf{Large memory
requirements} \\
\textbf{Generates association rules} & \textbf{Poor scalability} \\
\end{longtable}
}

\textbf{Advantages Details:}

\begin{itemize}
\tightlist
\item
  \textbf{Simplicity}: Straightforward logic and implementation
\item
  \textbf{Completeness}: Finds all frequent patterns
\item
  \textbf{Rule Generation}: Creates meaningful association rules
\end{itemize}

\textbf{Disadvantages Details:}

\begin{itemize}
\tightlist
\item
  \textbf{Performance}: Slow on large datasets
\item
  \textbf{Memory}: Stores many candidate itemsets
\item
  \textbf{Scalability}: Performance degrades with data size
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``EASY-SLOW'' - Easy to use but slow performance

\end{mnemonicbox}
\subsection*{Question 4(c) [7 marks]}\label{q4c}

\textbf{Write and explain applications of unsupervised learning.}

\begin{solutionbox}

Unsupervised learning discovers hidden patterns in data without labeled
examples.

\textbf{Unsupervised Learning Applications}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1951}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3171}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2683}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2195}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Domain
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Application
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Technique
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Marketing} & Customer segmentation & Clustering & Group similar
customers \\
\textbf{Retail} & Market basket analysis & Association rules & Find
buying patterns \\
\textbf{Anomaly Detection} & Fraud detection & Outlier detection &
Identify unusual behavior \\
\textbf{Data Compression} & Dimensionality reduction & PCA & Reduce data
size \\
\textbf{Recommendation} & Content filtering & Clustering & Suggest
similar items \\
\end{longtable}
}

\begin{verbatim}
mindmap
  root((Unsupervised Learning))
    Clustering
      Customer Segmentation
      Image Segmentation
      Gene Sequencing
    Association Rules
      Market Basket Analysis
      Web Usage Mining
      Protein Sequences
    Anomaly Detection
      Fraud Detection
      Network Security
      Quality Control
    Dimensionality Reduction
      Data Visualization
      Feature Extraction
      Data Compression
\end{verbatim}

\textbf{Key Benefits:}

\begin{itemize}
\tightlist
\item
  \textbf{Pattern Discovery}: Reveals hidden structures
\item
  \textbf{No Labels Required}: Works with raw data
\item
  \textbf{Exploratory Analysis}: Understand data characteristics
\end{itemize}

\textbf{Common Techniques:}

\begin{itemize}
\tightlist
\item
  \textbf{K-means}: Partition data into clusters
\item
  \textbf{Hierarchical Clustering}: Create cluster hierarchies
\item
  \textbf{Apriori}: Find association rules
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``MRAD'' - Marketing, Retail, Anomaly, Dimensionality

\end{mnemonicbox}
\subsection*{Question 4(a OR) [3
marks]}\label{question-4a-or-3-marks}

\textbf{List out applications of apriori algorithm.}

\begin{solutionbox}

\textbf{Apriori Algorithm Applications}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2667}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Domain
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Application
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Retail} & Market basket analysis & Find items bought together \\
\textbf{Web Mining} & Website usage patterns & Discover page visit
sequences \\
\textbf{Bioinformatics} & Gene pattern analysis & Identify gene
associations \\
\end{longtable}
}

\begin{itemize}
\tightlist
\item
  \textbf{Association Rules}: ``If A then B'' relationships
\item
  \textbf{Frequent Patterns}: Items appearing together often
\item
  \textbf{Cross-selling}: Recommend related products
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``RWB'' - Retail, Web, Bioinformatics

\end{mnemonicbox}
\subsection*{Question 4(b OR) [4
marks]}\label{question-4b-or-4-marks}

\textbf{Define: Support and Confidence.}

\begin{solutionbox}

\textbf{Association Rule Metrics}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2162}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2432}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3514}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1892}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Metric
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formula
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Range
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Support} & Support(A) = Count(A) / Total transactions & How
often itemset appears & 0 to 1 \\
\textbf{Confidence} & Confidence(A\rightarrowB) = Support(A\cupB) / Support(A) & How
often rule is true & 0 to 1 \\
\end{longtable}
}

\textbf{Support Example:}

\begin{itemize}
\tightlist
\item
  If itemset \{Bread, Milk\} appears in 3 out of 10 transactions
\item
  Support = 3/10 = 0.3 (30\%)
\end{itemize}

\textbf{Confidence Example:}

\begin{itemize}
\tightlist
\item
  Rule: ``Bread \rightarrow Milk''
\item
  If \{Bread, Milk\} appears 3 times, Bread alone appears 5 times
\item
  Confidence = 3/5 = 0.6 (60\%)
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``SUP-How often, CONF-How reliable''

\end{mnemonicbox}
\subsection*{Question 4(c OR) [7
marks]}\label{question-4c-or-7-marks}

\textbf{Write and explain K-means clustering approach in detail.}

\begin{solutionbox}

K-means clustering partitions data into K clusters by minimizing
within-cluster sum of squares.

\textbf{K-means Algorithm Steps}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Step & Action & Description \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{1} & Choose K & Select number of clusters \\
\textbf{2} & Initialize centroids & Place K points randomly \\
\textbf{3} & Assign points & Each point to nearest centroid \\
\textbf{4} & Update centroids & Calculate mean of assigned points \\
\textbf{5} & Repeat 3-4 & Until convergence \\
\end{longtable}
}

\begin{verbatim}
flowchart LR
    A[Choose K value] {-{-} B[Initialize K centroids randomly]}
    B {-{-} C[Assign each point to nearest centroid]}
    C {-{-} D[Update centroids to cluster means]}
    D {-{-} E\{Centroids changed?\}}
    E {-{-}|Yes| C}
    E {-{-}|No| F[Final clusters]}
\end{verbatim}

\textbf{Algorithm Details:}

\begin{itemize}
\tightlist
\item
  \textbf{Distance Metric}: Usually Euclidean distance
\item
  \textbf{Convergence}: When centroids stop moving significantly
\item
  \textbf{Objective}: Minimize within-cluster sum of squares (WCSS)
\end{itemize}

\textbf{Advantages:}

\begin{itemize}
\tightlist
\item
  \textbf{Simple}: Easy to understand and implement
\item
  \textbf{Efficient}: Linear time complexity
\item
  \textbf{Scalable}: Works well with large datasets
\end{itemize}

\textbf{Disadvantages:}

\begin{itemize}
\tightlist
\item
  \textbf{K Selection}: Must choose K beforehand
\item
  \textbf{Sensitive to Initialization}: Different starting points give
  different results
\item
  \textbf{Assumes Spherical Clusters}: May not work with irregular
  shapes
\end{itemize}

\textbf{Choosing K:}

\begin{itemize}
\tightlist
\item
  \textbf{Elbow Method}: Plot WCSS vs K, look for ``elbow''
\item
  \textbf{Silhouette Analysis}: Measure cluster quality
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``CIAUR'' - Choose K, Initialize, Assign, Update,
Repeat

\end{mnemonicbox}
\subsection*{Question 5(a) [3 marks]}\label{q5a}

\textbf{Give the difference between predictive model and descriptive
model.}

\begin{solutionbox}

\textbf{Predictive vs Descriptive Models}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1778}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4222}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Predictive Model
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Descriptive Model
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Purpose} & Forecast future outcomes & Explain current
patterns \\
\textbf{Output} & Predictions/classifications & Insights/summaries \\
\textbf{Examples} & Sales forecasting, spam detection & Customer
segmentation, trend analysis \\
\end{longtable}
}

\begin{itemize}
\tightlist
\item
  \textbf{Predictive}: Uses historical data to predict future
\item
  \textbf{Descriptive}: Analyzes existing data to understand patterns
\item
  \textbf{Goal}: Prediction vs Understanding
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``PRED-Future, DESC-Present''

\end{mnemonicbox}
\subsection*{Question 5(b) [4 marks]}\label{q5b}

\textbf{List out application of scikit-learn.}

\begin{solutionbox}

\textbf{Scikit-learn Applications}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2857}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3714}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3429}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Category
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Applications
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Algorithms
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Classification} & Email filtering, image recognition & SVM,
Random Forest, Naive Bayes \\
\textbf{Regression} & Price prediction, risk assessment & Linear
Regression, Decision Trees \\
\textbf{Clustering} & Customer segmentation, data exploration & K-means,
DBSCAN \\
\textbf{Preprocessing} & Data cleaning, feature scaling &
StandardScaler, LabelEncoder \\
\end{longtable}
}

\begin{itemize}
\tightlist
\item
  \textbf{Machine Learning Library}: Comprehensive Python toolkit
\item
  \textbf{Easy Integration}: Works with NumPy, Pandas
\item
  \textbf{Well-documented}: Extensive examples and tutorials
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``CRCP'' - Classification, Regression, Clustering,
Preprocessing

\end{mnemonicbox}
\subsection*{Question 5(c) [7 marks]}\label{q5c}

\textbf{Explain features and applications of Numpy.}

\begin{solutionbox}

NumPy (Numerical Python) is the fundamental library for scientific
computing in Python, providing support for large multi-dimensional
arrays and mathematical functions.

\textbf{NumPy Features Table}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2903}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4194}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2903}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Feature
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Benefit
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{N-dimensional Arrays} & Powerful array objects & Efficient data
storage \\
\textbf{Broadcasting} & Operations on different shaped arrays & Flexible
computations \\
\textbf{Mathematical Functions} & Trigonometric, logarithmic,
statistical & Complete math toolkit \\
\textbf{Performance} & Implemented in C/Fortran & Fast execution \\
\textbf{Memory Efficiency} & Contiguous memory layout & Reduced memory
usage \\
\end{longtable}
}

\textbf{NumPy Applications}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2667}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Domain
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Application
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Machine Learning} & Data preprocessing, feature engineering &
Handle numerical data \\
\textbf{Image Processing} & Image manipulation, filtering & Process
pixel arrays \\
\textbf{Scientific Computing} & Numerical simulations, modeling &
Mathematical computations \\
\textbf{Financial Analysis} & Portfolio optimization, risk modeling &
Quantitative analysis \\
\end{longtable}
}

\begin{verbatim}
mindmap
  root((NumPy))
    Core Features
      N{-dimensional Arrays}
      Broadcasting
      Mathematical Functions
      Fast Performance
    Applications
      Machine Learning
      Image Processing
      Scientific Computing
      Financial Analysis
    Benefits
      Memory Efficient
      Easy to Use
      Integrates Well
      Industry Standard
\end{verbatim}

\textbf{Key Capabilities:}

\begin{itemize}
\tightlist
\item
  \textbf{Array Operations}: Element-wise operations, slicing, indexing
\item
  \textbf{Linear Algebra}: Matrix operations, eigenvalues,
  decompositions
\item
  \textbf{Random Number Generation}: Statistical distributions, sampling
\item
  \textbf{Fourier Transforms}: Signal processing, frequency analysis
\end{itemize}

\textbf{Integration:}

\begin{itemize}
\tightlist
\item
  \textbf{Pandas}: DataFrames built on NumPy arrays
\item
  \textbf{Matplotlib}: Plotting NumPy arrays
\item
  \textbf{Scikit-learn}: ML algorithms use NumPy arrays
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``NFAMS'' - N-dimensional, Fast, Arrays, Math,
Scientific

\end{mnemonicbox}
\subsection*{Question 5(a OR) [3
marks]}\label{question-5a-or-3-marks}

\textbf{Write a short note on bagging}

\begin{solutionbox}

Bagging (Bootstrap Aggregating) is an ensemble method that improves
model performance by training multiple models on different subsets of
data.

\textbf{Bagging Process Table}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3750}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3750}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Step
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Process
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Bootstrap Sampling} & Create multiple training sets & Generate
diverse datasets \\
\textbf{Train Models} & Build model on each subset & Create multiple
predictors \\
\textbf{Aggregate Results} & Combine predictions (voting/averaging) &
Reduce overfitting \\
\end{longtable}
}

\begin{itemize}
\tightlist
\item
  \textbf{Variance Reduction}: Reduces model variance through averaging
\item
  \textbf{Parallel Training}: Models trained independently
\item
  \textbf{Example}: Random Forest uses bagging with decision trees
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``BTA'' - Bootstrap, Train, Aggregate

\end{mnemonicbox}
\subsection*{Question 5(b OR) [4
marks]}\label{question-5b-or-4-marks}

\textbf{List out features of Pandas.}

\begin{solutionbox}

\textbf{Pandas Features}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2903}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4194}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2903}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Feature
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Benefit
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{DataFrame/Series} & Structured data containers & Easy data
manipulation \\
\textbf{File I/O} & Read/write CSV, Excel, JSON & Handle various
formats \\
\textbf{Data Cleaning} & Handle missing values, duplicates & Prepare
clean data \\
\textbf{Grouping/Aggregation} & Group by operations, statistics &
Analyze data patterns \\
\end{longtable}
}

\textbf{Data Operations:}

\begin{itemize}
\tightlist
\item
  \textbf{Indexing}: Flexible data selection and filtering
\item
  \textbf{Merging}: Combine datasets with joins
\item
  \textbf{Reshaping}: Pivot tables and data transformation
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``DFIG'' - DataFrame, File I/O, Indexing, Grouping

\end{mnemonicbox}
\subsection*{Question 5(c OR) [7
marks]}\label{question-5c-or-7-marks}

\textbf{Explain features and applications of Matplotlib.}

\begin{solutionbox}

Matplotlib is a comprehensive 2D plotting library for Python that
produces publication-quality figures in various formats and interactive
environments.

\textbf{Matplotlib Features}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2647}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3824}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3529}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Feature
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Capability
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Plot Types} & Line, bar, scatter, histogram, pie & Diverse
visualization options \\
\textbf{Customization} & Colors, fonts, styles, layouts & Professional
appearance \\
\textbf{Interactive Features} & Zoom, pan, widgets & Dynamic
exploration \\
\textbf{Multiple Backends} & GUI, web, file output & Flexible
deployment \\
\textbf{3D Plotting} & Surface, wireframe, scatter plots &
Three-dimensional visualization \\
\end{longtable}
}

\textbf{Matplotlib Applications}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3250}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4750}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Domain
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Application
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Visualization Type
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Data Science} & Exploratory data analysis & Histograms, scatter
plots \\
\textbf{Scientific Research} & Publication figures & Line plots, error
bars \\
\textbf{Business Intelligence} & Dashboard creation & Bar charts, trend
lines \\
\textbf{Machine Learning} & Model performance visualization & Confusion
matrices, ROC curves \\
\textbf{Engineering} & Signal analysis & Time series, frequency plots \\
\end{longtable}
}

\begin{verbatim}
flowchart LR
    A[Raw Data] {-{-} B[Matplotlib Processing]}
    B {-{-} C[Static Plots]}
    B {-{-} D[Interactive Plots]}
    B {-{-} E[Publication Figures]}
    C {-{-} F[PNG/PDF Output]}
    D {-{-} G[Web Applications]}
    E {-{-} H[Research Papers]}
\end{verbatim}

\textbf{Key Components:}

\begin{itemize}
\tightlist
\item
  \textbf{Figure}: Top-level container for all plot elements
\item
  \textbf{Axes}: Individual plots within a figure
\item
  \textbf{Artist}: Everything drawn on figure (lines, text, etc.)
\item
  \textbf{Backend}: Handles rendering to different outputs
\end{itemize}

\textbf{Plot Customization:}

\begin{itemize}
\tightlist
\item
  \textbf{Colors/Styles}: Wide range of visual options
\item
  \textbf{Annotations}: Text labels, arrows, legends
\item
  \textbf{Subplots}: Multiple plots in single figure
\item
  \textbf{Layouts}: Grid arrangements, spacing control
\end{itemize}

\textbf{Integration Benefits:}

\begin{itemize}
\tightlist
\item
  \textbf{NumPy Arrays}: Direct plotting of numerical data
\item
  \textbf{Pandas}: Built-in plotting methods
\item
  \textbf{Jupyter Notebooks}: Inline plot display
\item
  \textbf{Web Frameworks}: Embed plots in applications
\end{itemize}

\textbf{Output Formats:}

\begin{itemize}
\tightlist
\item
  \textbf{Raster}: PNG, JPEG for web use
\item
  \textbf{Vector}: PDF, SVG for publications
\item
  \textbf{Interactive}: HTML for web deployment
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``MVICS'' - Multiple plots, Visualization,
Interactive, Customizable, Scientific

\end{mnemonicbox}

\end{document}
