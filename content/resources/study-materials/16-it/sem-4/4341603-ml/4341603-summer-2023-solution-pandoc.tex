\documentclass[10pt,a4paper]{article}
\input{../../../../../../latex-templates/gtu-solutions/preamble.tex}
\input{../../../../../../latex-templates/gtu-solutions/english-boxes.tex}

\begin{document}

\begin{center}
{\Huge\bfseries\color{headcolor} Subject Name Solutions}\\[5pt]
{\LARGE 4341603 -- Summer 2023}\\[3pt]
{\large Semester 1 Study Material}\\[3pt]
{\normalsize\textit{Detailed Solutions and Explanations}}
\end{center}

\vspace{10pt}

\subsection*{Question 1(a) [3 marks]}\label{q1a}

\textbf{Define human learning. List out types of human learning.}

\begin{solutionbox}

Human learning is the process by which humans acquire new knowledge,
skills, behaviors, or modify existing ones through experience, study, or
instruction.

\textbf{Types of Human Learning:}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.3158}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.6842}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Supervised Learning} & Learning with guidance from
teacher/mentor \\
\textbf{Unsupervised Learning} & Self-directed learning without external
guidance \\
\textbf{Reinforcement Learning} & Learning through trial and error with
feedback \\
\end{longtable}
}

\end{solutionbox}
\begin{mnemonicbox}
``SUR - Supervised, Unsupervised, Reinforcement''

\end{mnemonicbox}
\subsection*{Question 1(b) [4 marks]}\label{q1b}

\textbf{Differentiate between qualitative data and quantitative data.}

\begin{solutionbox}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Qualitative vs Quantitative Data}
\vspace{-10pt}
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Feature & Qualitative Data & Quantitative Data \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Nature} & Descriptive, categorical & Numerical, measurable \\
\textbf{Analysis} & Subjective interpretation & Statistical analysis \\
\textbf{Examples} & Colors, names, gender & Height, weight, age \\
\textbf{Representation} & Words, categories & Numbers, graphs \\
\end{longtable}
}

\end{solutionbox}
\begin{mnemonicbox}
``QUAN-Numbers, QUAL-Words''

\end{mnemonicbox}
\subsection*{Question 1(c) [7 marks]}\label{q1c}

\textbf{Compare the different types of machine learning.}

\begin{solutionbox}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Types of Machine Learning Comparison}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1622}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4054}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1622}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2703}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Training Data
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Goal
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Examples
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Supervised} & Labeled data & Predict outcomes & Classification,
Regression \\
\textbf{Unsupervised} & Unlabeled data & Find patterns & Clustering,
Association \\
\textbf{Reinforcement} & Reward/penalty & Maximize rewards & Gaming,
Robotics \\
\end{longtable}
}

\textbf{Key Differences:}

\begin{itemize}
\tightlist
\item
  \textbf{Supervised}: Uses input-output pairs for training
\item
  \textbf{Unsupervised}: Discovers hidden patterns in data
\item
  \textbf{Reinforcement}: Learns through interaction with environment
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``SUR-LAP: Supervised-Labeled, Unsupervised-Reveal,
Reinforcement-Action''

\end{mnemonicbox}
\subsection*{Question 1(c OR) [7
marks]}\label{question-1c-or-7-marks}

\textbf{Define machine learning. Explain any four applications of
machine learning in brief.}

\begin{solutionbox}

Machine learning is a subset of artificial intelligence that enables
computers to learn and make decisions from data without being explicitly
programmed.

\textbf{Four Applications:}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Application & Description \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Email Spam Detection} & Classifies emails as spam or
legitimate \\
\textbf{Image Recognition} & Identifies objects in photos \\
\textbf{Recommendation Systems} & Suggests products/content to users \\
\textbf{Medical Diagnosis} & Assists doctors in disease detection \\
\end{longtable}
}

\end{solutionbox}
\begin{mnemonicbox}
``SIRM - Spam, Image, Recommendation, Medical''

\end{mnemonicbox}
\subsection*{Question 2(a) [3 marks]}\label{q2a}

\textbf{Relate the appropriate data type of following examples.}

\begin{solutionbox}

\textbf{Data Type Classification:}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Example & Data Type \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Nationality of students} & Categorical (Nominal) \\
\textbf{Education status of students} & Categorical (Ordinal) \\
\textbf{Height of students} & Numerical (Continuous) \\
\end{longtable}
}

\end{solutionbox}
\begin{mnemonicbox}
``NCN - Nominal, Categorical, Numerical''

\end{mnemonicbox}
\subsection*{Question 2(b) [4 marks]}\label{q2b}

\textbf{Explain data pre-processing in brief.}

\begin{solutionbox}

Data pre-processing is the technique of preparing raw data for machine
learning algorithms.

\textbf{Key Steps:}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Step & Purpose \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Data Cleaning} & Remove errors and inconsistencies \\
\textbf{Data Integration} & Combine data from multiple sources \\
\textbf{Data Transformation} & Convert data to suitable format \\
\textbf{Data Reduction} & Reduce data size while preserving
information \\
\end{longtable}
}

\end{solutionbox}
\begin{mnemonicbox}
``CITR - Clean, Integrate, Transform, Reduce''

\end{mnemonicbox}
\subsection*{Question 2(c) [7 marks]}\label{q2c}

\textbf{Show K-fold cross validation in detail.}

\begin{solutionbox}

K-fold cross validation is a technique to evaluate model performance by
dividing data into K equal parts.

\textbf{Process:}

\begin{center}
\textbf{Mermaid Diagram (Code)}
\begin{verbatim}
{Shaded}
{Highlighting}[]
graph LR
    A[Original Dataset] {-{-}{} B[Split into K folds]}
    B {-{-}{} C[Use K{-}1 folds for training]}
    C {-{-}{} D[Use 1 fold for testing]}
    D {-{-}{} E[Repeat K times]}
    E {-{-}{} F[Average results]}
{Highlighting}
{Shaded}
\end{verbatim}
\end{center}

\textbf{Steps:}

\begin{itemize}
\tightlist
\item
  \textbf{Divide}: Split dataset into K equal parts
\item
  \textbf{Train}: Use K-1 folds for training
\item
  \textbf{Test}: Use remaining fold for validation
\item
  \textbf{Repeat}: Perform K iterations
\item
  \textbf{Average}: Calculate mean performance
\end{itemize}

\textbf{Advantages:}

\begin{itemize}
\tightlist
\item
  Reduces overfitting
\item
  Better use of limited data
\item
  More reliable performance estimate
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``DTRA - Divide, Train, Repeat, Average''

\end{mnemonicbox}
\subsection*{Question 2(a OR) [3
marks]}\label{question-2a-or-3-marks}

\textbf{Define following terms: i) Mean, ii) Outliers, iii)
Interquartile range}

\begin{solutionbox}

\textbf{Statistical Terms:}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.6667}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Term
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Mean} & Average of all values in dataset \\
\textbf{Outliers} & Data points significantly different from others \\
\textbf{Interquartile Range} & Difference between 75th and 25th
percentiles \\
\end{longtable}
}

\end{solutionbox}
\begin{mnemonicbox}
``MOI - Mean, Outliers, Interquartile''

\end{mnemonicbox}
\subsection*{Question 2(b OR) [4
marks]}\label{question-2b-or-4-marks}

\textbf{Explain structure of confusion matrix.}

\begin{solutionbox}

\textbf{Confusion Matrix Structure:}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
& Predicted & \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Actual} & \textbf{Positive} & \textbf{Negative} \\
\textbf{Positive} & True Positive (TP) & False Negative (FN) \\
\textbf{Negative} & False Positive (FP) & True Negative (TN) \\
\end{longtable}
}

\textbf{Components:}

\begin{itemize}
\tightlist
\item
  \textbf{TP}: Correctly predicted positive cases
\item
  \textbf{TN}: Correctly predicted negative cases
\item
  \textbf{FP}: Incorrectly predicted as positive
\item
  \textbf{FN}: Incorrectly predicted as negative
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``TTFF - True True, False False''

\end{mnemonicbox}
\subsection*{Question 2(c OR) [7
marks]}\label{question-2c-or-7-marks}

\textbf{Prepare short note on feature subset selection.}

\begin{solutionbox}

Feature subset selection is the process of selecting relevant features
from the original feature set.

\textbf{Methods:}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Method & Description \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Filter Methods} & Use statistical measures to rank features \\
\textbf{Wrapper Methods} & Use ML algorithms to evaluate feature
subsets \\
\textbf{Embedded Methods} & Feature selection during model training \\
\end{longtable}
}

\textbf{Benefits:}

\begin{itemize}
\tightlist
\item
  \textbf{Reduced complexity}: Fewer features, simpler models
\item
  \textbf{Improved performance}: Eliminates noise and irrelevant
  features
\item
  \textbf{Faster training}: Less computational overhead
\end{itemize}

\textbf{Popular Techniques:}

\begin{itemize}
\tightlist
\item
  Chi-square test
\item
  Recursive Feature Elimination
\item
  LASSO regularization
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``FWE - Filter, Wrapper, Embedded''

\end{mnemonicbox}
\subsection*{Question 3(a) [3 marks]}\label{q3a}

\textbf{Give the difference between predictive model and descriptive
model.}

\begin{solutionbox}

\textbf{Model Type Comparison:}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1957}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3913}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4130}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Feature
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Predictive Model
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Descriptive Model
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Purpose} & Forecast future outcomes & Understand current
patterns \\
\textbf{Output} & Predictions/classifications & Insights/summaries \\
\textbf{Examples} & Regression, classification & Clustering, association
rules \\
\end{longtable}
}

\end{solutionbox}
\begin{mnemonicbox}
``PF-DC: Predictive-Future, Descriptive-Current''

\end{mnemonicbox}
\subsection*{Question 3(b) [4 marks]}\label{q3b}

\textbf{Discuss the difference between classification and regression.}

\begin{solutionbox}

\textbf{Classification vs Regression:}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2222}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4444}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Classification
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Regression
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Output} & Discrete categories & Continuous values \\
\textbf{Goal} & Predict class labels & Predict numerical values \\
\textbf{Examples} & Spam detection, image recognition & Price
prediction, temperature \\
\textbf{Evaluation} & Accuracy, precision, recall & MSE, RMSE,
R-squared \\
\end{longtable}
}

\end{solutionbox}
\begin{mnemonicbox}
``CCNM - Classification-Categories,
Regression-Numbers''

\end{mnemonicbox}
\subsection*{Question 3(c) [7 marks]}\label{q3c}

\textbf{Define classification. Illustrate classification learning steps
in details.}

\begin{solutionbox}

Classification is a supervised learning technique that predicts discrete
class labels for input data.

\textbf{Classification Learning Steps:}

\begin{center}
\textbf{Mermaid Diagram (Code)}
\begin{verbatim}
{Shaded}
{Highlighting}[]
graph LR
    A[Data Collection] {-{-}{} B[Data Preprocessing]}
    B {-{-}{} C[Feature Selection]}
    C {-{-}{} D[Train{-}Test Split]}
    D {-{-}{} E[Model Training]}
    E {-{-}{} F[Model Evaluation]}
    F {-{-}{} G[Model Deployment]}
{Highlighting}
{Shaded}
\end{verbatim}
\end{center}

\textbf{Detailed Steps:}

\begin{itemize}
\tightlist
\item
  \textbf{Data Collection}: Gather labeled training data
\item
  \textbf{Preprocessing}: Clean and prepare data
\item
  \textbf{Feature Selection}: Choose relevant attributes
\item
  \textbf{Split Data}: Divide into training and testing sets
\item
  \textbf{Training}: Build model using training data
\item
  \textbf{Evaluation}: Test model performance
\item
  \textbf{Deployment}: Use model for predictions
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``DCFSTED - Data, Clean, Features, Split, Train,
Evaluate, Deploy''

\end{mnemonicbox}
\subsection*{Question 3(a OR) [3
marks]}\label{question-3a-or-3-marks}

\textbf{Give the difference between bagging and boosting.}

\begin{solutionbox}

\textbf{Bagging vs Boosting:}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Feature & Bagging & Boosting \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Sampling} & Bootstrap sampling & Sequential weighted sampling \\
\textbf{Training} & Parallel training & Sequential training \\
\textbf{Focus} & Reduce variance & Reduce bias \\
\end{longtable}
}

\end{solutionbox}
\begin{mnemonicbox}
``BPV-BSB: Bagging-Parallel-Variance,
Boosting-Sequential-Bias''

\end{mnemonicbox}
\subsection*{Question 3(b OR) [4
marks]}\label{question-3b-or-4-marks}

\textbf{Explain different types of logistic regression in brief.}

\begin{solutionbox}

\textbf{Types of Logistic Regression:}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Type & Classes & Use Case \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Binary} & 2 classes & Yes/No, Pass/Fail \\
\textbf{Multinomial} & 3+ classes (unordered) & Color classification \\
\textbf{Ordinal} & 3+ classes (ordered) & Rating scales \\
\end{longtable}
}

\end{solutionbox}
\begin{mnemonicbox}
``BMO - Binary, Multinomial, Ordinal''

\end{mnemonicbox}
\subsection*{Question 3(c OR) [7
marks]}\label{question-3c-or-7-marks}

\textbf{Write and show the use of k-NN algorithms.}

\begin{solutionbox}

K-Nearest Neighbors (k-NN) is a lazy learning algorithm that classifies
data points based on the majority class of k nearest neighbors.

\textbf{Algorithm:}

\begin{verbatim}
1. Choose value of k
2. Calculate distance to all training points
3. Select k nearest neighbors
4. For classification: majority vote
   For regression: average of k neighbors
5. Assign class/value to test point
\end{verbatim}

\textbf{Distance Calculation:}

\begin{itemize}
\tightlist
\item
  \textbf{Euclidean Distance}: \sqrt[(x_{1}-x_{2})^{2} + (y_{1}-y_{2})^{2}]
\end{itemize}

\textbf{Applications:}

\begin{itemize}
\tightlist
\item
  \textbf{Recommendation systems}: Similar user preferences
\item
  \textbf{Image recognition}: Pattern matching
\item
  \textbf{Medical diagnosis}: Symptom similarity
\end{itemize}

\textbf{Advantages:}

\begin{itemize}
\tightlist
\item
  Simple to implement
\item
  No training required
\item
  Works well with small datasets
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``CDSA - Choose, Distance, Select, Assign''

\end{mnemonicbox}
\subsection*{Question 4(a) [3 marks]}\label{q4a}

\textbf{List out applications of support vector machine.}

\begin{solutionbox}

\textbf{SVM Applications:}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Application & Domain \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Text Classification} & Document categorization \\
\textbf{Image Recognition} & Face detection \\
\textbf{Bioinformatics} & Gene classification \\
\end{longtable}
}

\end{solutionbox}
\begin{mnemonicbox}
``TIB - Text, Image, Bio''

\end{mnemonicbox}
\subsection*{Question 4(b) [4 marks]}\label{q4b}

\textbf{Create pseudo code for k-means algorithm.}

\begin{solutionbox}

\textbf{K-means Pseudo Code:}

\begin{verbatim}
BEGIN K-means
1. Initialize k cluster centroids randomly
2. REPEAT
   a. Assign each point to nearest centroid
   b. Update centroids as mean of assigned points
   c. Calculate total within-cluster sum of squares
3. UNTIL convergence or max iterations
4. RETURN final clusters and centroids
END
\end{verbatim}

\end{solutionbox}
\begin{mnemonicbox}
``IAUC - Initialize, Assign, Update, Check''

\end{mnemonicbox}
\subsection*{Question 4(c) [7 marks]}\label{q4c}

\textbf{Write and explain applications of unsupervised learning.}

\begin{solutionbox}

Unsupervised learning discovers hidden patterns in data without labeled
examples.

\textbf{Major Applications:}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3714}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3714}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2571}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Application
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Customer Segmentation} & Group customers by behavior & Market
research \\
\textbf{Anomaly Detection} & Identify unusual patterns & Fraud
detection \\
\textbf{Data Compression} & Reduce dimensionality & Image compression \\
\textbf{Association Rules} & Find item relationships & Market basket
analysis \\
\end{longtable}
}

\textbf{Clustering Applications:}

\begin{itemize}
\tightlist
\item
  \textbf{Market research}: Customer grouping
\item
  \textbf{Social network analysis}: Community detection
\item
  \textbf{Gene sequencing}: Biological classification
\end{itemize}

\textbf{Dimensionality Reduction:}

\begin{itemize}
\tightlist
\item
  \textbf{Visualization}: High-dimensional data plotting
\item
  \textbf{Feature extraction}: Noise reduction
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``CADA - Customer, Anomaly, Data, Association''

\end{mnemonicbox}
\subsection*{Question 4(a OR) [3
marks]}\label{question-4a-or-3-marks}

\textbf{List out applications of regression.}

\begin{solutionbox}

\textbf{Regression Applications:}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Application & Purpose \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Stock Price Prediction} & Financial forecasting \\
\textbf{Sales Forecasting} & Business planning \\
\textbf{Medical Diagnosis} & Risk assessment \\
\end{longtable}
}

\end{solutionbox}
\begin{mnemonicbox}
``SSM - Stock, Sales, Medical''

\end{mnemonicbox}
\subsection*{Question 4(b OR) [4
marks]}\label{question-4b-or-4-marks}

\textbf{Define following terms: i) Support ii) Confidence}

\begin{solutionbox}

\textbf{Association Rule Terms:}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2222}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4444}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Term
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formula
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Support} & Frequency of itemset in database & Support(A) = \\
\textbf{Confidence} & Conditional probability of rule & Confidence(A\rightarrowB)
= Support(A\cupB) / Support(A) \\
\end{longtable}
}

\textbf{Example:}

\begin{itemize}
\tightlist
\item
  If 30\% transactions contain bread and milk: Support = 0.3
\item
  If 80\% of bread buyers also buy milk: Confidence = 0.8
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``SF-CP: Support-Frequency, Confidence-Probability''

\end{mnemonicbox}
\subsection*{Question 4(c OR) [7
marks]}\label{question-4c-or-7-marks}

\textbf{Explain apriori algorithm in detail.}

\begin{solutionbox}

Apriori algorithm finds frequent itemsets in transactional data using
the apriori property.

\textbf{Algorithm Steps:}

\begin{center}
\textbf{Mermaid Diagram (Code)}
\begin{verbatim}
{Shaded}
{Highlighting}[]
graph LR
    A[Find frequent 1{-itemsets] {-}{-}{} B[Generate candidate 2{-}itemsets]}
    B {-{-}{} C[Prune using apriori property]}
    C {-{-}{} D[Count support in database]}
    D {-{-}{} E[Find frequent k{-}itemsets]}
    E {-{-}{} F\{More candidates?\}}
    F {-{-}{}|Yes| B}
    F {-{-}{}|No| G[Generate rules]}
{Highlighting}
{Shaded}
\end{verbatim}
\end{center}

\textbf{Apriori Property:}

\begin{itemize}
\tightlist
\item
  If an itemset is frequent, all its subsets are frequent
\item
  If an itemset is infrequent, all its supersets are infrequent
\end{itemize}

\textbf{Steps:}

\begin{enumerate}
\tightlist
\item
  \textbf{Scan database}: Count 1-item support
\item
  \textbf{Generate candidates}: Create k+1 itemsets from frequent
  k-itemsets
\item
  \textbf{Prune}: Remove candidates with infrequent subsets
\item
  \textbf{Count support}: Scan database for candidate frequencies
\item
  \textbf{Repeat}: Until no new frequent itemsets found
\end{enumerate}

\textbf{Applications:}

\begin{itemize}
\tightlist
\item
  Market basket analysis
\item
  Web usage patterns
\item
  Protein sequences
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``SGPCR - Scan, Generate, Prune, Count, Repeat''

\end{mnemonicbox}
\subsection*{Question 5(a) [3 marks]}\label{q5a}

\textbf{List out the major features of matplotlib.}

\begin{solutionbox}

\textbf{Matplotlib Features:}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Feature & Description \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Multiple Plot Types} & Line, bar, scatter, histogram \\
\textbf{Customization} & Colors, styles, labels \\
\textbf{Export Options} & PNG, PDF, SVG formats \\
\end{longtable}
}

\end{solutionbox}
\begin{mnemonicbox}
``MCE - Multiple, Customization, Export''

\end{mnemonicbox}
\subsection*{Question 5(b) [4 marks]}\label{q5b}

\textbf{How to load iris dataset in Numpy program? Explain.}

\begin{solutionbox}

\textbf{Loading Iris Dataset in NumPy:}

\begin{verbatim}
import numpy as np
from sklearn.datasets import load\_iris

\# Load iris dataset
iris = load\_iris()
data = iris.data    \# Features
target = iris.target \# Labels
\end{verbatim}

\textbf{Steps:}

\begin{itemize}
\tightlist
\item
  \textbf{Import}: Import required libraries
\item
  \textbf{Load}: Use sklearn's load\_iris() function
\item
  \textbf{Extract}: Get features and target arrays
\item
  \textbf{Access}: Use .data and .target attributes
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``ILEA - Import, Load, Extract, Access''

\end{mnemonicbox}
\subsection*{Question 5(c) [7 marks]}\label{q5c}

\textbf{Explain features and applications of Pandas.}

\begin{solutionbox}

Pandas is a powerful data manipulation and analysis library for Python.

\textbf{Key Features:}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Feature & Description \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{DataFrame} & 2D labeled data structure \\
\textbf{Series} & 1D labeled array \\
\textbf{Data I/O} & Read/write various file formats \\
\textbf{Data Cleaning} & Handle missing values \\
\textbf{Grouping} & Group and aggregate operations \\
\end{longtable}
}

\textbf{Applications:}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Application & Use Case \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Data Analysis} & Statistical analysis \\
\textbf{Data Cleaning} & Preprocessing for ML \\
\textbf{Financial Analysis} & Stock market data \\
\textbf{Web Scraping} & Parse HTML tables \\
\end{longtable}
}

\textbf{Common Operations:}

\begin{itemize}
\tightlist
\item
  \textbf{Reading data}: pd.read\_csv(), pd.read\_excel()
\item
  \textbf{Filtering}: df[df[`column'] \textgreater{} value]
\item
  \textbf{Grouping}: df.groupby(`column').mean()
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``DSDCG - DataFrame, Series, Data I/O, Cleaning,
Grouping''

\end{mnemonicbox}
\subsection*{Question 5(a OR) [3
marks]}\label{question-5a-or-3-marks}

\textbf{List out the applications of matplotlib.}

\begin{solutionbox}

\textbf{Matplotlib Applications:}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Application & Purpose \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Scientific Visualization} & Research data plotting \\
\textbf{Business Analytics} & Dashboard creation \\
\textbf{Educational Content} & Teaching materials \\
\end{longtable}
}

\end{solutionbox}
\begin{mnemonicbox}
``SBE - Scientific, Business, Educational''

\end{mnemonicbox}
\subsection*{Question 5(b OR) [4
marks]}\label{question-5b-or-4-marks}

\textbf{Develop and explain the steps to import csv file in Pandas.}

\begin{solutionbox}

\textbf{Steps to Import CSV in Pandas:}

\begin{verbatim}
import pandas as pd

\# Step 1: Import pandas library
\# Step 2: Use read\_csv() function
df = pd.read\_csv({filename.csv})

\# Optional parameters
df = pd.read\_csv({file.csv}, 
                 header=0,     \# First row as header
                 sep={,},      \# Comma separator
                 index\_col=0)  \# First column as index
\end{verbatim}

\textbf{Process:}

\begin{itemize}
\tightlist
\item
  \textbf{Import}: Import pandas library
\item
  \textbf{Read}: Use pd.read\_csv() function
\item
  \textbf{Specify}: Add file path and parameters
\item
  \textbf{Store}: Assign to DataFrame variable
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``IRSS - Import, Read, Specify, Store''

\end{mnemonicbox}
\subsection*{Question 5(c OR) [7
marks]}\label{question-5c-or-7-marks}

\textbf{Explain features and applications of Scikit-Learn.}

\begin{solutionbox}

Scikit-Learn is a comprehensive machine learning library for Python.

\textbf{Key Features:}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Feature & Description \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Algorithms} & Classification, regression, clustering \\
\textbf{Preprocessing} & Data scaling and transformation \\
\textbf{Model Selection} & Cross-validation and grid search \\
\textbf{Metrics} & Performance evaluation tools \\
\end{longtable}
}

\textbf{Applications:}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Domain & Use Case \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Healthcare} & Disease prediction \\
\textbf{Finance} & Credit scoring \\
\textbf{Marketing} & Customer segmentation \\
\textbf{Technology} & Recommendation systems \\
\end{longtable}
}

\textbf{Algorithm Categories:}

\begin{itemize}
\tightlist
\item
  \textbf{Supervised}: SVM, Random Forest, Linear Regression
\item
  \textbf{Unsupervised}: K-means, DBSCAN, PCA
\item
  \textbf{Ensemble}: Bagging, Boosting
\end{itemize}

\textbf{Workflow:}

\begin{enumerate}
\tightlist
\item
  \textbf{Data preparation}: Preprocessing
\item
  \textbf{Model selection}: Choose algorithm
\item
  \textbf{Training}: Fit model to data
\item
  \textbf{Evaluation}: Assess performance
\item
  \textbf{Prediction}: Make forecasts
\end{enumerate}

\end{solutionbox}
\begin{mnemonicbox}
``APME - Algorithms, Preprocessing, Metrics,
Evaluation''

\end{mnemonicbox}

\end{document}
