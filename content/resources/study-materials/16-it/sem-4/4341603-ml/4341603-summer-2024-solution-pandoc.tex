\documentclass[10pt,a4paper]{article}
\input{../../../../../../latex-templates/gtu-solutions/preamble.tex}
\input{../../../../../../latex-templates/gtu-solutions/english-boxes.tex}

\begin{document}

\begin{center}
{\Huge\bfseries\color{headcolor} Subject Name Solutions}\\[5pt]
{\LARGE 4341603 -- Summer 2024}\\[3pt]
{\large Semester 1 Study Material}\\[3pt]
{\normalsize\textit{Detailed Solutions and Explanations}}
\end{center}

\vspace{10pt}

\subsection*{Question 1(a) [3 marks]}\label{q1a}

\textbf{Define Machine Learning using suitable example?}

\begin{solutionbox}

Machine Learning is a subset of artificial intelligence that enables
computers to learn and make decisions from data without being explicitly
programmed for every task.


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Key Components of Machine Learning}
\vspace{-10pt}
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Component & Description \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Data} & Input information used for training \\
\textbf{Algorithm} & Mathematical model that learns patterns \\
\textbf{Training} & Process of teaching the algorithm \\
\textbf{Prediction} & Output based on learned patterns \\
\end{longtable}
}

\textbf{Example}: Email spam detection system learns from thousands of
emails labeled as ``spam'' or ``not spam'' to automatically classify new
emails.

\end{solutionbox}
\begin{mnemonicbox}
``Data Drives Decisions'' - Data trains algorithms to
make intelligent decisions

\end{mnemonicbox}
\subsection*{Question 1(b) [4 marks]}\label{q1b}

\textbf{Explain the process of machine learning with the help of
schematic representation}

\begin{solutionbox}

The machine learning process involves systematic steps from data
collection to model deployment.

\begin{verbatim}
flowchart LR
    A[Data Collection] {-{-} B[Data Preprocessing]}
    B {-{-} C[Feature Selection]}
    C {-{-} D[Model Selection]}
    D {-{-} E[Training]}
    E {-{-} F[Validation]}
    F {-{-} G\{Performance OK?\}}
    G {-{-}|No| D}
    G {-{-}|Yes| H[Testing]}
    H {-{-} I[Deployment]}
\end{verbatim}

\textbf{Process Steps:}

\begin{itemize}
\tightlist
\item
  \textbf{Data Collection}: Gathering relevant dataset
\item
  \textbf{Preprocessing}: Cleaning and preparing data
\item
  \textbf{Training}: Teaching algorithm using training data
\item
  \textbf{Validation}: Testing model performance
\item
  \textbf{Deployment}: Using model for real predictions
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``Computers Can Truly Think'' - Collect, Clean,
Train, Test

\end{mnemonicbox}
\subsection*{Question 1(c) [7 marks]}\label{q1c}

\textbf{Explain different types of machine learning with suitable
application.}

\begin{solutionbox}

Machine learning algorithms are categorized based on learning approach
and available data.


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Types of Machine Learning}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1017}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2712}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3051}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3220}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Learning Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Data Requirement
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example Application
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Supervised} & Uses labeled data & Input-output pairs & Email
classification \\
\textbf{Unsupervised} & Finds hidden patterns & Only input data &
Customer segmentation \\
\textbf{Reinforcement} & Learns through rewards & Environment feedback &
Game playing AI \\
\end{longtable}
}

\textbf{Applications:}

\begin{itemize}
\tightlist
\item
  \textbf{Supervised Learning}: Medical diagnosis, image recognition,
  fraud detection
\item
  \textbf{Unsupervised Learning}: Market research, anomaly detection,
  recommendation systems\\
\item
  \textbf{Reinforcement Learning}: Autonomous vehicles, robotics,
  strategic games
\end{itemize}

\textbf{Diagram: Learning Types}

\begin{verbatim}
mindmap
  root((Machine Learning))
    Supervised
      Classification
      Regression
    Unsupervised
      Clustering
      Association
    Reinforcement
      Policy Learning
      Value Function
\end{verbatim}

\end{solutionbox}
\begin{mnemonicbox}
``Students Usually Remember'' - Supervised,
Unsupervised, Reinforcement

\end{mnemonicbox}
\subsection*{Question 1(c) OR [7
marks]}\label{q1c}

\textbf{What are various issues with machine learning? List three
problems that are not to be solved using machine learning.}

\begin{solutionbox}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Machine Learning Issues}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4167}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3611}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2222}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Issue Category
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Impact
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Data Quality} & Incomplete, noisy, biased data & Poor model
performance \\
\textbf{Overfitting} & Model memorizes training data & Poor
generalization \\
\textbf{Computational} & High processing requirements & Resource
constraints \\
\textbf{Interpretability} & Black box models & Lack of transparency \\
\end{longtable}
}

\textbf{Problems NOT suitable for ML:}

\begin{enumerate}
\tightlist
\item
  \textbf{Simple rule-based tasks} - Basic calculations, simple if-then
  logic
\item
  \textbf{Ethical decisions} - Moral judgments requiring human values
\item
  \textbf{Creative expression} - Original artistic creation requiring
  human emotion
\end{enumerate}

\textbf{Other Issues:}

\begin{itemize}
\tightlist
\item
  \textbf{Privacy concerns}: Sensitive data handling
\item
  \textbf{Bias propagation}: Unfair algorithmic decisions
\item
  \textbf{Feature selection}: Choosing relevant input variables
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``Data Drives Quality'' - Data quality directly
affects model quality

\end{mnemonicbox}
\subsection*{Question 2(a) [3 marks]}\label{q2a}

\textbf{Give a summarized view of different types of data in a typical
machine learning problem.}

\begin{solutionbox}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Data Types in Machine Learning}
\vspace{-10pt}
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Data Type & Description & Example \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Numerical} & Quantitative values & Age: 25, Height: 170cm \\
\textbf{Categorical} & Discrete categories & Color: Red, Blue, Green \\
\textbf{Ordinal} & Ordered categories & Rating: Poor, Good, Excellent \\
\textbf{Binary} & Two possible values & Gender: Male/Female \\
\end{longtable}
}

\textbf{Characteristics:}

\begin{itemize}
\tightlist
\item
  \textbf{Structured}: Organized in tables (databases, spreadsheets)
\item
  \textbf{Unstructured}: Images, text, audio files
\item
  \textbf{Time-series}: Data points over time
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``Numbers Count Better Than Words'' - Numerical,
Categorical, Binary, Text

\end{mnemonicbox}
\subsection*{Question 2(b) [4 marks]}\label{q2b}

\textbf{Calculate variance for both attributes. Determine which
attribute is spread out around mean.}

\begin{solutionbox}

\textbf{Given Data:}

\begin{itemize}
\tightlist
\item
  Attribute 1: 32, 37, 47, 50, 59
\item
  Attribute 2: 48, 40, 41, 47, 49
\end{itemize}

\textbf{Calculations:}

\textbf{Attribute 1:}

\begin{itemize}
\tightlist
\item
  Mean = (32+37+47+50+59)/5 = 225/5 = 45
\item
  Variance = [(32-45)^{2} + (37-45)^{2} + (47-45)^{2} + (50-45)^{2} +
  (59-45)^{2}]/5
\item
  Variance = [169 + 64 + 4 + 25 + 196]/5 = 458/5 = 91.6
\end{itemize}

\textbf{Attribute 2:}

\begin{itemize}
\tightlist
\item
  Mean = (48+40+41+47+49)/5 = 225/5 = 45\\
\item
  Variance = [(48-45)^{2} + (40-45)^{2} + (41-45)^{2} + (47-45)^{2} +
  (49-45)^{2}]/5
\item
  Variance = [9 + 25 + 16 + 4 + 16]/5 = 70/5 = 14
\end{itemize}

\textbf{Result}: Attribute 1 (variance = 91.6) is more spread out than
Attribute 2 (variance = 14).

\end{solutionbox}
\begin{mnemonicbox}
``Higher Variance Shows Spread'' - Greater variance
indicates more dispersion

\end{mnemonicbox}
\subsection*{Question 2(c) [7 marks]}\label{q2c}

\textbf{List Factors that lead to data quality issue. How to handle
outliers and missing values.}

\begin{solutionbox}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Data Quality Issues}
\vspace{-10pt}
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Factor & Cause & Solution \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Incompleteness} & Missing data collection & Imputation
techniques \\
\textbf{Inconsistency} & Different data formats & Standardization \\
\textbf{Inaccuracy} & Human/sensor errors & Validation rules \\
\textbf{Noise} & Random variations & Filtering methods \\
\end{longtable}
}

\textbf{Handling Outliers:}

\begin{itemize}
\tightlist
\item
  \textbf{Detection}: Statistical methods (Z-score, IQR)
\item
  \textbf{Treatment}: Remove, transform, or cap extreme values
\item
  \textbf{Visualization}: Box plots, scatter plots
\end{itemize}

\textbf{Handling Missing Values:}

\begin{itemize}
\tightlist
\item
  \textbf{Deletion}: Remove incomplete records
\item
  \textbf{Imputation}: Fill with mean, median, or mode
\item
  \textbf{Prediction}: Use ML to predict missing values
\end{itemize}

\textbf{Code Example:}

\begin{verbatim}
\# Handle missing values
df.fillna(df.mean())  \# Mean imputation
df.dropna()          \# Remove missing rows
\end{verbatim}

\end{solutionbox}
\begin{mnemonicbox}
``Clean Data Makes Models'' - Clean data produces
better models

\end{mnemonicbox}
\subsection*{Question 2(a) OR [3
marks]}\label{q2a}

\textbf{Give different machine learning activities.}

\begin{solutionbox}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Machine Learning Activities}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3571}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3214}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3214}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Activity
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Data Collection} & Gather relevant information & Surveys,
sensors, databases \\
\textbf{Data Preprocessing} & Clean and prepare data & Remove noise,
handle missing values \\
\textbf{Feature Engineering} & Create meaningful variables & Extract
features from raw data \\
\textbf{Model Training} & Teach algorithm patterns & Use training
dataset \\
\textbf{Model Evaluation} & Assess performance & Test accuracy,
precision, recall \\
\textbf{Model Deployment} & Put model into production & Web services,
mobile apps \\
\end{longtable}
}

\textbf{Key Activities:}

\begin{itemize}
\tightlist
\item
  \textbf{Exploratory Data Analysis}: Understanding data patterns
\item
  \textbf{Hyperparameter Tuning}: Optimizing model settings
\item
  \textbf{Cross-validation}: Robust performance assessment
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``Data Models Perform Excellently'' - Data
preparation, Model building, Performance evaluation, Execution

\end{mnemonicbox}
\subsection*{Question 2(b) OR [4
marks]}\label{q2b}

\textbf{Calculate mean and median of the following numbers:
12,15,18,20,22,24,28,30}

\begin{solutionbox}

\textbf{Given numbers:} 12, 15, 18, 20, 22, 24, 28, 30

\textbf{Mean Calculation:} Mean = (12+15+18+20+22+24+28+30)/8 = 169/8 =
21.125

\textbf{Median Calculation:}

\begin{itemize}
\tightlist
\item
  Numbers are already sorted: 12, 15, 18, 20, 22, 24, 28, 30
\item
  Even count (8 numbers)
\item
  Median = (4th number + 5th number)/2 = (20 + 22)/2 = 21
\end{itemize}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Statistical Summary}
\vspace{-10pt}
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Measure & Value & Description \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Mean} & 21.125 & Average value \\
\textbf{Median} & 21 & Middle value \\
\textbf{Count} & 8 & Total numbers \\
\end{longtable}
}

\end{solutionbox}
\begin{mnemonicbox}
``Middle Makes Median'' - Middle value gives median

\end{mnemonicbox}
\subsection*{Question 2(c) OR [7
marks]}\label{q2c}

\textbf{Write a short note on dimensionality reduction and feature
subset selection in context with data preprocessing.}

\begin{solutionbox}

\textbf{Dimensionality Reduction} removes irrelevant features and
reduces computational complexity while preserving important information.


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Dimensionality Reduction Techniques}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3793}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2759}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3448}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Technique
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Use Case
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{PCA} & Principal Component Analysis & Linear reduction \\
\textbf{LDA} & Linear Discriminant Analysis & Classification tasks \\
\textbf{t-SNE} & Non-linear embedding & Visualization \\
\textbf{Feature Selection} & Select important features & Reduce
overfitting \\
\end{longtable}
}

\textbf{Feature Subset Selection Methods:}

\begin{itemize}
\tightlist
\item
  \textbf{Filter Methods}: Statistical tests, correlation analysis
\item
  \textbf{Wrapper Methods}: Forward/backward selection
\item
  \textbf{Embedded Methods}: LASSO, Ridge regression
\end{itemize}

\textbf{Benefits:}

\begin{itemize}
\tightlist
\item
  \textbf{Computational Efficiency}: Faster training and prediction
\item
  \textbf{Storage Reduction}: Less memory requirements
\item
  \textbf{Noise Reduction}: Remove irrelevant features
\item
  \textbf{Visualization}: Enable 2D/3D plotting
\end{itemize}

\textbf{Code Example:}

\begin{verbatim}
from sklearn.decomposition import PCA
pca = PCA(n\_components=2)
reduced\_data = pca.fit\_transform(data)
\end{verbatim}

\end{solutionbox}
\begin{mnemonicbox}
``Reduce Features, Improve Performance'' - Fewer
features often lead to better models

\end{mnemonicbox}
\subsection*{Question 3(a) [3 marks]}\label{q3a}

\textbf{Does bias affect the performance of the ML model? Explain
briefly.}

\begin{solutionbox}

Yes, bias significantly affects ML model performance by creating
systematic errors in predictions.


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Types of Bias}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3438}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4062}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2500}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Bias Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Impact
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Selection Bias} & Non-representative data & Poor
generalization \\
\textbf{Confirmation Bias} & Favoring expected results & Skewed
conclusions \\
\textbf{Algorithmic Bias} & Model assumptions & Unfair predictions \\
\end{longtable}
}

\textbf{Effects on Performance:}

\begin{itemize}
\tightlist
\item
  \textbf{Underfitting}: High bias leads to oversimplified models
\item
  \textbf{Poor Accuracy}: Systematic errors reduce overall performance
\item
  \textbf{Unfair Decisions}: Biased models discriminate against groups
\end{itemize}

\textbf{Mitigation Strategies:}

\begin{itemize}
\tightlist
\item
  Diverse training data
\item
  Cross-validation techniques
\item
  Bias detection algorithms
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``Bias Breaks Better Performance'' - Bias reduces
model effectiveness

\end{mnemonicbox}
\subsection*{Question 3(b) [4 marks]}\label{q3b}

\textbf{Compare cross-validation and bootstrap sampling}

\begin{solutionbox}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Cross-validation vs Bootstrap Sampling}
\vspace{-10pt}
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Aspect & Cross-validation & Bootstrap Sampling \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Method} & Split data into folds & Sample with replacement \\
\textbf{Data Usage} & Uses all data & Creates multiple samples \\
\textbf{Purpose} & Model evaluation & Estimate uncertainty \\
\textbf{Overlap} & No overlap between sets & Allows duplicate samples \\
\end{longtable}
}

\textbf{Cross-validation:}

\begin{itemize}
\tightlist
\item
  Divides data into k equal parts
\item
  Trains on k-1 parts, tests on 1 part
\item
  Repeats k times for robust evaluation
\end{itemize}

\textbf{Bootstrap Sampling:}

\begin{itemize}
\tightlist
\item
  Creates random samples with replacement
\item
  Generates multiple datasets of same size
\item
  Estimates confidence intervals
\end{itemize}

\textbf{Applications:}

\begin{itemize}
\tightlist
\item
  \textbf{Cross-validation}: Model selection, hyperparameter tuning
\item
  \textbf{Bootstrap}: Statistical inference, confidence estimation
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``Cross Checks, Bootstrap Builds'' - Cross-validation
checks performance, Bootstrap builds confidence

\end{mnemonicbox}
\subsection*{Question 3(c) [7 marks]}\label{q3c}

\textbf{Confusion Matrix Calculation and Metrics}

\begin{solutionbox}

\textbf{Given Information:}

\begin{itemize}
\tightlist
\item
  True Positive (TP): 83 (predicted buy, actually bought)
\item
  False Positive (FP): 7 (predicted buy, didn't buy)
\item
  False Negative (FN): 5 (predicted no buy, actually bought)
\item
  True Negative (TN): 5 (predicted no buy, didn't buy)
\end{itemize}

\textbf{Confusion Matrix:}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
& Predicted Buy & Predicted No Buy \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Actually Buy} & 83 (TP) & 5 (FN) \\
\textbf{Actually No Buy} & 7 (FP) & 5 (TN) \\
\end{longtable}
}

\textbf{Calculations:}

\textbf{a) Error Rate:} Error Rate = (FP + FN) / Total = (7 + 5) / 100 =
0.12 = 12\%

\textbf{b) Precision:} Precision = TP / (TP + FP) = 83 / (83 + 7) =
83/90 = 0.922 = 92.2\%

\textbf{c) Recall:} Recall = TP / (TP + FN) = 83 / (83 + 5) = 83/88 =
0.943 = 94.3\%

\textbf{d) F-measure:} F-measure = 2 \times (Precision \times Recall) / (Precision
+ Recall) F-measure = 2 \times (0.922 \times 0.943) / (0.922 + 0.943) = 0.932 =
93.2\%


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Performance Metrics}
\vspace{-10pt}
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Metric & Value & Interpretation \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Error Rate} & 12\% & Model makes 12\% wrong predictions \\
\textbf{Precision} & 92.2\% & 92.2\% of predicted buyers actually buy \\
\textbf{Recall} & 94.3\% & Model identifies 94.3\% of actual buyers \\
\textbf{F-measure} & 93.2\% & Balanced performance measure \\
\end{longtable}
}

\end{solutionbox}
\begin{mnemonicbox}
``Perfect Recall Finds Everyone'' - Precision
measures accuracy, Recall finds all positives

\end{mnemonicbox}
\subsection*{Question 3(a) OR [3
marks]}\label{q3a}

\textbf{Define in brief: a) Target function b) Cost function c) Loss
Function}

\begin{solutionbox}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Function Definitions}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3226}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3871}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2903}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Function
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Target Function} & Ideal mapping from input to output & What we
want to learn \\
\textbf{Cost Function} & Measures overall model error & Evaluate total
performance \\
\textbf{Loss Function} & Measures error for single prediction &
Individual prediction error \\
\end{longtable}
}

\textbf{Detailed Explanation:}

\begin{itemize}
\tightlist
\item
  \textbf{Target Function}: f(x) = y, the true relationship we want to
  approximate
\item
  \textbf{Cost Function}: Average of all loss functions, J =
  (1/n)Σloss(yi, ŷi)
\item
  \textbf{Loss Function}: Error for one sample, e.g., (yi - ŷi)^{2}
\end{itemize}

\textbf{Relationship}: Cost function is typically the average of loss
functions across all training examples.

\end{solutionbox}
\begin{mnemonicbox}
``Target Costs Less'' - Target function is ideal,
Cost function measures overall error, Loss function measures individual
error

\end{mnemonicbox}
\subsection*{Question 3(b) OR [4
marks]}\label{q3b}

\textbf{Explain balanced fit, underfit and overfit}

\begin{solutionbox}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Model Fitting Types}
\vspace{-10pt}
\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Fit Type & Training Error & Validation Error & Characteristics \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Underfit} & High & High & Too simple model \\
\textbf{Balanced Fit} & Low & Low & Optimal complexity \\
\textbf{Overfit} & Very Low & High & Too complex model \\
\end{longtable}
}

\textbf{Visualization:}

\begin{center}
\textbf{Mermaid Diagram (Code)}
\begin{verbatim}
{Shaded}
{Highlighting}[]
graph LR
    A[Underfit] {-{-}{} B[Balanced Fit]}
    B {-{-}{} C[Overfit]}
    A {-{-}{} D[High Bias]}
    C {-{-}{} E[High Variance]}
    B {-{-}{} F[Optimal Performance]}
{Highlighting}
{Shaded}
\end{verbatim}
\end{center}

\textbf{Characteristics:}

\begin{itemize}
\tightlist
\item
  \textbf{Underfit}: Model too simple, cannot capture patterns
\item
  \textbf{Balanced Fit}: Right complexity, generalizes well
\item
  \textbf{Overfit}: Model too complex, memorizes training data
\end{itemize}

\textbf{Solutions:}

\begin{itemize}
\tightlist
\item
  \textbf{Underfit}: Increase model complexity, add features
\item
  \textbf{Overfit}: Regularization, cross-validation, more data
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``Balance Brings Best Results'' - Balanced models
perform best on new data

\end{mnemonicbox}
\subsection*{Question 4(a) [3 marks]}\label{q4a}

\textbf{Give classification learning steps.}

\begin{solutionbox}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Classification Learning Steps}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2143}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4643}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3214}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Step
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Data Collection} & Gather labeled examples & Provide training
material \\
\textbf{Preprocessing} & Clean and prepare data & Improve data
quality \\
\textbf{Feature Selection} & Choose relevant attributes & Reduce
complexity \\
\textbf{Model Training} & Learn from training data & Build classifier \\
\textbf{Evaluation} & Test model performance & Assess accuracy \\
\textbf{Deployment} & Use for new predictions & Practical application \\
\end{longtable}
}

\textbf{Detailed Process:}

\begin{enumerate}
\tightlist
\item
  \textbf{Prepare dataset} with input features and class labels
\item
  \textbf{Split data} into training and testing sets
\item
  \textbf{Train classifier} using training data
\item
  \textbf{Validate model} using test data
\item
  \textbf{Fine-tune parameters} for optimal performance
\end{enumerate}

\end{solutionbox}
\begin{mnemonicbox}
``Data Preparation Facilitates Model Excellence'' -
Data prep, Feature selection, Model training, Evaluation

\end{mnemonicbox}
\subsection*{Question 4(b) [4 marks]}\label{q4b}

\textbf{Linear Relationship Calculation}

\begin{solutionbox}

\textbf{Given Data:}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Hours (X) & Exam Score (Y) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
2 & 85 \\
3 & 80 \\
4 & 75 \\
5 & 70 \\
6 & 60 \\
\end{longtable}
}

\textbf{Linear Regression Calculation:}

\textbf{Step 1: Calculate means}

\begin{itemize}
\tightlist
\item
  X̄ = (2+3+4+5+6)/5 = 4
\item
  Ȳ = (85+80+75+70+60)/5 = 74
\end{itemize}

\textbf{Step 2: Calculate slope (b)}

\begin{itemize}
\tightlist
\item
  Numerator = Σ(X-X̄)(Y-Ȳ) = (2-4)(85-74) + (3-4)(80-74) + (4-4)(75-74) +
  (5-4)(70-74) + (6-4)(60-74)
\item
  = (-2)(11) + (-1)(6) + (0)(1) + (1)(-4) + (2)(-14) = -22 - 6 + 0 - 4 -
  28 = -60
\item
  Denominator = Σ(X-X̄)^{2} = (-2)^{2} + (-1)^{2} + (0)^{2} + (1)^{2} + (2)^{2} = 4 + 1 + 0
  + 1 + 4 = 10
\item
  b = -60/10 = -6
\end{itemize}

\textbf{Step 3: Calculate intercept (a)}

\begin{itemize}
\tightlist
\item
  a = Ȳ - b\timesX̄ = 74 - (-6)\times4 = 74 + 24 = 98
\end{itemize}

\textbf{Linear Equation: Y = 98 - 6X}

\textbf{Interpretation}: For every additional hour of smartphone use,
exam score decreases by 6 points.

\end{solutionbox}
\begin{mnemonicbox}
``More Phone, Less Score'' - Negative correlation
between phone use and grades

\end{mnemonicbox}
\subsection*{Question 4(c) [7 marks]}\label{q4c}

\textbf{Explain classification steps in detail}

\begin{solutionbox}

Classification is a supervised learning process that assigns input data
to predefined categories or classes.

\textbf{Detailed Classification Steps:}

\textbf{1. Problem Definition}

\begin{itemize}
\tightlist
\item
  Define classes and objectives
\item
  Identify input features and target variable
\item
  Determine success criteria
\end{itemize}

\textbf{2. Data Collection and Preparation}

\begin{verbatim}
flowchart LR
    A[Raw Data] {-{-} B[Data Cleaning]}
    B {-{-} C[Handle Missing Values]}
    C {-{-} D[Remove Outliers]}
    D {-{-} E[Feature Engineering]}
    E {-{-} F[Data Splitting]}
\end{verbatim}

\textbf{3. Feature Engineering}

\begin{itemize}
\tightlist
\item
  \textbf{Feature Selection}: Choose relevant attributes
\item
  \textbf{Feature Extraction}: Create new meaningful features
\item
  \textbf{Normalization}: Scale features to similar ranges
\end{itemize}

\textbf{4. Model Selection and Training}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Common Classification Algorithms}
\vspace{-10pt}
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Algorithm & Best For & Advantages \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Decision Tree} & Interpretable rules & Easy to understand \\
\textbf{SVM} & High-dimensional data & Good generalization \\
\textbf{Neural Networks} & Complex patterns & High accuracy \\
\textbf{Naive Bayes} & Text classification & Fast training \\
\end{longtable}
}

\textbf{5. Model Evaluation}

\begin{itemize}
\tightlist
\item
  \textbf{Confusion Matrix}: Detailed performance analysis
\item
  \textbf{Cross-validation}: Robust performance estimation
\item
  \textbf{Metrics}: Accuracy, Precision, Recall, F1-score
\end{itemize}

\textbf{6. Hyperparameter Tuning}

\begin{itemize}
\tightlist
\item
  Grid search for optimal parameters
\item
  Validation set for parameter selection
\end{itemize}

\textbf{7. Final Evaluation and Deployment}

\begin{itemize}
\tightlist
\item
  Test on unseen data
\item
  Deploy model for production use
\item
  Monitor performance over time
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``Proper Data Modeling Evaluates Performance
Thoroughly'' - Problem definition, Data prep, Modeling, Evaluation,
Performance testing, Tuning

\end{mnemonicbox}
\subsection*{Question 4(a) OR [3
marks]}\label{q4a}

\textbf{Does the choice of the k value influence the performance of the
KNN algorithm? Explain briefly}

\begin{solutionbox}

Yes, the k value significantly influences KNN algorithm performance by
affecting the decision boundary and model complexity.


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{K Value Impact}
\vspace{-10pt}
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
K Value & Effect & Performance \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Small K (k=1)} & Sensitive to noise & High variance, low bias \\
\textbf{Medium K} & Balanced decisions & Optimal performance \\
\textbf{Large K} & Smooth boundaries & Low variance, high bias \\
\end{longtable}
}

\textbf{Impact Analysis:}

\begin{itemize}
\tightlist
\item
  \textbf{k=1}: May overfit to training data, sensitive to outliers
\item
  \textbf{Optimal k}: Usually odd number, balances bias-variance
  tradeoff
\item
  \textbf{Large k}: May underfit, loses local patterns
\end{itemize}

\textbf{Selection Strategy:}

\begin{itemize}
\tightlist
\item
  Use cross-validation to find optimal k
\item
  Try k = \sqrtn as starting point
\item
  Consider computational cost vs accuracy
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``Small K Varies, Large K Smooths'' - Small k creates
variance, large k creates smooth boundaries

\end{mnemonicbox}
\subsection*{Question 4(b) OR [4
marks]}\label{q4b}

\textbf{Define Support Vectors in the SVM model.}

\begin{solutionbox}

Support Vectors are the critical data points that lie closest to the
decision boundary (hyperplane) in Support Vector Machine algorithm.


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Support Vector Characteristics}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2424}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3939}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3636}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Importance
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Location} & Closest points to hyperplane & Define decision
boundary \\
\textbf{Distance} & Equal distance from boundary & Maximize margin \\
\textbf{Role} & Support the hyperplane & Determine optimal separation \\
\textbf{Sensitivity} & Removing them changes model & Critical for model
structure \\
\end{longtable}
}

\textbf{Key Properties:}

\begin{itemize}
\tightlist
\item
  \textbf{Margin Definition}: Support vectors determine the maximum
  margin between classes
\item
  \textbf{Model Dependency}: Only support vectors affect the final model
\item
  \textbf{Boundary Formation}: Create the optimal separating hyperplane
\end{itemize}

\textbf{Diagram:}

\begin{verbatim}
      Class A  |     |  Class B
         o     |     |     x
           o   |     |   x
         o   O |     | X   x
           o   |     |   x  
         o     |     |     x
              
         Support Vectors: O and X
         Hyperplane: {-{-}{-}|{-}{-}{-}}
\end{verbatim}

\textbf{Mathematical Significance}: Support vectors satisfy the
constraint yi(w·xi + b) = 1, where they lie exactly on the margin
boundary.

\end{solutionbox}
\begin{mnemonicbox}
``Support Vectors Support Decisions'' - These vectors
support the decision boundary

\end{mnemonicbox}
\subsection*{Question 4(c) OR [7
marks]}\label{q4c}

\textbf{Explain logistic regression in detail.}

\begin{solutionbox}

Logistic Regression is a statistical method used for binary
classification that models the probability of class membership using the
logistic function.

\textbf{Mathematical Foundation:}

\textbf{Sigmoid Function:} σ(z) = 1 / (1 + e\^{}(-z)) where z = β_{0} +
β_{1}x_{1} + β_{2}x_{2} + \ldots{} + β_{n}x_{n}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Linear vs Logistic Regression}
\vspace{-10pt}
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Aspect & Linear Regression & Logistic Regression \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Output} & Continuous values & Probabilities (0-1) \\
\textbf{Function} & Linear & Sigmoid (S-curve) \\
\textbf{Purpose} & Prediction & Classification \\
\textbf{Error Function} & Mean Squared Error & Log-likelihood \\
\end{longtable}
}

\textbf{Key Components:}

\textbf{1. Logistic Function Properties:}

\begin{itemize}
\tightlist
\item
  \textbf{S-shaped curve}: Smooth transition between 0 and 1
\item
  \textbf{Asymptotes}: Approaches 0 and 1 but never reaches them
\item
  \textbf{Monotonic}: Always increasing function
\end{itemize}

\textbf{2. Model Training:}

\begin{itemize}
\tightlist
\item
  \textbf{Maximum Likelihood Estimation}: Find parameters that maximize
  probability of observed data
\item
  \textbf{Gradient Descent}: Iterative optimization algorithm
\item
  \textbf{Cost Function}: Log-loss or cross-entropy
\end{itemize}

\textbf{3. Decision Making:}

\begin{itemize}
\tightlist
\item
  \textbf{Threshold}: Typically 0.5 for binary classification
\item
  \textbf{Probability Output}: P(y=1\textbar x) gives class probability
\item
  \textbf{Decision Rule}: Classify as positive if P(y=1\textbar x)
  \textgreater{} 0.5
\end{itemize}

\textbf{Advantages:}

\begin{itemize}
\tightlist
\item
  \textbf{Probabilistic Output}: Provides confidence in predictions
\item
  \textbf{No Assumptions}: About distribution of independent variables
\item
  \textbf{Less Overfitting}: Compared to complex models
\item
  \textbf{Fast Training}: Efficient computation
\end{itemize}

\textbf{Applications:}

\begin{itemize}
\tightlist
\item
  Medical diagnosis
\item
  Marketing response prediction
\item
  Credit approval decisions
\item
  Email spam detection
\end{itemize}

\textbf{Code Example:}

\begin{verbatim}
from sklearn.linear\_model import LogisticRegression
model = LogisticRegression()
model.fit(X\_train, y\_train)
predictions = model.predict(X\_test)
probabilities = model.predict\_proba(X\_test)
\end{verbatim}

\end{solutionbox}
\begin{mnemonicbox}
``Sigmoid Squashes Infinite Input'' - Sigmoid
function converts any real number to probability

\end{mnemonicbox}
\subsection*{Question 5(a) [3 marks]}\label{q5a}

\textbf{Write a short note on Matplotlib python library.}

\begin{solutionbox}

Matplotlib is a comprehensive Python library for creating static,
animated, and interactive visualizations in data science and machine
learning.


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Matplotlib Key Features}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Feature
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Pyplot} & MATLAB-like plotting interface & Line plots, scatter
plots \\
\textbf{Object-oriented} & Advanced customization & Figure and axes
objects \\
\textbf{Multiple formats} & Save in various formats & PNG, PDF, SVG,
EPS \\
\textbf{Subplots} & Multiple plots in one figure & Grid arrangements \\
\end{longtable}
}

\textbf{Common Plot Types:}

\begin{itemize}
\tightlist
\item
  \textbf{Line Plot}: Trends over time
\item
  \textbf{Scatter Plot}: Relationship between variables\\
\item
  \textbf{Histogram}: Data distribution
\item
  \textbf{Bar Chart}: Categorical comparisons
\item
  \textbf{Box Plot}: Statistical summaries
\end{itemize}

\textbf{Basic Usage:}

\begin{verbatim}
import matplotlib.pyplot as plt
plt.plot(x, y)
plt.xlabel({X Label})
plt.ylabel({Y Label})
plt.title({Plot Title})
plt.show()
\end{verbatim}

\textbf{Applications}: Data exploration, model performance
visualization, presentation graphics

\end{solutionbox}
\begin{mnemonicbox}
``Matplotlib Makes Pretty Plots'' - Essential tool
for data visualization

\end{mnemonicbox}
\subsection*{Question 5(b) [4 marks]}\label{q5b}

\textbf{K-means clustering for two-dimensional data}

\begin{solutionbox}

\textbf{Given Points:}
\{(2,3),(3,3),(4,3),(5,3),(6,3),(7,3),(8,3),(25,20),(26,20),(27,20),(28,20),(29,20),(30,20)\}

\textbf{K-means Algorithm Steps:}

\textbf{Step 1: Initialize centroids}

\begin{itemize}
\tightlist
\item
  Cluster 1: (4, 3) - chosen from left group
\item
  Cluster 2: (27, 20) - chosen from right group
\end{itemize}

\textbf{Step 2: Assign points to nearest centroid}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Point Assignments}
\vspace{-10pt}
\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Point & Distance to C1 & Distance to C2 & Assigned Cluster \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
(2,3) & 2.0 & 25.8 & Cluster 1 \\
(3,3) & 1.0 & 24.8 & Cluster 1 \\
(4,3) & 0.0 & 23.8 & Cluster 1 \\
(5,3) & 1.0 & 22.8 & Cluster 1 \\
(6,3) & 2.0 & 21.8 & Cluster 1 \\
(7,3) & 3.0 & 20.8 & Cluster 1 \\
(8,3) & 4.0 & 19.8 & Cluster 1 \\
(25,20) & 23.8 & 2.0 & Cluster 2 \\
(26,20) & 24.8 & 1.0 & Cluster 2 \\
(27,20) & 25.8 & 0.0 & Cluster 2 \\
(28,20) & 26.8 & 1.0 & Cluster 2 \\
(29,20) & 27.8 & 2.0 & Cluster 2 \\
(30,20) & 28.8 & 3.0 & Cluster 2 \\
\end{longtable}
}

\textbf{Step 3: Update centroids}

\begin{itemize}
\tightlist
\item
  New C1 = ((2+3+4+5+6+7+8)/7, (3+3+3+3+3+3+3)/7) = (5, 3)
\item
  New C2 = ((25+26+27+28+29+30)/6, (20+20+20+20+20+20)/6) = (27.5, 20)
\end{itemize}

\textbf{Final Clusters:}

\begin{itemize}
\tightlist
\item
  \textbf{Cluster 1}: \{(2,3),(3,3),(4,3),(5,3),(6,3),(7,3),(8,3)\}
\item
  \textbf{Cluster 2}:
  \{(25,20),(26,20),(27,20),(28,20),(29,20),(30,20)\}
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``Centroids Attract Nearest Neighbors'' - Points join
closest centroid

\end{mnemonicbox}
\subsection*{Question 5(c) [7 marks]}\label{q5c}

\textbf{Give functions and its use of Scikit-learn for: a. Data
Preprocessing b. Model Selection c.~Model Evaluation and Metrics}

\begin{solutionbox}

Scikit-learn provides comprehensive tools for machine learning workflow
from data preprocessing to model evaluation.

\textbf{a) Data Preprocessing Functions:}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Preprocessing Functions}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3030}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2727}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4242}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Function
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example Usage
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{StandardScaler()} & Normalize features & Remove mean, unit
variance \\
\texttt{MinMaxScaler()} & Scale to range [0,1] & Feature scaling \\
\texttt{LabelEncoder()} & Encode categorical labels & Convert text to
numbers \\
\texttt{OneHotEncoder()} & Create dummy variables & Handle categorical
features \\
\texttt{train\_test\_split()} & Split dataset & Training/testing
division \\
\end{longtable}
}

\textbf{Code Example:}

\begin{verbatim}
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X\_scaled = scaler.fit\_transform(X)
\end{verbatim}

\textbf{b) Model Selection Functions:}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Model Selection Tools}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3125}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2812}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4062}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Function
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Application
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{GridSearchCV()} & Hyperparameter tuning & Find optimal
parameters \\
\texttt{RandomizedSearchCV()} & Random parameter search & Faster
parameter optimization \\
\texttt{cross\_val\_score()} & Cross-validation & Model performance
evaluation \\
\texttt{StratifiedKFold()} & Stratified sampling & Balanced
cross-validation \\
\texttt{Pipeline()} & Combine preprocessing and modeling & Streamlined
workflow \\
\end{longtable}
}

\textbf{Code Example:}

\begin{verbatim}
from sklearn.model\_selection import GridSearchCV
param\_grid = \{{C}: [0.1, 1, 10]\}
grid\_search = GridSearchCV(SVM(), param\_grid, cv=5)
grid\_search.fit(X\_train, y\_train)
\end{verbatim}

\textbf{c) Model Evaluation and Metrics Functions:}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Evaluation Metrics}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3448}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3103}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3448}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Function
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Use Case
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{accuracy\_score()} & Overall accuracy & General
classification \\
\texttt{precision\_score()} & Positive prediction accuracy & Minimize
false positives \\
\texttt{recall\_score()} & True positive rate & Minimize false
negatives \\
\texttt{f1\_score()} & Harmonic mean of precision/recall & Balanced
metric \\
\texttt{confusion\_matrix()} & Detailed error analysis & Understanding
mistakes \\
\texttt{classification\_report()} & Comprehensive metrics & Complete
evaluation \\
\texttt{roc\_auc\_score()} & Area under ROC curve & Binary
classification \\
\end{longtable}
}

\textbf{Code Example:}

\begin{verbatim}
from sklearn.metrics import classification\_report
print(classification\_report(y\_true, y\_pred))
\end{verbatim}

\textbf{Workflow Integration:}

\begin{itemize}
\tightlist
\item
  \textbf{Preprocessing}: Clean and prepare data
\item
  \textbf{Model Selection}: Choose and tune algorithms
\item
  \textbf{Evaluation}: Assess performance comprehensively
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``Preprocess, Select, Evaluate'' - Complete ML
workflow in Scikit-learn

\end{mnemonicbox}
\subsection*{Question 5(a) OR [3
marks]}\label{q5a}

\textbf{List out the major features of Numpy.}

\begin{solutionbox}

NumPy (Numerical Python) is the fundamental package for scientific
computing in Python, providing powerful array operations and
mathematical functions.


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Major NumPy Features}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2903}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4194}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2903}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Feature
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Benefit
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{N-dimensional Arrays} & Efficient array objects & Fast
mathematical operations \\
\textbf{Broadcasting} & Operations on different sized arrays & Flexible
computations \\
\textbf{Linear Algebra} & Matrix operations, decompositions & Scientific
computing \\
\textbf{Random Numbers} & Random sampling and distributions &
Statistical simulations \\
\textbf{Integration} & Works with C/C++/Fortran & High performance \\
\end{longtable}
}

\textbf{Key Capabilities:}

\begin{itemize}
\tightlist
\item
  \textbf{Mathematical Functions}: Trigonometric, logarithmic,
  exponential
\item
  \textbf{Array Manipulation}: Reshaping, splitting, joining arrays
\item
  \textbf{Indexing}: Advanced slicing and boolean indexing
\item
  \textbf{Memory Efficiency}: Optimized data storage
\end{itemize}

\textbf{Applications}: Data analysis, machine learning, image
processing, scientific research

\end{solutionbox}
\begin{mnemonicbox}
``Numbers Need Numpy's Power'' - Essential for
numerical computations

\end{mnemonicbox}
\subsection*{Question 5(b) OR [4
marks]}\label{q5b}

\textbf{K-means clustering for one-dimensional data}

\begin{solutionbox}

\textbf{Given Dataset:} \{1,2,4,5,7,8,10,11,12,14,15,17\}

\textbf{K-means Algorithm for 3 clusters:}

\textbf{Step 1: Initialize centroids}

\begin{itemize}
\tightlist
\item
  C1 = 3 (around early values)
\item
  C2 = 9 (around middle values)\\
\item
  C3 = 15 (around later values)
\end{itemize}

\textbf{Step 2: Assign points to nearest centroid}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Point Assignments (Iteration 1)}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.0986}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2113}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2254}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2254}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2394}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Point
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Distance to C1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Distance to C2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Distance to C3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Assigned Cluster
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 2 & 8 & 14 & Cluster 1 \\
2 & 1 & 7 & 13 & Cluster 1 \\
4 & 1 & 5 & 11 & Cluster 1 \\
5 & 2 & 4 & 10 & Cluster 1 \\
7 & 4 & 2 & 8 & Cluster 2 \\
8 & 5 & 1 & 7 & Cluster 2 \\
10 & 7 & 1 & 5 & Cluster 2 \\
11 & 8 & 2 & 4 & Cluster 2 \\
12 & 9 & 3 & 3 & Cluster 2 \\
14 & 11 & 5 & 1 & Cluster 3 \\
15 & 12 & 6 & 0 & Cluster 3 \\
17 & 14 & 8 & 2 & Cluster 3 \\
\end{longtable}
}

\textbf{Step 3: Update centroids}

\begin{itemize}
\tightlist
\item
  New C1 = (1+2+4+5)/4 = 3
\item
  New C2 = (7+8+10+11+12)/5 = 9.6
\item
  New C3 = (14+15+17)/3 = 15.33
\end{itemize}

\textbf{Final Clusters:}

\begin{itemize}
\tightlist
\item
  \textbf{Cluster 1}: \{1, 2, 4, 5\}
\item
  \textbf{Cluster 2}: \{7, 8, 10, 11, 12\}
\item
  \textbf{Cluster 3}: \{14, 15, 17\}
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``Groups Gather by Distance'' - Similar points form
natural clusters

\end{mnemonicbox}
\subsection*{Question 5(c) OR [7
marks]}\label{q5c}

\textbf{Give function and its use of Pandas library for: a. Data
Preprocessing b. Data Inspection c.~Data Cleaning and Transformation}

\begin{solutionbox}

Pandas is a powerful Python library for data manipulation and analysis,
providing high-level data structures and operations.

\textbf{a) Data Preprocessing Functions:}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Preprocessing Functions}
\vspace{-10pt}
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Function & Purpose & Example \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{read\_csv()} & Load CSV files &
\texttt{pd.read\_csv(\textquotesingle{}data.csv\textquotesingle{})} \\
\texttt{head()} & View first n rows & \texttt{df.head(10)} \\
\texttt{tail()} & View last n rows & \texttt{df.tail(5)} \\
\texttt{sample()} & Random sampling & \texttt{df.sample(100)} \\
\texttt{set\_index()} & Set column as index &
\texttt{df.set\_index(\textquotesingle{}id\textquotesingle{})} \\
\end{longtable}
}

\textbf{b) Data Inspection Functions:}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Inspection Functions}
\vspace{-10pt}
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Function & Purpose & Information Provided \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{info()} & Dataset overview & Data types, memory usage \\
\texttt{describe()} & Statistical summary & Mean, std, min, max \\
\texttt{shape} & Dataset dimensions & (rows, columns) \\
\texttt{dtypes} & Data types & Column data types \\
\texttt{isnull()} & Missing values & Boolean mask for nulls \\
\texttt{value\_counts()} & Count unique values & Frequency
distribution \\
\texttt{corr()} & Correlation matrix & Feature relationships \\
\end{longtable}
}

\textbf{Code Example:}

\begin{verbatim}
\# Data inspection
print(df.info())
print(df.describe())
print(df.isnull().sum())
\end{verbatim}

\textbf{c) Data Cleaning and Transformation Functions:}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Cleaning Functions}
\vspace{-10pt}
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Function & Purpose & Usage \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{dropna()} & Remove missing values & \texttt{df.dropna()} \\
\texttt{fillna()} & Fill missing values & \texttt{df.fillna(0)} \\
\texttt{drop\_duplicates()} & Remove duplicate rows &
\texttt{df.drop\_duplicates()} \\
\texttt{replace()} & Replace values &
\texttt{df.replace(\textquotesingle{}old\textquotesingle{},\ \textquotesingle{}new\textquotesingle{})} \\
\texttt{astype()} & Change data types &
\texttt{df[\textquotesingle{}col\textquotesingle{}].astype(\textquotesingle{}int\textquotesingle{})} \\
\texttt{apply()} & Apply function to data &
\texttt{df.apply(lambda\ x:\ x*2)} \\
\texttt{groupby()} & Group data &
\texttt{df.groupby(\textquotesingle{}category\textquotesingle{})} \\
\texttt{merge()} & Join datasets & \texttt{pd.merge(df1,\ df2)} \\
\texttt{pivot()} & Reshape data &
\texttt{df.pivot(columns=\textquotesingle{}col\textquotesingle{})} \\
\end{longtable}
}

\textbf{Advanced Operations:}

\begin{itemize}
\tightlist
\item
  \textbf{String Operations}: \texttt{str.contains()},
  \texttt{str.replace()}
\item
  \textbf{Date Operations}: \texttt{to\_datetime()}, \texttt{dt.year}
\item
  \textbf{Categorical Data}: \texttt{pd.Categorical()}
\end{itemize}

\textbf{Workflow Example:}

\begin{verbatim}
\# Complete preprocessing pipeline
df = pd.read\_csv({data.csv})
df = df.dropna()
df[{category}] = df[{category}].astype({category})
df\_grouped = df.groupby({type}).mean()
\end{verbatim}

\textbf{Benefits:}

\begin{itemize}
\tightlist
\item
  \textbf{Intuitive Syntax}: Easy to learn and use
\item
  \textbf{Performance}: Optimized for large datasets
\item
  \textbf{Integration}: Works well with NumPy, Matplotlib
\item
  \textbf{Flexibility}: Handles various data formats
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``Pandas Processes Data Perfectly'' - Comprehensive
data manipulation tool

\end{mnemonicbox}

\end{document}
