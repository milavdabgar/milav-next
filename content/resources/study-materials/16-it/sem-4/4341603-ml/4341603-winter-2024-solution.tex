\documentclass{article}
\input{/Users/milav/Code/milav-next/latex-templates/gtu-solutions/preamble.tex}
\input{/Users/milav/Code/milav-next/latex-templates/gtu-solutions/english-boxes.tex}
\input{/Users/milav/Code/milav-next/latex-templates/gtu-solutions/commands.tex}

\title{Fundamentals of Machine Learning (4341603) - Winter 2024 Solution}
\date{November 28, 2024}

\begin{document}
\maketitle

\questionmarks{1(a)}{3}{Describe human learning in brief.}
\begin{solutionbox}
\textbf{Human learning} is the process by which humans acquire knowledge, skills, and behaviors through experience, practice, and instruction.

\begin{center}
\captionof{table}{Human Learning Process}
\begin{tabulary}{\linewidth}{L L}
\hline
\textbf{Aspect} & \textbf{Description} \\
\hline
\textbf{Observation} & Gathering information from environment \\
\textbf{Experience} & Learning through trial and error \\
\textbf{Practice} & Repetition to improve skills \\
\textbf{Memory} & Storing and retrieving information \\
\hline
\end{tabulary}
\end{center}

\begin{itemize}
    \item \textbf{Learning Types}: Visual, auditory, kinesthetic learning styles.
    \item \textbf{Feedback Loop}: Humans learn from mistakes and successes.
    \item \textbf{Adaptation}: Ability to apply knowledge to new situations.
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Observe, Experience, Practice, Memory, Adapt (OEPMA)}
\end{mnemonicbox}

\questionmarks{1(b)}{4}{Differentiate: Supervised Learning v/s Unsupervised Learning}
\begin{solutionbox}
\begin{center}
\captionof{table}{Supervised vs Unsupervised Learning}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{Parameter} & \textbf{Supervised Learning} & \textbf{Unsupervised Learning} \\
\hline
\textbf{Training Data} & Labeled data (input-output pairs) & Unlabeled data (only inputs) \\
\textbf{Goal} & Predict output for new inputs & Find hidden patterns \\
\textbf{Examples} & Classification, Regression & Clustering, Association \\
\textbf{Feedback} & Direct feedback available & No direct feedback \\
\hline
\end{tabulary}
\end{center}

\begin{itemize}
    \item \textbf{Supervised}: Teacher guides learning with correct answers.
    \item \textbf{Unsupervised}: Self-discovery of patterns without guidance.
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{SL-Labels, UL-Unknown}
\end{mnemonicbox}

\questionmarks{1(c)}{7}{List out machine learning activities. Explain each in detail.}
\begin{solutionbox}
\begin{center}
\captionof{table}{Machine Learning Activities}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{Activity} & \textbf{Purpose} & \textbf{Description} \\
\hline
\textbf{Data Collection} & Gather raw data & Collecting relevant data from various sources \\
\textbf{Data Preprocessing} & Clean and prepare data & Handling missing values, normalization \\
\textbf{Feature Selection} & Choose important features & Selecting relevant attributes for learning \\
\textbf{Model Training} & Build learning model & Training algorithm on prepared dataset \\
\textbf{Model Evaluation} & Assess performance & Testing model accuracy and effectiveness \\
\textbf{Model Deployment} & Put model to use & Implementing model in real-world applications \\
\hline
\end{tabulary}
\end{center}

\begin{center}
\begin{tikzpicture}[node distance=1.5cm, auto]
    \node [gtu block] (Data) {Data Collection};
    \node [gtu block, right=of Data] (Prep) {Preprocessing};
    \node [gtu block, right=of Prep] (Feat) {Feature Selection};
    \node [gtu block, below=1cm of Data] (Train) {Model Training};
    \node [gtu block, right=of Train] (Eval) {Evaluation};
    \node [gtu block, right=of Eval] (Dep) {Deployment};
    \node [gtu block, below=1cm of Train] (Mon) {Monitoring};
    
    \path [gtu arrow] (Data) -- (Prep);
    \path [gtu arrow] (Prep) -- (Feat);
    \path [gtu arrow] (Feat) -- (Train);
    \path [gtu arrow] (Train) -- (Eval);
    \path [gtu arrow] (Eval) -- (Dep);
    \path [gtu arrow] (Dep) -- (Mon);
\end{tikzpicture}
\captionof{figure}{Machine Learning Activity Flow}
\end{center}

\begin{itemize}
    \item \textbf{Iterative Process}: Activities repeat for model improvement.
    \item \textbf{Quality Control}: Each step ensures better model performance.
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Collect, Preprocess, Feature, Train, Evaluate, Deploy, Monitor (CPFTEDM)}
\end{mnemonicbox}

\questionmarks{1(c) OR}{7}{Find mean, median, and mode for the following data: 1, 1, 1, 2, 4, 5, 5, 6, 6, 7, 7, 7, 7, 8, 9, 10, 11}
\begin{solutionbox}
\textbf{Data}: 1, 1, 1, 2, 4, 5, 5, 6, 6, 7, 7, 7, 7, 8, 9, 10, 11 (Sorted, N=17)

\begin{center}
\captionof{table}{Data Analysis}
\begin{tabulary}{\linewidth}{L L L L}
\hline
\textbf{Statistic} & \textbf{Formula} & \textbf{Calculation} & \textbf{Result} \\
\hline
\textbf{Mean} & Sum/Count & 100 / 17 & 5.88 \\
\textbf{Median} & Middle value & 9th position & 6 \\
\textbf{Mode} & Most frequent & Value 7 (4 times) & 7 \\
\hline
\end{tabulary}
\end{center}

\textbf{Step-by-step Calculation:}
\begin{itemize}
    \item \textbf{Count (N)}: 17 values
    \item \textbf{Sum}: \(1+1+1+2+4+5+5+6+6+7+7+7+7+8+9+10+11 = 100\)
    \item \textbf{Mean}: \(100/17 = 5.88\)
    \item \textbf{Median}: Odd number of values, so \((N+1)/2 = 9\)th value. The 9th value in the sorted list is 6.
    \item \textbf{Mode}: The number 7 appears most frequently (4 times).
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Mean=Average, Median=Middle, Mode=Most frequent (MMM)}
\end{mnemonicbox}

\questionmarks{2(a)}{3}{Write down steps to use hold out method for model training.}
\begin{solutionbox}
\begin{center}
\captionof{table}{Hold Out Method Steps}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{Step} & \textbf{Action} & \textbf{Purpose} \\
\hline
\textbf{1} & Split dataset (70-80\% train, 20-30\% test) & Separate data for training and evaluation \\
\textbf{2} & Train model on training set & Build learning algorithm \\
\textbf{3} & Test model on testing set & Evaluate model performance \\
\hline
\end{tabulary}
\end{center}

\begin{itemize}
    \item \textbf{Random Split}: Ensure representative distribution in both sets.
    \item \textbf{No Overlap}: Testing data never used in training.
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Split, Train, Test (STT)}
\end{mnemonicbox}

\questionmarks{2(b)}{4}{Explain structure of confusion matrix.}
\begin{solutionbox}
\textbf{Confusion Matrix Structure}

\begin{center}
\begin{tabulary}{\linewidth}{L L L}
\hline
 & \textbf{Predicted Positive} & \textbf{Predicted Negative} \\
\hline
\textbf{Actual Positive} & True Positive (TP) & False Negative (FN) \\
\textbf{Actual Negative} & False Positive (FP) & True Negative (TN) \\
\hline
\end{tabulary}
\captionof{table}{Confusion Matrix Layout}
\end{center}

\textbf{Components Explanation:}
\begin{itemize}
    \item \textbf{TP}: Correctly predicted positive cases.
    \item \textbf{TN}: Correctly predicted negative cases.
    \item \textbf{FP}: Incorrectly predicted as positive (Type I error).
    \item \textbf{FN}: Incorrectly predicted as negative (Type II error).
\end{itemize}

\textbf{Performance Metrics:}
\begin{itemize}
    \item \textbf{Accuracy} = \((TP+TN)/(TP+TN+FP+FN)\)
    \item \textbf{Precision} = \(TP/(TP+FP)\)
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{TPFN-FPTN for matrix positions}
\end{mnemonicbox}

\questionmarks{2(c)}{7}{Define data pre-processing. Explain various methods used in data pre-processing.}
\begin{solutionbox}
\textbf{Data pre-processing} is the technique of preparing raw data by cleaning, transforming, and organizing it for machine learning algorithms.

\begin{center}
\captionof{table}{Data Pre-processing Methods}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{Method} & \textbf{Purpose} & \textbf{Techniques} \\
\hline
\textbf{Data Cleaning} & Remove noise/inconsistencies & Handle missing values, remove duplicates \\
\textbf{Data Transformation} & Convert data format & Normalization, standardization \\
\textbf{Data Reduction} & Reduce dataset size & Feature selection, dimensionality reduction \\
\textbf{Data Integration} & Combine multiple sources & Merge datasets, resolve conflicts \\
\hline
\end{tabulary}
\end{center}

\begin{center}
\begin{tikzpicture}[node distance=1.5cm, auto]
    \node [gtu block] (Raw) {Raw Data};
    \node [gtu block, right=of Raw] (Clean) {Cleaning};
    \node [gtu block, right=of Clean] (Trans) {Transformation};
    \node [gtu block, right=of Trans] (Red) {Reduction};
    \node [gtu block, right=of Red] (Out) {Clean Data};
    
    \path [gtu arrow] (Raw) -- (Clean);
    \path [gtu arrow] (Clean) -- (Trans);
    \path [gtu arrow] (Trans) -- (Red);
    \path [gtu arrow] (Red) -- (Out);
\end{tikzpicture}
\captionof{figure}{Preprocessing Steps}
\end{center}

\textbf{Key Techniques:}
\begin{itemize}
    \item \textbf{Missing Values}: Use mean, median, or mode for imputation.
    \item \textbf{Outliers}: Detect and handle extreme values.
    \item \textbf{Feature Scaling}: Normalize data to same scale.
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Clean, Transform, Reduce, Integrate (CTRI)}
\end{mnemonicbox}

\questionmarks{2(a) OR}{3}{Explain histogram with suitable example.}
\begin{solutionbox}
A \textbf{Histogram} is a graphical representation showing the frequency distribution of numerical data by dividing it into bins.

\begin{center}
\captionof{table}{Histogram Components}
\begin{tabulary}{\linewidth}{L L}
\hline
\textbf{Component} & \textbf{Description} \\
\hline
\textbf{X-axis} & Data ranges (bins) \\
\textbf{Y-axis} & Frequency of occurrence \\
\textbf{Bars} & Height represents frequency \\
\hline
\end{tabulary}
\end{center}

\textbf{Example}: Student marks distribution.
\begin{itemize}
    \item Bins: 0-20, 21-40, 41-60, 61-80, 81-100.
    \item Heights show number of students in each range.
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Bins, Axes, Range (BAR)}
\end{mnemonicbox}

\questionmarks{2(b) OR}{4}{Relate the appropriate data type of following examples: i) Gender of a person ii) Rank of students iii) Price of a home iv) Color of a flower}
\begin{solutionbox}
\begin{center}
\captionof{table}{Data Types Classification}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{Example} & \textbf{Data Type} & \textbf{Characteristics} \\
\hline
\textbf{Gender of person} & Nominal Categorical & No natural order (Male/Female) \\
\textbf{Rank of students} & Ordinal Categorical & Has meaningful order (1st, 2nd) \\
\textbf{Price of home} & Continuous Numerical & Can take any value within range \\
\textbf{Color of flower} & Nominal Categorical & No natural order (Red, Blue) \\
\hline
\end{tabulary}
\end{center}

\begin{itemize}
    \item \textbf{Categorical}: distinct categories.
    \item \textbf{Numerical}: mathematical operations possible.
    \item \textbf{Ordinal}: categories with sequence.
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Nominal, Ordinal, Continuous (NOCO)}
\end{mnemonicbox}

\questionmarks{2(c) OR}{7}{Describe K-fold cross validation in details.}
\begin{solutionbox}
\textbf{K-fold cross validation} is a model evaluation technique that divides dataset into K equal parts for robust performance assessment.

\begin{center}
\captionof{table}{K-fold Process}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{Step} & \textbf{Action} & \textbf{Purpose} \\
\hline
\textbf{1} & Divide data into K equal folds & Create K subsets \\
\textbf{2} & Use K-1 folds for training & Train model \\
\textbf{3} & Use 1 fold for testing & Evaluate performance \\
\textbf{4} & Repeat K times & Each fold acts as test set once \\
\textbf{5} & Average all results & Final performance metric \\
\hline
\end{tabulary}
\end{center}

\begin{center}
\begin{tikzpicture}[node distance=1.5cm, auto]
    \node [gtu block] (Data) {Dataset};
    \node [gtu block, right=of Data] (Folds) {K Folds};
    \node [gtu block, below=1cm of Data] (Train) {Train K-1};
    \node [gtu block, right=of Train] (Test) {Test 1};
    \node [gtu block, right=of Test] (Avg) {Average};
    
    \path [gtu arrow] (Data) -- (Folds);
    \path [gtu arrow] (Folds) -- (Train);
    \path [gtu arrow] (Train) -- (Test);
    \path [gtu arrow] (Test) -- node[above]{Repeat K} (Avg);
\end{tikzpicture}
\captionof{figure}{K-Fold Cross Validation}
\end{center}

\textbf{Advantages:}
\begin{itemize}
    \item \textbf{Robust Evaluation}: Every data point used for both training and testing.
    \item \textbf{Reduced Overfitting}: Multiple validation rounds.
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Divide, Use, Repeat, Average, Test (DURAT)}
\end{mnemonicbox}

\questionmarks{3(a)}{3}{List out applications of regression.}
\begin{solutionbox}
\begin{center}
\captionof{table}{Regression Applications}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{Domain} & \textbf{Application} & \textbf{Purpose} \\
\hline
\textbf{Finance} & Stock price prediction & Forecast market trends \\
\textbf{Healthcare} & Drug dosage calculation & Determine optimal treatment \\
\textbf{Marketing} & Sales forecasting & Predict revenue \\
\textbf{Real Estate} & Property valuation & Estimate house prices \\
\hline
\end{tabulary}
\end{center}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Finance, Healthcare, Marketing, Real estate (FHMR)}
\end{mnemonicbox}

\questionmarks{3(b)}{4}{Write a short note on single linear regression.}
\begin{solutionbox}
\textbf{Single linear regression} models the relationship between one independent variable (X) and one dependent variable (Y) using a straight line.

\begin{center}
\captionof{table}{Linear Regression Components}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{Component} & \textbf{Formula} & \textbf{Description} \\
\hline
\textbf{Equation} & \(Y = a + bX\) & Linear relationship \\
\textbf{Slope (b)} & \(\Delta Y / \Delta X\) & Rate of change \\
\textbf{Intercept (a)} & Y when X=0 & Starting point \\
\hline
\end{tabulary}
\end{center}

\begin{itemize}
    \item \textbf{Goal}: Find best-fit line minimizing errors.
    \item \textbf{Method}: Least squares optimization.
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Y equals a plus b times X (YABX)}
\end{mnemonicbox}

\questionmarks{3(c)}{7}{Write and discuss K-NN algorithm.}
\begin{solutionbox}
\textbf{K-Nearest Neighbors (K-NN)} is a lazy learning algorithm that classifies data points based on the majority class of their K nearest neighbors.

\begin{center}
\captionof{table}{K-NN Algorithm Steps}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{Step} & \textbf{Action} & \textbf{Description} \\
\hline
\textbf{1} & Choose K value & Select number of neighbors \\
\textbf{2} & Calculate distances & Find distance to all training points \\
\textbf{3} & Sort distances & Arrange in ascending order \\
\textbf{4} & Select K nearest & Choose K closest points \\
\textbf{5} & Majority voting & Assign most common class \\
\hline
\end{tabulary}
\end{center}

\begin{center}
\begin{tikzpicture}[node distance=1.5cm, auto]
    \node [gtu block] (New) {New Data};
    \node [gtu block, right=of New] (Calc) {Calc Dist};
    \node [gtu block, right=of Calc] (Sort) {Sort};
    \node [gtu block, below=1cm of Calc] (Sel) {Select K};
    \node [gtu block, right=of Sel] (Vote) {Vote};
    \node [gtu block, right=of Vote] (Class) {Classify};
    
    \path [gtu arrow] (New) -- (Calc);
    \path [gtu arrow] (Calc) -- (Sort);
    \path [gtu arrow] (Sort) -- (Sel);
    \path [gtu arrow] (Sel) -- (Vote);
    \path [gtu arrow] (Vote) -- (Class);
\end{tikzpicture}
\captionof{figure}{K-NN Process}
\end{center}

\textbf{Advantages:}
\begin{itemize}
    \item Simple to understand and implement.
    \item No training phase (Lazy).
\end{itemize}
\textbf{Disadvantages:}
\begin{itemize}
    \item Computationally expensive on large data.
    \item Sensitive to K value.
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Choose, Calculate, Sort, Majority vote (CCSM)}
\end{mnemonicbox}

\questionmarks{3(a) OR}{3}{Write any three examples of supervised learning in the field of healthcare}
\begin{solutionbox}
\begin{center}
\captionof{table}{Healthcare Supervised Learning}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{Application} & \textbf{Input} & \textbf{Output} \\
\hline
\textbf{Disease Diagnosis} & Symptoms, tests & Disease type \\
\textbf{Drug Prediction} & Patient genetics & Drug response \\
\textbf{Image Analysis} & X-rays, MRI & Tumor detection \\
\hline
\end{tabulary}
\end{center}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Diagnosis, Drug response, Medical imaging (DDM)}
\end{mnemonicbox}

\questionmarks{3(b) OR}{4}{Differentiate: Classification v/s Regression.}
\begin{solutionbox}
\begin{center}
\captionof{table}{Classification vs Regression}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{Aspect} & \textbf{Classification} & \textbf{Regression} \\
\hline
\textbf{Output Type} & Discrete categories & Continuous values \\
\textbf{Goal} & Predict class labels & Predict numerical values \\
\textbf{Examples} & Spam/Not Spam & House price \\
\textbf{Evaluation} & Accuracy, Precision & MSE, R-squared \\
\hline
\end{tabulary}
\end{center}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{CLASS-Categories, REG-Real numbers}
\end{mnemonicbox}

\questionmarks{3(c) OR}{7}{Explain classification learning steps in details.}
\begin{solutionbox}
\textbf{Classification learning} involves training a model to assign input data to predefined categories or classes.

\begin{center}
\captionof{table}{Classification Steps}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{Step} & \textbf{Process} & \textbf{Description} \\
\hline
\textbf{1} & Data Collection & Gather labeled training examples \\
\textbf{2} & Preprocessing & Clean and prepare data \\
\textbf{3} & Feature Selection & Choose relevant attributes \\
\textbf{4} & Model Selection & Choose algorithm \\
\textbf{5} & Training & Learn from labeled data \\
\textbf{6} & Evaluation & Test performance \\
\hline
\end{tabulary}
\end{center}

\begin{center}
\begin{tikzpicture}[node distance=1.5cm, auto]
    \node [gtu block] (Data) {Data};
    \node [gtu block, right=of Data] (Prep) {Prep};
    \node [gtu block, right=of Prep] (Feat) {Features};
    \node [gtu block, below=1cm of Data] (Train) {Train};
    \node [gtu block, right=of Train] (Eval) {Eval};
    \node [gtu block, right=of Eval] (Dep) {Deploy};
    
    \path [gtu arrow] (Data) -- (Prep);
    \path [gtu arrow] (Prep) -- (Feat);
    \path [gtu arrow] (Feat) -- (Train);
    \path [gtu arrow] (Train) -- (Eval);
    \path [gtu arrow] (Eval) -- (Dep);
\end{tikzpicture}
\captionof{figure}{Classification Pipeline}
\end{center}

\textbf{Key Concepts:}
\begin{itemize}
    \item \textbf{Supervised Learning}: Requires labeled data.
    \item \textbf{Performance}: Measured by accuracy, precision, recall.
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Data, Clean, Features, Model, Train, Evaluate, Deploy (DCFMTED)}
\end{mnemonicbox}

\questionmarks{4(a)}{3}{Differentiate: Clustering v/s Classification.}
\begin{solutionbox}
\begin{center}
\captionof{table}{Clustering vs Classification}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{Aspect} & \textbf{Clustering} & \textbf{Classification} \\
\hline
\textbf{Learning Type} & Unsupervised & Supervised \\
\textbf{Training Data} & Unlabeled & Labeled \\
\textbf{Goal} & Find patterns & Predict classes \\
\textbf{Output} & Groups & Predictions \\
\hline
\end{tabulary}
\end{center}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{CL-Unknown groups, CLASS-Known categories}
\end{mnemonicbox}

\questionmarks{4(b)}{4}{List out advantages and disadvantages of apriori algorithm.}
\begin{solutionbox}
\begin{center}
\captionof{table}{Apriori Pros and Cons}
\begin{tabulary}{\linewidth}{L L}
\hline
\textbf{Advantages} & \textbf{Disadvantages} \\
\hline
Easy to understand & Computationally expensive \\
Finds all frequent itemsets & Multiple database scans \\
Generates association rules & Large memory requirements \\
Simple logic & Poor scalability \\
\hline
\end{tabulary}
\end{center}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Easy to use but slow performance (EASY-SLOW)}
\end{mnemonicbox}

\questionmarks{4(c)}{7}{Write and explain applications of unsupervised learning.}
\begin{solutionbox}
\textbf{Unsupervised learning} discovers hidden patterns in data without labeled examples.

\begin{center}
\captionof{table}{Applications}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{Domain} & \textbf{Application} & \textbf{Technique} \\
\hline
\textbf{Marketing} & Customer segmentation & Clustering \\
\textbf{Retail} & Market basket analysis & Association rules \\
\textbf{Security} & Fraud detection & Anomaly detection \\
\textbf{Compression} & Dimensionality reduction & PCA \\
\hline
\end{tabulary}
\end{center}

\begin{center}
\begin{tikzpicture}[node distance=1.5cm, auto]
    \node [gtu block] (UL) {Unsupervised Learning};
    
    \node [gtu block, below left=1.2cm and 2cm of UL] (Clust) {Clustering};
    \node [gtu block, below left=1.2cm and -0.5cm of UL] (Assoc) {Association};
    \node [gtu block, below right=1.2cm and -0.5cm of UL] (Anom) {Anomaly};
    \node [gtu block, below right=1.2cm and 2cm of UL] (Dim) {Reduction};
    
    \node [gtu state, below=0.5cm of Clust] {Segmentation};
    \node [gtu state, below=0.5cm of Assoc] {Market Basket};
    \node [gtu state, below=0.5cm of Anom] {Fraud Detect};
    \node [gtu state, below=0.5cm of Dim] {Compression};
    
    \path [gtu arrow] (UL) -- (Clust);
    \path [gtu arrow] (UL) -- (Assoc);
    \path [gtu arrow] (UL) -- (Anom);
    \path [gtu arrow] (UL) -- (Dim);
\end{tikzpicture}
\captionof{figure}{Unsupervised Applications}
\end{center}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Marketing, Retail, Anomaly, Dimensionality (MRAD)}
\end{mnemonicbox}

\questionmarks{4(a) OR}{3}{List out applications of apriori algorithm.}
\begin{solutionbox}
\begin{center}
\captionof{table}{Apriori Applications}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{Domain} & \textbf{Application} & \textbf{Purpose} \\
\hline
\textbf{Retail} & Market basket analysis & Find items bought together \\
\textbf{Web Mining} & Website usage patterns & Discover page visit sequences \\
\textbf{Bioinformatics} & Gene pattern analysis & Identify gene associations \\
\hline
\end{tabulary}
\end{center}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Retail, Web, Bioinformatics (RWB)}
\end{mnemonicbox}

\questionmarks{4(b) OR}{4}{Define: Support and Confidence.}
\begin{solutionbox}
\begin{center}
\captionof{table}{Association Rule Metrics}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{Metric} & \textbf{Formula} & \textbf{Description} \\
\hline
\textbf{Support} & \(Count(A) / Total\) & How often itemset appears \\
\textbf{Confidence} & \(Support(A \cup B) / Support(A)\) & How often rule is true \\
\hline
\end{tabulary}
\end{center}

\textbf{Example:}
\begin{itemize}
    \item \textbf{Support}: If Bread \& Milk appear in 30\% of transactions.
    \item \textbf{Confidence}: If 60\% of Bread buyers also buy Milk.
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{SUP-How often, CONF-How reliable}
\end{mnemonicbox}

\questionmarks{4(c) OR}{7}{Write and explain K-means clustering approach in detail.}
\begin{solutionbox}
\textbf{K-means clustering} partitions data into K clusters by minimizing within-cluster sum of squares.

\begin{center}
\captionof{table}{K-means Steps}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{Step} & \textbf{Action} & \textbf{Description} \\
\hline
\textbf{1} & Choose K & Select number of clusters \\
\textbf{2} & Initialize & Place K centroids randomly \\
\textbf{3} & Assign & Points to nearest centroid \\
\textbf{4} & Update & Recalculate centroids \\
\textbf{5} & Repeat & Until convergence \\
\hline
\end{tabulary}
\end{center}

\begin{center}
\begin{tikzpicture}[node distance=1.5cm, auto]
    \node [gtu block] (K) {Choose K};
    \node [gtu block, right=of K] (Init) {Initialize};
    \node [gtu block, right=of Init] (Assign) {Assign};
    \node [gtu block, below=1cm of Assign] (Update) {Update};
    
    \path [gtu arrow] (K) -- (Init);
    \path [gtu arrow] (Init) -- (Assign);
    \path [gtu arrow] (Assign) -- (Update);
    \path [gtu arrow] (Update.west) -- ++(-1,0) |- (Assign.west);
\end{tikzpicture}
\captionof{figure}{K-means Process}
\end{center}

\textbf{Choosing K}: Use Elbow Method to find optimal K value.
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Choose K, Initialize, Assign, Update, Repeat (CIAUR)}
\end{mnemonicbox}

\questionmarks{5(a)}{3}{Give the difference between predictive model and descriptive model.}
\begin{solutionbox}
\begin{center}
\captionof{table}{Predictive vs Descriptive Models}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{Aspect} & \textbf{Predictive} & \textbf{Descriptive} \\
\hline
\textbf{Purpose} & Forecast future & Explain present \\
\textbf{Output} & Predictions & Insights \\
\textbf{Examples} & Forecasting & Segmentation \\
\hline
\end{tabulary}
\end{center}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{PRED-Future, DESC-Present}
\end{mnemonicbox}

\questionmarks{5(b)}{4}{List out application of scikit-learn.}
\begin{solutionbox}
\begin{center}
\captionof{table}{Scikit-learn Applications}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{Category} & \textbf{Algorithm} & \textbf{Application} \\
\hline
\textbf{Classification} & SVM, Random Forest & Spam filtering \\
\textbf{Regression} & Linear Regression & Price prediction \\
\textbf{Clustering} & K-means & Customer grouping \\
\textbf{Preprocessing} & Scalers & Data cleaning \\
\hline
\end{tabulary}
\end{center}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Classification, Regression, Clustering, Preprocessing (CRCP)}
\end{mnemonicbox}

\questionmarks{5(c)}{7}{Explain features and applications of Numpy.}
\begin{solutionbox}
\textbf{NumPy} is the fundamental library for scientific computing in Python.

\begin{center}
\captionof{table}{NumPy Features}
\begin{tabulary}{\linewidth}{L L}
\hline
\textbf{Feature} & \textbf{Benefit} \\
\hline
\textbf{N-dim Arrays} & Efficient data storage \\
\textbf{Broadcasting} & Flexible computations \\
\textbf{Math Functions} & Complete math toolkit \\
\textbf{Performance} & Fast C implementation \\
\hline
\end{tabulary}
\end{center}

\begin{center}
\begin{tikzpicture}[node distance=1.5cm, auto]
    \node [gtu block] (Num) {NumPy};
    \node [gtu block, below left=1.5cm and 1cm of Num] (Feat) {Features};
    \node [gtu block, below right=1.5cm and 1cm of Num] (App) {Apps};
    \node [gtu state, below=0.5cm of Feat] {Arrays\\Broadcasting};
    \node [gtu state, below=0.5cm of App] {ML, Science,\\Finance};
    
    \path [gtu arrow] (Num) -- (Feat);
    \path [gtu arrow] (Num) -- (App);
\end{tikzpicture}
\captionof{figure}{NumPy Functions}
\end{center}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{N-dimensional, Fast, Arrays, Math, Scientific (NFAMS)}
\end{mnemonicbox}

\questionmarks{5(a) OR}{3}{Write a short note on bagging}
\begin{solutionbox}
\textbf{Bagging} (Bootstrap Aggregating) improves model performance by training multiple models on different data subsets.

\begin{center}
\captionof{table}{Bagging Process}
\begin{tabulary}{\linewidth}{L L}
\hline
\textbf{Step} & \textbf{Purpose} \\
\hline
\textbf{Bootstrap} & Create diverse training sets \\
\textbf{Train} & Build independent models \\
\textbf{Aggregate} & Average predictions to reduce variance \\
\hline
\end{tabulary}
\end{center}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Bootstrap, Train, Aggregate (BTA)}
\end{mnemonicbox}

\questionmarks{5(b) OR}{4}{List out features of Pandas.}
\begin{solutionbox}
\begin{center}
\captionof{table}{Pandas Features}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{Feature} & \textbf{Description} & \textbf{Benefit} \\
\hline
\textbf{DataFrame} & Structured container & Easy manipulation \\
\textbf{File I/O} & Read/Write & Format support \\
\textbf{Cleaning} & Missing values & Data prep \\
\textbf{Grouping} & Aggregation & Analysis \\
\hline
\end{tabulary}
\end{center}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{DataFrame, File I/O, Indexing, Grouping (DFIG)}
\end{mnemonicbox}

\questionmarks{5(c) OR}{7}{Explain features and applications of Matplotlib.}
\begin{solutionbox}
\textbf{Matplotlib} is a comprehensive 2D plotting library for creating publication-quality figures.

\begin{center}
\captionof{table}{Matplotlib Capabilities}
\begin{tabulary}{\linewidth}{L L}
\hline
\textbf{Feature} & \textbf{Description} \\
\hline
\textbf{Plot Types} & Line, bar, scatter, histogram \\
\textbf{Customization} & Full control over style \\
\textbf{Interactive} & Zoom, pan support \\
\textbf{Output} & PNG, PDF, SVG support \\
\hline
\end{tabulary}
\end{center}

\begin{center}
\begin{tikzpicture}[node distance=1.5cm, auto]
    \node [gtu block] (Raw) {Raw Data};
    \node [gtu block, right=of Raw] (Proc) {Process};
    \node [gtu block, right=of Proc] (Plot) {Plotting};
    \node [gtu block, below=1cm of Plot] (Out) {Output};
    \node [gtu state, below=0.5cm of Out] {PDF/PNG/Web};
    
    \path [gtu arrow] (Raw) -- (Proc);
    \path [gtu arrow] (Proc) -- (Plot);
    \path [gtu arrow] (Plot) -- (Out);
\end{tikzpicture}
\captionof{figure}{Visualization Pipeline}
\end{center}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Multiple plots, Visualization, Interactive, Customizable, Scientific (MVICS)}
\end{mnemonicbox}

\end{document}
