\documentclass{article}
\input{/Users/milav/Code/milav-next/latex-templates/gtu-solutions/preamble.tex}
\input{/Users/milav/Code/milav-next/latex-templates/gtu-solutions/english-boxes.tex}
\input{/Users/milav/Code/milav-next/latex-templates/gtu-solutions/commands.tex}

\title{Fundamentals of Machine Learning (4341603) - Winter 2023 Solution}
\date{February 2, 2024}

\begin{document}
\maketitle

\questionmarks{1(a)}{3}{Define human learning and explain how machine learning is different from human learning?}
\begin{solutionbox}
\textbf{Human Learning} is the process of acquiring knowledge through experience, observation, and reasoning.

\begin{center}
\captionof{table}{Human Learning vs Machine Learning}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{Aspect} & \textbf{Human Learning} & \textbf{Machine Learning} \\
\hline
\textbf{Method} & Experience, trial and error & Data and algorithms \\
\textbf{Speed} & Slow, gradual & Fast processing \\
\textbf{Data Requirement} & Limited examples needed & Large datasets required \\
\hline
\end{tabulary}
\end{center}

\textbf{Machine Learning} is the automated learning from data using algorithms to identify patterns without explicit programming.
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Humans Experience, Machines Analyze Data (HEMAD)}
\end{mnemonicbox}

\questionmarks{1(b)}{4}{Describe the use of machine learning in finance and banking.}
\begin{solutionbox}
\begin{center}
\captionof{table}{Applications in Finance and Banking}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{Application} & \textbf{Purpose} & \textbf{Benefit} \\
\hline
\textbf{Fraud Detection} & Identify suspicious transactions & Reduce financial losses \\
\textbf{Credit Scoring} & Assess loan default risk & Better lending decisions \\
\textbf{Algorithmic Trading} & Automated trading decisions & Faster market responses \\
\hline
\end{tabulary}
\end{center}

\textbf{Risk Assessment}: ML analyzes customer data to predict creditworthiness.
\textbf{Customer Service}: Chatbots provide 24/7 support using NLP.
\textbf{Regulatory Compliance}: Automated monitoring for suspicious activities.
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Finance Needs Smart Analysis (FNSA)}
\end{mnemonicbox}

\questionmarks{1(c)}{7}{Give difference between Supervised Learning, Unsupervised Learning and Reinforcement Learning.}
\begin{solutionbox}
\begin{center}
\captionof{table}{Comparison}
\begin{tabulary}{\linewidth}{L L L L}
\hline
\textbf{Feature} & \textbf{Supervised} & \textbf{Unsupervised} & \textbf{Reinforcement} \\
\hline
\textbf{Data Type} & Labeled data & Unlabeled data & Environment interaction \\
\textbf{Goal} & Predict output & Find patterns & Maximize rewards \\
\textbf{Examples} & Classification & Clustering & Game playing \\
\textbf{Feedback} & Immediate & None & Delayed rewards \\
\hline
\end{tabulary}
\end{center}

\textbf{Supervised Learning}: Teacher-guided learning with correct answers provided.

\textbf{Unsupervised Learning}: Self-discovery of hidden patterns in data.

\textbf{Reinforcement Learning}: Learning through trial and error with rewards/penalties.
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Supervised Teachers, Unsupervised Explores, Reinforcement Rewards (STUER)}
\end{mnemonicbox}

\questionmarks{1(c) OR}{7}{Explain different tools and technology used in machine learning.}
\begin{solutionbox}
\begin{center}
\captionof{table}{ML Tools and Technologies}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{Category} & \textbf{Tools} & \textbf{Purpose} \\
\hline
\textbf{Programming} & Python, R, Java & Algorithm implementation \\
\textbf{Libraries} & Scikit-learn, TensorFlow & Ready-made algorithms \\
\textbf{Visualization} & Matplotlib, Seaborn & Data visualization \\
\textbf{Data Processing} & Pandas, NumPy & Data manipulation \\
\hline
\end{tabulary}
\end{center}

\textbf{Key Technologies:}
\begin{itemize}
    \item \textbf{Cloud Platforms}: AWS, Google Cloud for scalable computing
    \item \textbf{Development Environments}: Jupyter Notebook, Google Colab
    \item \textbf{Big Data Tools}: Spark, Hadoop for large datasets
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Python Libraries Visualize Data Effectively (PLVDE)}
\end{mnemonicbox}

\questionmarks{2(a)}{3}{Define outliers with one example.}
\begin{solutionbox}
\textbf{Definition}: Outliers are data points that significantly differ from other observations in a dataset.

\begin{center}
\captionof{table}{Example: Student Heights}
\begin{tabulary}{\linewidth}{L L}
\hline
\textbf{Student Heights (cm)} & \textbf{Classification} \\
\hline
165, 170, 168, 172 & Normal values \\
195 & Outlier (too tall) \\
140 & Outlier (too short) \\
\hline
\end{tabulary}
\end{center}

\textbf{Detection}: Values beyond 1.5 $\times$ IQR from quartiles.
\textbf{Impact}: Can skew statistical analysis and model performance.
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Outliers Stand Apart (OSA)}
\end{mnemonicbox}

\questionmarks{2(b)}{4}{Explain regression steps in detail.}
\begin{solutionbox}
\begin{center}
\begin{tikzpicture}[node distance=1.5cm, auto]
    \node [gtu block] (Coll) {Data Collection};
    \node [gtu block, below=0.8cm of Coll] (Prep) {Data Preprocessing};
    \node [gtu block, below=0.8cm of Prep] (Feat) {Feature Selection};
    \node [gtu block, below=0.8cm of Feat] (Train) {Model Training};
    \node [gtu block, below=0.8cm of Train] (Eval) {Model Evaluation};
    \node [gtu block, below=0.8cm of Eval] (Pred) {Prediction};
    
    \path [gtu arrow] (Coll) -- (Prep);
    \path [gtu arrow] (Prep) -- (Feat);
    \path [gtu arrow] (Feat) -- (Train);
    \path [gtu arrow] (Train) -- (Eval);
    \path [gtu arrow] (Eval) -- (Pred);
\end{tikzpicture}
\captionof{figure}{Regression Process Steps}
\end{center}

\textbf{Detailed Steps:}
\begin{itemize}
    \item \textbf{Data Collection}: Gather relevant dataset with input-output pairs
    \item \textbf{Preprocessing}: Clean data, handle missing values, normalize features
    \item \textbf{Feature Selection}: Choose relevant variables that affect target
    \item \textbf{Model Training}: Fit regression line to minimize prediction errors
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Data Preprocessing Features Train Evaluation Predicts (DPFTEP)}
\end{mnemonicbox}

\questionmarks{2(c)}{7}{Define Accuracy and for the following binary classifier's confusion matrix, find the various measurement parameters like 1. Accuracy 2. Precision.}
\begin{solutionbox}
\textbf{Confusion Matrix Analysis:}

\begin{center}
\captionof{table}{Given Confusion Matrix}
\begin{tabulary}{\linewidth}{L L L}
\hline
 & \textbf{Predicted No} & \textbf{Predicted Yes} \\
\hline
\textbf{Actual No} & 10 (TN) & 3 (FP) \\
\textbf{Actual Yes} & 2 (FN) & 15 (TP) \\
\hline
\end{tabulary}
\end{center}

\textbf{Calculations:}
\begin{itemize}
    \item \textbf{Accuracy} formula: \((TP+TN)/(TP+TN+FP+FN)\)
    \item Calculation: \((15+10)/(15+10+3+2) = 25/30 = 0.8333\)
    \item \textbf{Accuracy Result: 83.33\%}
    
    \item \textbf{Precision} formula: \(TP/(TP+FP)\)
    \item Calculation: \(15/(15+3) = 15/18 = 0.8333\)
    \item \textbf{Precision Result: 83.33\%}
\end{itemize}

\textbf{Definitions:}
\begin{itemize}
    \item \textbf{Accuracy}: Proportion of correct predictions out of total predictions.
    \item \textbf{Precision}: Proportion of true positive predictions out of all positive predictions.
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Accuracy Counts All, Precision Picks Positives (ACAPP)}
\end{mnemonicbox}

\questionmarks{2(a) OR}{3}{Identify basic steps of feature subset selection.}
\begin{solutionbox}
\begin{center}
\begin{tikzpicture}[node distance=1.5cm, auto]
    \node [gtu block] (Orig) {Original Features};
    \node [gtu block, right=of Orig] (Gen) {Generate Subsets};
    \node [gtu block, right=of Gen] (Eval) {Evaluate Subsets};
    \node [gtu block, right=of Eval] (Sel) {Select Best};
    
    \path [gtu arrow] (Orig) -- (Gen);
    \path [gtu arrow] (Gen) -- (Eval);
    \path [gtu arrow] (Eval) -- (Sel);
\end{tikzpicture}
\captionof{figure}{Feature Subset Selection Process}
\end{center}

\textbf{Basic Steps:}
\begin{enumerate}
    \item \textbf{Generation}: Create different combinations of features
    \item \textbf{Evaluation}: Test each subset using performance metrics
    \item \textbf{Selection}: Choose optimal subset based on criteria
\end{enumerate}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Generate, Evaluate, Select (GES)}
\end{mnemonicbox}

\questionmarks{2(b) OR}{4}{Discuss the strength and weakness of the KNN algorithm.}
\begin{solutionbox}
\begin{center}
\captionof{table}{KNN Algorithm Analysis}
\begin{tabulary}{\linewidth}{L L}
\hline
\textbf{Strengths} & \textbf{Weaknesses} \\
\hline
Simple to understand & Computationally expensive \\
No training required (Lazy) & Sensitive to irrelevant features \\
Works with non-linear data & Performance degrades with high dimensions \\
Effective for small datasets & Requires optimal K value selection \\
\hline
\end{tabulary}
\end{center}

\textbf{Key Points:}
\begin{itemize}
    \item \textbf{Lazy Learning}: No explicit training phase required.
    \item \textbf{Distance-Based}: Classification based on neighbor proximity.
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Simple but Slow, Effective but Expensive (SBSEBE)}
\end{mnemonicbox}

\questionmarks{2(c) OR}{7}{Define Error-rate and for the following binary classifier's confusion matrix, find the various measurement parameters like 1. Error value 2. Recall.}
\begin{solutionbox}
\textbf{Confusion Matrix Analysis:}

\begin{center}
\captionof{table}{Given Confusion Matrix}
\begin{tabulary}{\linewidth}{L L L}
\hline
 & \textbf{Predicted No} & \textbf{Predicted Yes} \\
\hline
\textbf{Actual No} & 20 (TN) & 3 (FP) \\
\textbf{Actual Yes} & 2 (FN) & 15 (TP) \\
\hline
\end{tabulary}
\end{center}

\textbf{Calculations:}
\begin{itemize}
    \item \textbf{Error Rate} formula: \((FP+FN)/(Total)\)
    \item Calculation: \((3+2)/(15+20+3+2) = 5/40 = 0.125\)
    \item \textbf{Error Rate Result: 12.5\%}
    
    \item \textbf{Recall} formula: \(TP/(TP+FN)\)
    \item Calculation: \(15/(15+2) = 15/17 = 0.8824\)
    \item \textbf{Recall Result: 88.24\%}
\end{itemize}

\textbf{Definitions:}
\begin{itemize}
    \item \textbf{Error Rate}: Proportion of incorrect predictions out of total predictions.
    \item \textbf{Recall}: Proportion of actual positives correctly identified.
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Error Excludes, Recall Retrieves (EERR)}
\end{mnemonicbox}

\questionmarks{3(a)}{3}{Give any three examples of unsupervised learning.}
\begin{solutionbox}
\begin{center}
\captionof{table}{Unsupervised Learning Examples}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{Example} & \textbf{Description} & \textbf{Application} \\
\hline
\textbf{Customer Segmentation} & Group customers by behavior & Marketing strategies \\
\textbf{Document Classification} & Organize documents by topics & Information retrieval \\
\textbf{Gene Sequencing} & Group similar DNA patterns & Medical research \\
\hline
\end{tabulary}
\end{center}

\textbf{Additional Examples:}
\begin{itemize}
    \item \textbf{Market Basket Analysis}: Finding product purchase patterns
    \item \textbf{Anomaly Detection}: Detecting unusual patterns in data
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Customers, Documents, Genes Group Automatically (CDGGA)}
\end{mnemonicbox}

\questionmarks{3(b)}{4}{Find Mean and Median for the following data: 4,6,7,8,9,12,14,15,20}
\begin{solutionbox}
\textbf{Data}: 4, 6, 7, 8, 9, 12, 14, 15, 20 (Already sorted)

\textbf{Mean Calculation:}
\begin{itemize}
    \item Sum = \(4+6+7+8+9+12+14+15+20 = 95\)
    \item Count = 9
    \item \textbf{Mean} = \(95/9 = 10.56\)
\end{itemize}

\textbf{Median Calculation:}
\begin{itemize}
    \item N = 9 (Odd number)
    \item Position = \((N+1)/2 = 5\)th value
    \item 5th value in sorted list is 9
    \item \textbf{Median} = 9
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Mean Averages All, Median Middle Value (MAAMV)}
\end{mnemonicbox}

\questionmarks{3(c)}{7}{Describe k-fold cross validation method in detail.}
\begin{solutionbox}
\begin{center}
\begin{tikzpicture}[node distance=1.5cm, auto]
    \node [gtu block] (Data) {Dataset};
    \node [gtu block, right=of Data] (Split) {Split K Folds};
    \node [gtu block, below=1cm of Split] (Train) {Train K-1 Folds};
    \node [gtu block, right=of Train] (Test) {Test 1 Fold};
    \node [gtu block, right=of Test] (Avg) {Average};
    
    \path [gtu arrow] (Data) -- (Split);
    \path [gtu arrow] (Split) -- (Train);
    \path [gtu arrow] (Train) -- (Test);
    \path [gtu arrow] (Test) -- node[above] {Repeat K} (Avg);
    \path [gtu arrow] (Test.south) -- ++(0,-0.5) -| (Train.south);
\end{tikzpicture}
\captionof{figure}{K-Fold Cross Validation}
\end{center}

\textbf{Process Steps:}
\begin{enumerate}
    \item \textbf{Data Division}: Split data into K equal parts (folds).
    \item \textbf{Iterative Training}: Use K-1 folds for training the model.
    \item \textbf{Validation}: Test the model on the remaining fold.
    \item \textbf{Averaging}: Repeat K times and average the performance metrics.
\end{enumerate}

\textbf{Advantages:}
\begin{itemize}
    \item \textbf{Unbiased Estimation}: Each data point used for both training and testing.
    \item \textbf{Reduced Overfitting}: Multiple validation rounds increase reliability.
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{K-fold Keeps Keen Knowledge (KKKK)}
\end{mnemonicbox}

\questionmarks{3(a) OR}{3}{Give any three applications of multiple linear regression.}
\begin{solutionbox}
\begin{center}
\captionof{table}{Multiple Linear Regression Applications}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{Application} & \textbf{Variables} & \textbf{Purpose} \\
\hline
\textbf{House Price Prediction} & Size, location, age & Real estate valuation \\
\textbf{Sales Forecasting} & Marketing spend, season & Business planning \\
\textbf{Medical Diagnosis} & Symptoms, age, history & Disease prediction \\
\hline
\end{tabulary}
\end{center}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Houses, Sales, Medicine Predict Multiple Variables (HSMPV)}
\end{mnemonicbox}

\questionmarks{3(b) OR}{4}{Find Standard Deviation for the following data: 4,15,20,28,35,45}
\begin{solutionbox}
\textbf{Data}: 4, 15, 20, 28, 35, 45 (N=6)

\textbf{Step 1: Calculate Mean}
\begin{itemize}
    \item Sum = \(4+15+20+28+35+45 = 147\)
    \item Mean (\(\bar{x}\)) = \(147/6 = 24.5\)
\end{itemize}

\textbf{Step 2: Calculate Squared Deviations}
\begin{itemize}
    \item \((4-24.5)^2 = (-20.5)^2 = 420.25\)
    \item \((15-24.5)^2 = (-9.5)^2 = 90.25\)
    \item \((20-24.5)^2 = (-4.5)^2 = 20.25\)
    \item \((28-24.5)^2 = (3.5)^2 = 12.25\)
    \item \((35-24.5)^2 = (10.5)^2 = 110.25\)
    \item \((45-24.5)^2 = (20.5)^2 = 420.25\)
\end{itemize}

\textbf{Step 3: Calculate Variance and Std Dev}
\begin{itemize}
    \item Sum of squared deviations = 1073.5
    \item Variance (\(\sigma^2\)) = \(1073.5/6 = 178.92\)
    \item \textbf{Standard Deviation (\(\sigma\))} = \(\sqrt{178.92} = 13.376\)
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Deviation Measures Data Spread (DMDS)}
\end{mnemonicbox}

\questionmarks{3(c) OR}{7}{Explain Bagging, Boosting in detail.}
\begin{solutionbox}
\begin{center}
\captionof{table}{Bagging vs Boosting}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{Aspect} & \textbf{Bagging} & \textbf{Boosting} \\
\hline
\textbf{Strategy} & Parallel training & Sequential training \\
\textbf{Data Sampling} & Random with replacement & Weighted sampling \\
\textbf{Goal} & Reduces variance & Reduces bias \\
\hline
\end{tabulary}
\end{center}

\textbf{Bagging (Bootstrap Aggregating):}
Trains multiple independent models in parallel using random subsets of data and averages their predictions.

\begin{center}
\begin{tikzpicture}[node distance=1.5cm]
    \node [gtu block] (Data) {Original Data};
    \node [gtu block, below left=1.5cm and 1cm of Data] (S1) {Sample 1};
    \node [gtu block, below=1.5cm of Data] (S2) {Sample 2};
    \node [gtu block, below right=1.5cm and 1cm of Data] (Sn) {Sample n};
    
    \node [gtu state, below=0.8cm of S1] (M1) {Model 1};
    \node [gtu state, below=0.8cm of S2] (M2) {Model 2};
    \node [gtu state, below=0.8cm of Sn] (Mn) {Model n};
    
    \node [gtu block, below=2cm of M2] (Final) {Final Prediction};
    
    \path [gtu arrow] (Data) -- (S1);
    \path [gtu arrow] (Data) -- (S2);
    \path [gtu arrow] (Data) -- (Sn);
    \path [gtu arrow] (S1) -- (M1);
    \path [gtu arrow] (S2) -- (M2);
    \path [gtu arrow] (Sn) -- (Mn);
    \path [gtu arrow] (M1) -- (Final);
    \path [gtu arrow] (M2) -- (Final);
    \path [gtu arrow] (Mn) -- (Final);
\end{tikzpicture}
\captionof{figure}{Bagging Process}
\end{center}

\textbf{Boosting:}
Trains models sequentially, where each new model focuses on the errors made by previous models.
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Bagging Builds Parallel, Boosting Builds Sequential (BBPBS)}
\end{mnemonicbox}

\questionmarks{4(a)}{3}{Define: Support, Confidence.}
\begin{solutionbox}
\begin{center}
\captionof{table}{Association Rule Metrics}
\begin{tabulary}{\linewidth}{L L}
\hline
\textbf{Metric} & \textbf{Definition and Formula} \\
\hline
\textbf{Support} & Frequency of itemset in transactions. \\
 & \(Support(A) = Count(A)/Total\) \\
\hline
\textbf{Confidence} & Conditional probability of rule. \\
 & \(Confidence(A \to B) = Support(A \cup B)/Support(A)\) \\
\hline
\end{tabulary}
\end{center}

\textbf{Example:}
\begin{itemize}
    \item \textbf{Support}: 60\% of transactions have Bread.
    \item \textbf{Confidence}: 80\% of people buying Bread also buy Butter.
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Support Shows Frequency, Confidence Shows Connection (SSFC)}
\end{mnemonicbox}

\questionmarks{4(b)}{4}{Illustrate any two applications of logistic regression.}
\begin{solutionbox}
\begin{center}
\captionof{table}{Logistic Regression Applications}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{Application} & \textbf{Description} & \textbf{Outcome} \\
\hline
\textbf{Email Spam} & Detect spam based on words & Spam/Not Spam \\
\textbf{Medical Diagnosis} & Predict disease from symptoms & Disease/Healthy \\
\textbf{Credit Approval} & Assess loan risk & Approve/Reject \\
\hline
\end{tabulary}
\end{center}

\textbf{Key Features:}
\begin{itemize}
    \item \textbf{Binary Classification}: Predicts probability (0 to 1).
    \item \textbf{Sigmoid Function}: Maps output to S-shaped curve.
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Logistic Limits Linear Logic (LLLL)}
\end{mnemonicbox}

\questionmarks{4(c)}{7}{Discuss the main purpose of Numpy and Pandas in machine learning.}
\begin{solutionbox}
\textbf{NumPy} provides support for large, multi-dimensional arrays and matrices, along with mathematical functions.
\textbf{Pandas} offers data structures and operations for manipulating numerical tables and time series.

\begin{center}
\begin{tikzpicture}[node distance=1.5cm]
    \node [gtu block] (NumPy) {NumPy};
    \node [gtu state, below left=1.5cm and 0.5cm of NumPy] (Array) {Array Ops};
    \node [gtu state, below right=1.5cm and 0.5cm of NumPy] (Math) {Math Funcs};
    \node [gtu state, below=1.5cm of NumPy] (LinAlg) {Linear Algebra};
    
    \path [gtu arrow] (NumPy) -- (Array);
    \path [gtu arrow] (NumPy) -- (Math);
    \path [gtu arrow] (NumPy) -- (LinAlg);
\end{tikzpicture}
\captionof{figure}{NumPy Features}
\end{center}

\begin{center}
\captionof{table}{Comparison}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{Library} & \textbf{Primary Purpose} & \textbf{Key Features} \\
\hline
\textbf{NumPy} & Numerical Computing & N-dim arrays, broadcasting \\
\textbf{Pandas} & Data Manipulation & DataFrames, cleaning, analysis \\
\hline
\end{tabulary}
\end{center}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{NumPy Numbers, Pandas Processes Data (NNPD)}
\end{mnemonicbox}

\questionmarks{4(a) OR}{3}{Give any three examples of Supervised Learning.}
\begin{solutionbox}
\begin{center}
\captionof{table}{Supervised Learning Examples}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{Example} & \textbf{Type} & \textbf{Input \(\to\) Output} \\
\hline
\textbf{Email Classification} & Classification & Email features \(\to\) Spam/Not \\
\textbf{House Price Prediction} & Regression & House features \(\to\) Price \\
\textbf{Image Recognition} & Classification & Pixels \(\to\) Object Class \\
\hline
\end{tabulary}
\end{center}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Emails, Houses, Images Learn Supervised (EHILS)}
\end{mnemonicbox}

\questionmarks{4(b) OR}{4}{Explain any two applications of the apriori algorithm.}
\begin{solutionbox}
\begin{center}
\captionof{table}{Apriori Applications}
\begin{tabulary}{\linewidth}{L L}
\hline
\textbf{Application} & \textbf{Description} \\
\hline
\textbf{Market Basket Analysis} & Finding products bought together (e.g., Bread \& Butter) to optimize store layout. \\
\hline
\textbf{Web Usage Mining} & Discovering reliable navigation patterns to improve website UX. \\
\hline
\end{tabulary}
\end{center}

\textbf{Process}:
\begin{enumerate}
    \item Generate frequent itemsets.
    \item Prune infrequent items based on support.
    \item Generate association rules based on confidence.
\end{enumerate}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Apriori Analyzes Associations Automatically (AAAA)}
\end{mnemonicbox}

\questionmarks{4(c) OR}{7}{Explain the features and applications of Matplotlib.}
\begin{solutionbox}
\textbf{Matplotlib} is a comprehensive library for creating static, animated, and interactive visualizations in Python.

\begin{center}
\begin{tikzpicture}[node distance=1.5cm]
    \node [gtu block] (Mat) {Matplotlib};
    \node [gtu block, below left=1.5cm and 1cm of Mat] (2D) {2D Plotting};
    \node [gtu block, below right=1.5cm and 1cm of Mat] (3D) {3D Plotting};
    
    \node [gtu state, below=0.8cm of 2D] (2Dtype) {Line, Bar, Scatter};
    \node [gtu state, below=0.8cm of 3D] (3Dtype) {Surface, Mesh};
    
    \path [gtu arrow] (Mat) -- (2D);
    \path [gtu arrow] (Mat) -- (3D);
    \path [gtu arrow] (2D) -- (2Dtype);
    \path [gtu arrow] (3D) -- (3Dtype);
\end{tikzpicture}
\captionof{figure}{Matplotlib Capabilities}
\end{center}

\textbf{Applications:}
\begin{itemize}
    \item \textbf{Data Exploration}: Histograms, scatter plots to understand data.
    \item \textbf{Model Performance}: Plotting loss curves and accuracy.
    \item \textbf{Result Presentation}: Publication-quality figures.
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Matplotlib Makes Meaningful Visual Displays (MMVD)}
\end{mnemonicbox}

\questionmarks{5(a)}{3}{List out the major features of Numpy.}
\begin{solutionbox}
\textbf{Features of NumPy:}
\begin{itemize}
    \item \textbf{N-dimensional Arrays}: Fast and efficient multidimensional array object (ndarray).
    \item \textbf{Broadcasting}: Functions to perform operations on arrays of different shapes.
    \item \textbf{Linear Algebra}: Built-in support for matrix operations and Fourier transforms.
    \item \textbf{C/C++ Integration}: Tools for integrating C/C++ and Fortran code.
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{NumPy Numbers Need Neat Operations (NNNNO)}
\end{mnemonicbox}

\questionmarks{5(b)}{4}{How to load an iris dataset csv file in a Pandas Dataframe program? Explain with example.}
\begin{solutionbox}
\begin{lstlisting}[language=Python]
import pandas as pd

# Method 1: Load from local CSV file
df = pd.read_csv('iris.csv')

# Method 2: From sklearn (common in ML)
from sklearn.datasets import load_iris
iris = load_iris()
df_iris = pd.DataFrame(iris.data, columns=iris.feature_names)

# Display first 5 rows
print(df.head())
\end{lstlisting}

\textbf{Explanation:}
\begin{itemize}
    \item \code{pd.read\_csv()}: Function to read comma-separated values.
    \item \code{df.head()}: Returns the first n rows (default 5).
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Pandas Reads CSV Files Easily (PRCFE)}
\end{mnemonicbox}

\questionmarks{5(c)}{7}{Compare and Contrast Supervised Learning and Unsupervised Learning.}
\begin{solutionbox}
\begin{center}
\begin{tikzpicture}[node distance=1.5cm]
    \node [gtu block] (ML) {Machine Learning};
    \node [gtu block, below left=1.5cm and 1cm of ML] (Sup) {Supervised};
    \node [gtu block, below right=1.5cm and 1cm of ML] (Unsup) {Unsupervised};
    
    \node [gtu state, below=0.8cm of Sup] (SupEx) {Classification\\Regression};
    \node [gtu state, below=0.8cm of Unsup] (UnsupEx) {Clustering\\Association};
    
    \path [gtu arrow] (ML) -- (Sup);
    \path [gtu arrow] (ML) -- (Unsup);
    \path [gtu arrow] (Sup) -- (SupEx);
    \path [gtu arrow] (Unsup) -- (UnsupEx);
\end{tikzpicture}
\captionof{figure}{ML Learning Types}
\end{center}

\begin{center}
\captionof{table}{Comparison}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{Aspect} & \textbf{Supervised} & \textbf{Unsupervised} \\
\hline
\textbf{Data} & Labeled & Unlabeled \\
\textbf{Goal} & Predict output & Discover patterns \\
\textbf{Feedback} & Direct feedback & No feedback \\
\textbf{Complexity} & Validation is easier & Validation is harder \\
\hline
\end{tabulary}
\end{center}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Supervised Seeks Specific Solutions, Unsupervised Uncovers Unknown (SSSUU)}
\end{mnemonicbox}

\questionmarks{5(a) OR}{3}{List out the applications of Pandas.}
\begin{solutionbox}
\begin{center}
\captionof{table}{Pandas Applications}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{Application} & \textbf{Description} & \textbf{Field} \\
\hline
\textbf{Data Cleaning} & Handling missing data & General ML \\
\textbf{Financial Analysis} & Stock market trends & Finance \\
\textbf{Recommendation} & Analyzing user behavior & E-commerce \\
\hline
\end{tabulary}
\end{center}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Pandas Processes Data Perfectly (PPDP)}
\end{mnemonicbox}

\questionmarks{5(b) OR}{4}{How to plot a vertical line and horizontal line in matplotlib? Explain with examples.}
\begin{solutionbox}
\begin{lstlisting}[language=Python]
import matplotlib.pyplot as plt

# Plot a simple line
plt.plot([1, 2, 3], [1, 4, 9])

# Vertical line at x = 2 (Red dashed)
plt.axvline(x=2, color='red', linestyle='--')

# Horizontal line at y = 4 (Green solid)
plt.axhline(y=4, color='green', linestyle='-')

plt.show()
\end{lstlisting}

\textbf{Functions:}
\begin{itemize}
    \item \code{axvline(x)}: Adds a vertical line across the axes.
    \item \code{axhline(y)}: Adds a horizontal line across the axes.
\end{itemize}
\end{solutionbox}

\questionmarks{5(c) OR}{7}{Describe the concept of clustering using appropriate real-world examples.}
\begin{solutionbox}
\textbf{Clustering} is an unsupervised learning technique that groups similar data points such that points in the same group are more similar to each other than to those in other groups.

\begin{center}
\captionof{table}{Clustering Applications}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{Type} & \textbf{Example} & \textbf{Impact} \\
\hline
\textbf{Customer Seg.} & Group by purchase behavior & Targeted marketing \\
\textbf{Image Seg.} & Tumor detection in MRI & Improved diagnosis \\
\textbf{Gene Analysis} & Group genes by expression & Drug discovery \\
\hline
\end{tabulary}
\end{center}

\begin{center}
\begin{tikzpicture}[node distance=1.5cm, auto]
    \node [gtu block] (Raw) {Raw Data};
    \node [gtu block, right=of Raw] (Feat) {Feature Selection};
    \node [gtu block, right=of Feat] (Dist) {Distance Calc};
    \node [gtu block, below=1cm of Feat] (Form) {Cluster Formation};
    \node [gtu block, right=of Form] (Val) {Validation};
    \node [gtu block, right=of Val] (Ins) {Business Insights};
    
    \path [gtu arrow] (Raw) -- (Feat);
    \path [gtu arrow] (Feat) -- (Dist);
    \path [gtu arrow] (Dist) |- (Form);
    \path [gtu arrow] (Form) -- (Val);
    \path [gtu arrow] (Val) -- (Ins);
\end{tikzpicture}
\captionof{figure}{Clustering Process}
\end{center}

\textbf{Real-World Examples:}
\begin{enumerate}
    \item \textbf{Customer Segmentation}: Identifying high-value customers vs. seasonal shoppers.
    \item \textbf{Social Media Analysis}: Grouping users by interests (e.g., sports, tech).
    \item \textbf{Market Research}: Finding segments with similar product needs.
\end{enumerate}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Clustering Creates Clear Categories (CCCC)}
\end{mnemonicbox}

\end{document}
