\documentclass{article}
\input{/Users/milav/Code/milav-next/latex-templates/gtu-solutions/preamble.tex}
\input{/Users/milav/Code/milav-next/latex-templates/gtu-solutions/english-boxes.tex}
\input{/Users/milav/Code/milav-next/latex-templates/gtu-solutions/commands.tex}

\title{Fundamentals of Machine Learning (4341603) - Summer 2023 Solution}
\date{July 18, 2023}

\begin{document}
\maketitle

\questionmarks{1(a)}{3}{Define human learning. List out types of human learning.}

\begin{solutionbox}
Human learning is the process by which humans acquire new knowledge, skills, behaviors, or modify existing ones through experience, study, or instruction.

\begin{center}
\captionof{table}{Types of Human Learning}
\begin{tabulary}{\linewidth}{|L|L|}
\hline
\textbf{Type} & \textbf{Description} \\ \hline
\textbf{Supervised Learning} & Learning with guidance from teacher/mentor \\ \hline
\textbf{Unsupervised Learning} & Self-directed learning without external guidance \\ \hline
\textbf{Reinforcement Learning} & Learning through trial and error with feedback \\ \hline
\end{tabulary}
\end{center}

\begin{mnemonicbox}SUR - Supervised, Unsupervised, Reinforcement\end{mnemonicbox}
\end{solutionbox}

\questionmarks{1(b)}{4}{Differentiate between qualitative data and quantitative data.}

\begin{solutionbox}
\begin{center}
\captionof{table}{Qualitative vs Quantitative Data}
\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{Feature} & \textbf{Qualitative Data} & \textbf{Quantitative Data} \\ \hline
\textbf{Nature} & Descriptive, categorical & Numerical, measurable \\ \hline
\textbf{Analysis} & Subjective interpretation & Statistical analysis \\ \hline
\textbf{Examples} & Colors, names, gender & Height, weight, age \\ \hline
\textbf{Representation} & Words, categories & Numbers, graphs \\ \hline
\end{tabulary}
\end{center}

\begin{mnemonicbox}QUAN-Numbers, QUAL-Words\end{mnemonicbox}
\end{solutionbox}

\questionmarks{1(c)}{7}{Compare the different types of machine learning.}

\begin{solutionbox}
\begin{center}
\captionof{table}{Types of Machine Learning Comparison}
\begin{tabulary}{\linewidth}{|L|L|L|L|}
\hline
\textbf{Type} & \textbf{Training Data} & \textbf{Goal} & \textbf{Examples} \\ \hline
\textbf{Supervised} & Labeled data & Predict outcomes & Classification, Regression \\ \hline
\textbf{Unsupervised} & Unlabeled data & Find patterns & Clustering, Association \\ \hline
\textbf{Reinforcement} & Reward/penalty & Maximize rewards & Gaming, Robotics \\ \hline
\end{tabulary}
\end{center}

\textbf{Key Differences:}
\begin{itemize}
    \item \textbf{Supervised}: Uses input-output pairs for training
    \item \textbf{Unsupervised}: Discovers hidden patterns in data
    \item \textbf{Reinforcement}: Learns through interaction with environment
\end{itemize}

\begin{mnemonicbox}SUR-LAP: Supervised-Labeled, Unsupervised-Reveal, Reinforcement-Action\end{mnemonicbox}
\end{solutionbox}

\questionmarks{1(c OR)}{7}{Define machine learning. Explain any four applications of machine learning in brief.}

\begin{solutionbox}
Machine learning is a subset of artificial intelligence that enables computers to learn and make decisions from data without being explicitly programmed.

\begin{center}
\captionof{table}{Four Applications}
\begin{tabulary}{\linewidth}{|L|L|}
\hline
\textbf{Application} & \textbf{Description} \\ \hline
\textbf{Email Spam Detection} & Classifies emails as spam or legitimate \\ \hline
\textbf{Image Recognition} & Identifies objects in photos \\ \hline
\textbf{Recommendation Systems} & Suggests products/content to users \\ \hline
\textbf{Medical Diagnosis} & Assists doctors in disease detection \\ \hline
\end{tabulary}
\end{center}

\begin{mnemonicbox}SIRM - Spam, Image, Recommendation, Medical\end{mnemonicbox}
\end{solutionbox}

\questionmarks{2(a)}{3}{Relate the appropriate data type of following examples.}

\begin{solutionbox}
\begin{center}
\captionof{table}{Data Type Classification}
\begin{tabulary}{\linewidth}{|L|L|}
\hline
\textbf{Example} & \textbf{Data Type} \\ \hline
\textbf{Nationality of students} & Categorical (Nominal) \\ \hline
\textbf{Education status of students} & Categorical (Ordinal) \\ \hline
\textbf{Height of students} & Numerical (Continuous) \\ \hline
\end{tabulary}
\end{center}

\begin{mnemonicbox}NCN - Nominal, Categorical, Numerical\end{mnemonicbox}
\end{solutionbox}

\questionmarks{2(b)}{4}{Explain data pre-processing in brief.}

\begin{solutionbox}
Data pre-processing is the technique of preparing raw data for machine learning algorithms.

\begin{center}
\captionof{table}{Key Steps}
\begin{tabulary}{\linewidth}{|L|L|}
\hline
\textbf{Step} & \textbf{Purpose} \\ \hline
\textbf{Data Cleaning} & Remove errors and inconsistencies \\ \hline
\textbf{Data Integration} & Combine data from multiple sources \\ \hline
\textbf{Data Transformation} & Convert data to suitable format \\ \hline
\textbf{Data Reduction} & Reduce data size while preserving information \\ \hline
\end{tabulary}
\end{center}

\begin{mnemonicbox}CITR - Clean, Integrate, Transform, Reduce\end{mnemonicbox}
\end{solutionbox}

\questionmarks{2(c)}{7}{Show K-fold cross validation in detail.}

\begin{solutionbox}
K-fold cross validation is a technique to evaluate model performance by dividing data into K equal parts.

\begin{center}
\begin{tikzpicture}[node distance=1.5cm, auto]
    \node [gtu block] (Data) {Original Dataset};
    \node [gtu block, below=1.2cm of Data] (Split) {Split into K folds};
    \node [gtu block, below left=1.5cm and -0.5cm of Split] (Train) {Use K-1 folds\\for training};
    \node [gtu block, below right=1.5cm and -0.5cm of Split] (Test) {Use 1 fold\\for testing};
    \node [gtu state, below=2cm of Split] (Loop) {Repeat K times};
    \node [gtu block, below=1.5cm of Loop] (Average) {Average results};

    \path [gtu arrow] (Data) -- (Split);
    \path [gtu arrow] (Split) -- (Train);
    \path [gtu arrow] (Split) -- (Test);
    \path [gtu arrow] (Train) -- (Loop);
    \path [gtu arrow] (Test) -- (Loop);
    \path [gtu arrow] (Loop) -- (Average);
\end{tikzpicture}
\captionof{figure}{K-Fold Cross Validation Process}
\end{center}

\textbf{Steps:}
\begin{itemize}
    \item \textbf{Divide}: Split dataset into K equal parts
    \item \textbf{Train}: Use K-1 folds for training
    \item \textbf{Test}: Use remaining fold for validation
    \item \textbf{Repeat}: Perform K iterations
    \item \textbf{Average}: Calculate mean performance
\end{itemize}

\textbf{Advantages:}
\begin{itemize}
    \item Reduces overfitting
    \item Better use of limited data
    \item More reliable performance estimate
\end{itemize}

\begin{mnemonicbox}DTRA - Divide, Train, Repeat, Average\end{mnemonicbox}
\end{solutionbox}

\questionmarks{2(a OR)}{3}{Define following terms: i) Mean, ii) Outliers, iii) Interquartile range}

\begin{solutionbox}
\begin{center}
\captionof{table}{Statistical Terms}
\begin{tabulary}{\linewidth}{|L|L|}
\hline
\textbf{Term} & \textbf{Definition} \\ \hline
\textbf{Mean} & Average of all values in dataset \\ \hline
\textbf{Outliers} & Data points significantly different from others \\ \hline
\textbf{Interquartile Range} & Difference between 75th and 25th percentiles \\ \hline
\end{tabulary}
\end{center}

\begin{mnemonicbox}MOI - Mean, Outliers, Interquartile\end{mnemonicbox}
\end{solutionbox}

\questionmarks{2(b OR)}{4}{Explain structure of confusion matrix.}

\begin{solutionbox}
\textbf{Confusion Matrix Structure:}

\begin{center}
\captionof{table}{Confusion Matrix}
\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
 & \textbf{Predicted Positive} & \textbf{Predicted Negative} \\ \hline
\textbf{Actual Positive} & True Positive (TP) & False Negative (FN) \\ \hline
\textbf{Actual Negative} & False Positive (FP) & True Negative (TN) \\ \hline
\end{tabulary}
\end{center}

\textbf{Components:}
\begin{itemize}
    \item \textbf{TP}: Correctly predicted positive cases
    \item \textbf{TN}: Correctly predicted negative cases
    \item \textbf{FP}: Incorrectly predicted as positive
    \item \textbf{FN}: Incorrectly predicted as negative
\end{itemize}

\begin{mnemonicbox}TTFF - True True, False False\end{mnemonicbox}
\end{solutionbox}

\questionmarks{2(c OR)}{7}{Prepare short note on feature subset selection.}

\begin{solutionbox}
Feature subset selection is the process of selecting relevant features from the original feature set.

\begin{center}
\captionof{table}{Methods}
\begin{tabulary}{\linewidth}{|L|L|}
\hline
\textbf{Method} & \textbf{Description} \\ \hline
\textbf{Filter Methods} & Use statistical measures to rank features \\ \hline
\textbf{Wrapper Methods} & Use ML algorithms to evaluate feature subsets \\ \hline
\textbf{Embedded Methods} & Feature selection during model training \\ \hline
\end{tabulary}
\end{center}

\textbf{Benefits:}
\begin{itemize}
    \item \textbf{Reduced complexity}: Fewer features, simpler models
    \item \textbf{Improved performance}: Eliminates noise and irrelevant features
    \item \textbf{Faster training}: Less computational overhead
\end{itemize}

\textbf{Popular Techniques:}
\begin{itemize}
    \item Chi-square test
    \item Recursive Feature Elimination
    \item LASSO regularization
\end{itemize}

\begin{mnemonicbox}FWE - Filter, Wrapper, Embedded\end{mnemonicbox}
\end{solutionbox}

\questionmarks{3(a)}{3}{Give the difference between predictive model and descriptive model.}

\begin{solutionbox}
\begin{center}
\captionof{table}{Predictive vs Descriptive Models}
\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{Feature} & \textbf{Predictive Model} & \textbf{Descriptive Model} \\ \hline
\textbf{Purpose} & Forecast future outcomes & Understand current patterns \\ \hline
\textbf{Output} & Predictions/classifications & Insights/summaries \\ \hline
\textbf{Examples} & Regression, classification & Clustering, association rules \\ \hline
\end{tabulary}
\end{center}

\begin{mnemonicbox}PF-DC: Predictive-Future, Descriptive-Current\end{mnemonicbox}
\end{solutionbox}

\questionmarks{3(b)}{4}{Discuss the difference between classification and regression.}

\begin{solutionbox}
\begin{center}
\captionof{table}{Classification vs Regression}
\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{Aspect} & \textbf{Classification} & \textbf{Regression} \\ \hline
\textbf{Output} & Discrete categories & Continuous values \\ \hline
\textbf{Goal} & Predict class labels & Predict numerical values \\ \hline
\textbf{Examples} & Spam detection, image recognition & Price prediction, temperature \\ \hline
\textbf{Evaluation} & Accuracy, precision, recall & MSE, RMSE, R-squared \\ \hline
\end{tabulary}
\end{center}

\begin{mnemonicbox}CCNM - Classification-Categories, Regression-Numbers\end{mnemonicbox}
\end{solutionbox}

\questionmarks{3(c)}{7}{Define classification. Illustrate classification learning steps in details.}

\begin{solutionbox}
Classification is a supervised learning technique that predicts discrete class labels for input data.

\begin{center}
\begin{tikzpicture}[node distance=1.2cm, auto]
    \node [gtu block] (Data) {Data Collection};
    \node [gtu block, below=0.8cm of Data] (Pre) {Data Preprocessing};
    \node [gtu block, below=0.8cm of Pre] (Feature) {Feature Selection};
    \node [gtu block, below=0.8cm of Feature] (Split) {Train-Test Split};
    
    \node [gtu block, below left=1.2cm and -1.5cm of Split] (Train) {Model Training};
    \node [gtu block, below right=1.2cm and -1.5cm of Split] (Eval) {Model Evaluation};
    
    \node [gtu block, below=2cm of Split] (Deploy) {Model Deployment};

    \path [gtu arrow] (Data) -- (Pre);
    \path [gtu arrow] (Pre) -- (Feature);
    \path [gtu arrow] (Feature) -- (Split);
    \path [gtu arrow] (Split) -- (Train);
    \path [gtu arrow] (Split) -- (Eval);
    \path [gtu arrow] (Train) -- (Deploy);
    \path [gtu arrow] (Eval) -- (Deploy);
\end{tikzpicture}
\captionof{figure}{Classification Learning Steps}
\end{center}

\textbf{Detailed Steps:}
\begin{itemize}
    \item \textbf{Data Collection}: Gather labeled training data
    \item \textbf{Preprocessing}: Clean and prepare data
    \item \textbf{Feature Selection}: Choose relevant attributes
    \item \textbf{Split Data}: Divide into training and testing sets
    \item \textbf{Training}: Build model using training data
    \item \textbf{Evaluation}: Test model performance
    \item \textbf{Deployment}: Use model for predictions
\end{itemize}

\begin{mnemonicbox}DCFSTED - Data, Clean, Features, Split, Train, Evaluate, Deploy\end{mnemonicbox}
\end{solutionbox}

\questionmarks{3(a OR)}{3}{Give the difference between bagging and boosting.}

\begin{solutionbox}
\begin{center}
\captionof{table}{Bagging vs Boosting}
\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{Feature} & \textbf{Bagging} & \textbf{Boosting} \\ \hline
\textbf{Sampling} & Bootstrap sampling & Sequential weighted sampling \\ \hline
\textbf{Training} & Parallel training & Sequential training \\ \hline
\textbf{Focus} & Reduce variance & Reduce bias \\ \hline
\end{tabulary}
\end{center}

\begin{mnemonicbox}BPV-BSB: Bagging-Parallel-Variance, Boosting-Sequential-Bias\end{mnemonicbox}
\end{solutionbox}

\questionmarks{3(b OR)}{4}{Explain different types of logistic regression in brief.}

\begin{solutionbox}
\begin{center}
\captionof{table}{Types of Logistic Regression}
\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{Type} & \textbf{Classes} & \textbf{Use Case} \\ \hline
\textbf{Binary} & 2 classes & Yes/No, Pass/Fail \\ \hline
\textbf{Multinomial} & 3+ classes (unordered) & Color classification \\ \hline
\textbf{Ordinal} & 3+ classes (ordered) & Rating scales \\ \hline
\end{tabulary}
\end{center}

\begin{mnemonicbox}BMO - Binary, Multinomial, Ordinal\end{mnemonicbox}
\end{solutionbox}

\questionmarks{3(c OR)}{7}{Write and show the use of k-NN algorithms.}

\begin{solutionbox}
K-Nearest Neighbors (k-NN) is a lazy learning algorithm that classifies data points based on the majority class of k nearest neighbors.

\textbf{Algorithm Steps:}
\begin{enumerate}
    \item Choose value of k
    \item Calculate distance to all training points
    \item Select k nearest neighbors
    \item For classification: majority vote; For regression: average of k neighbors
    \item Assign class/value to test point
\end{enumerate}

\textbf{Distance Calculation:}
\begin{itemize}
    \item \textbf{Euclidean Distance}: $\sqrt{(x_1-x_2)^2 + (y_1-y_2)^2}$
\end{itemize}

\textbf{Applications:}
\begin{itemize}
    \item \textbf{Recommendation systems}: Similar user preferences
    \item \textbf{Image recognition}: Pattern matching
    \item \textbf{Medical diagnosis}: Symptom similarity
\end{itemize}

\textbf{Advantages:}
\begin{itemize}
    \item Simple to implement
    \item No training required
    \item Works well with small datasets
\end{itemize}

\begin{mnemonicbox}CDSA - Choose, Distance, Select, Assign\end{mnemonicbox}
\end{solutionbox}

\questionmarks{4(a)}{3}{List out applications of support vector machine.}

\begin{solutionbox}
\begin{center}
\captionof{table}{SVM Applications}
\begin{tabulary}{\linewidth}{|L|L|}
\hline
\textbf{Application} & \textbf{Domain} \\ \hline
\textbf{Text Classification} & Document categorization \\ \hline
\textbf{Image Recognition} & Face detection \\ \hline
\textbf{Bioinformatics} & Gene classification \\ \hline
\end{tabulary}
\end{center}

\begin{mnemonicbox}TIB - Text, Image, Bio\end{mnemonicbox}
\end{solutionbox}

\questionmarks{4(b)}{4}{Create pseudo code for k-means algorithm.}

\begin{solutionbox}
\textbf{K-means Pseudo Code:}
\begin{lstlisting}[language=python, frame=single]
BEGIN K-means
1. Initialize k cluster centroids randomly
2. REPEAT
   a. Assign each point to nearest centroid
   b. Update centroids to mean of assigned points
   c. Calculate total within-cluster sum of squares
3. UNTIL convergence or max iterations
4. RETURN final clusters and centroids
END
\end{lstlisting}

\begin{mnemonicbox}IAUC - Initialize, Assign, Update, Check\end{mnemonicbox}
\end{solutionbox}

\questionmarks{4(c)}{7}{Write and explain applications of unsupervised learning.}

\begin{solutionbox}
Unsupervised learning discovers hidden patterns in data without labeled examples.

\begin{center}
\captionof{table}{Major Applications}
\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{Application} & \textbf{Description} & \textbf{Example} \\ \hline
\textbf{Customer Segmentation} & Group customers by behavior & Market research \\ \hline
\textbf{Anomaly Detection} & Identify unusual patterns & Fraud detection \\ \hline
\textbf{Data Compression} & Reduce dimensionality & Image compression \\ \hline
\textbf{Association Rules} & Find item relationships & Market basket analysis \\ \hline
\end{tabulary}
\end{center}

\textbf{Clustering Applications:}
\begin{itemize}
    \item \textbf{Market research}: Customer grouping
    \item \textbf{Social network analysis}: Community detection
    \item \textbf{Gene sequencing}: Biological classification
\end{itemize}

\textbf{Dimensionality Reduction:}
\begin{itemize}
    \item \textbf{Visualization}: High-dimensional data plotting
    \item \textbf{Feature extraction}: Noise reduction
\end{itemize}

\begin{mnemonicbox}CADA - Customer, Anomaly, Data, Association\end{mnemonicbox}
\end{solutionbox}

\questionmarks{4(a OR)}{3}{List out applications of regression.}

\begin{solutionbox}
\begin{center}
\captionof{table}{Regression Applications}
\begin{tabulary}{\linewidth}{|L|L|}
\hline
\textbf{Application} & \textbf{Purpose} \\ \hline
\textbf{Stock Price Prediction} & Financial forecasting \\ \hline
\textbf{Sales Forecasting} & Business planning \\ \hline
\textbf{Medical Diagnosis} & Risk assessment \\ \hline
\end{tabulary}
\end{center}

\begin{mnemonicbox}SSM - Stock, Sales, Medical\end{mnemonicbox}
\end{solutionbox}

\questionmarks{4(b OR)}{4}{Define following terms: i) Support ii) Confidence}

\begin{solutionbox}
\begin{center}
\captionof{table}{Association Rule Terms}
\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{Term} & \textbf{Definition} & \textbf{Formula} \\ \hline
\textbf{Support} & Frequency of itemset in database & $Support(A) = \frac{|A|}{|D|}$ \\ \hline
\textbf{Confidence} & Conditional probability of rule & $Confidence(A \to B) = \frac{Support(A \cup B)}{Support(A)}$ \\ \hline
\end{tabulary}
\end{center}

\textbf{Example:}
\begin{itemize}
    \item If 30\% transactions contain bread and milk: Support = 0.3
    \item If 80\% of bread buyers also buy milk: Confidence = 0.8
\end{itemize}

\begin{mnemonicbox}SF-CP: Support-Frequency, Confidence-Probability\end{mnemonicbox}
\end{solutionbox}

\questionmarks{4(c OR)}{7}{Explain apriori algorithm in detail.}

\begin{solutionbox}
Apriori algorithm finds frequent itemsets in transactional data using the apriori property.

\begin{center}
\begin{tikzpicture}[node distance=1.5cm, auto]
    \node [gtu block] (Find) {Find frequent\\1-itemsets};
    \node [gtu block, below=1.2cm of Find] (Gen) {Generate candidate\\2-itemsets};
    \node [gtu block, below=1.2cm of Gen] (Prune) {Prune using\\apriori property};
    \node [gtu block, below=1.2cm of Prune] (Count) {Count support\\in database};
    \node [gtu block, below=1.2cm of Count] (FindK) {Find frequent\\k-itemsets};
    \node [gtu state, right=2cm of Prune] (Check) {More\\candidates?};

    \path [gtu arrow] (Find) -- (Gen);
    \path [gtu arrow] (Gen) -- (Prune);
    \path [gtu arrow] (Prune) -- (Count);
    \path [gtu arrow] (Count) -- (FindK);
    \path [gtu arrow] (FindK) -| (Check);
    \path [gtu arrow] (Check) |- node[above, near end] {Yes} (Gen);
    \path [gtu arrow] (Check) -- node[right] {No} ++(0,-2) node[gtu block, anchor=north] {Generate rules};

\end{tikzpicture}
\captionof{figure}{Apriori Algorithm Process}
\end{center}

\textbf{Apriori Property:}
\begin{itemize}
    \item If an itemset is frequent, all its subsets are frequent
    \item If an itemset is infrequent, all its supersets are infrequent
\end{itemize}

\textbf{Steps:}
\begin{enumerate}
    \item \textbf{Scan database}: Count 1-item support
    \item \textbf{Generate candidates}: Create k+1 itemsets from frequent k-itemsets
    \item \textbf{Prune}: Remove candidates with infrequent subsets
    \item \textbf{Count support}: Scan database for candidate frequencies
    \item \textbf{Repeat}: Until no new frequent itemsets found
\end{enumerate}

\textbf{Applications:}
\begin{itemize}
    \item Market basket analysis
    \item Web usage patterns
    \item Protein sequences
\end{itemize}

\begin{mnemonicbox}SGPCR - Scan, Generate, Prune, Count, Repeat\end{mnemonicbox}
\end{solutionbox}

\questionmarks{5(a)}{3}{List out the major features of matplotlib.}

\begin{solutionbox}
\begin{center}
\captionof{table}{Matplotlib Features}
\begin{tabulary}{\linewidth}{|L|L|}
\hline
\textbf{Feature} & \textbf{Description} \\ \hline
\textbf{Multiple Plot Types} & Line, bar, scatter, histogram \\ \hline
\textbf{Customization} & Colors, styles, labels \\ \hline
\textbf{Export Options} & PNG, PDF, SVG formats \\ \hline
\end{tabulary}
\end{center}

\begin{mnemonicbox}MCE - Multiple, Customization, Export\end{mnemonicbox}
\end{solutionbox}

\questionmarks{5(b)}{4}{How to load iris dataset in Numpy program? Explain.}

\begin{solutionbox}
\textbf{Loading Iris Dataset in NumPy:}
\begin{lstlisting}[language=python]
import numpy as np
from sklearn.datasets import load_iris

# Load iris dataset
iris = load_iris()
data = iris.data    # Features
target = iris.target # Labels
\end{lstlisting}

\textbf{Steps:}
\begin{itemize}
    \item \textbf{Import}: Import required libraries
    \item \textbf{Load}: Use sklearn's load\_iris() function
    \item \textbf{Extract}: Get features and target arrays
    \item \textbf{Access}: Use .data and .target attributes
\end{itemize}

\begin{mnemonicbox}ILEA - Import, Load, Extract, Access\end{mnemonicbox}
\end{solutionbox}

\questionmarks{5(c)}{7}{Explain features and applications of Pandas.}

\begin{solutionbox}
Pandas is a powerful data manipulation and analysis library for Python.

\begin{center}
\captionof{table}{Key Features}
\begin{tabulary}{\linewidth}{|L|L|}
\hline
\textbf{Feature} & \textbf{Description} \\ \hline
\textbf{DataFrame} & 2D labeled data structure \\ \hline
\textbf{Series} & 1D labeled array \\ \hline
\textbf{Data I/O} & Read/write various file formats \\ \hline
\textbf{Data Cleaning} & Handle missing values \\ \hline
\textbf{Grouping} & Group and aggregate operations \\ \hline
\end{tabulary}
\end{center}

\textbf{Applications:}
\begin{itemize}
    \item \textbf{Data Analysis}: Statistical analysis
    \item \textbf{Data Cleaning}: Preprocessing for ML
    \item \textbf{Financial Analysis}: Stock market data
    \item \textbf{Web Scraping}: Parse HTML tables
\end{itemize}

\textbf{Common Operations:}
\begin{itemize}
    \item \textbf{Reading data}: \code{pd.read\_csv()}, \code{pd.read\_excel()}
    \item \textbf{Filtering}: \code{df[df['column'] > value]}
    \item \textbf{Grouping}: \code{df.groupby('column').mean()}
\end{itemize}

\begin{mnemonicbox}DSDCG - DataFrame, Series, Data I/O, Cleaning, Grouping\end{mnemonicbox}
\end{solutionbox}

\questionmarks{5(a OR)}{3}{List out the applications of matplotlib.}

\begin{solutionbox}
\begin{center}
\captionof{table}{Matplotlib Applications}
\begin{tabulary}{\linewidth}{|L|L|}
\hline
\textbf{Application} & \textbf{Purpose} \\ \hline
\textbf{Scientific Visualization} & Research data plotting \\ \hline
\textbf{Business Analytics} & Dashboard creation \\ \hline
\textbf{Educational Content} & Teaching materials \\ \hline
\end{tabulary}
\end{center}

\begin{mnemonicbox}SBE - Scientific, Business, Educational\end{mnemonicbox}
\end{solutionbox}

\questionmarks{5(b OR)}{4}{Develop and explain the steps to import csv file in Pandas.}

\begin{solutionbox}
\textbf{Steps to Import CSV in Pandas:}
\begin{lstlisting}[language=python]
import pandas as pd

# Step 1: Import pandas library
# Step 2: Use read_csv() function
df = pd.read_csv('filename.csv')

# Optional parameters
df = pd.read_csv('file.csv', 
                 header=0,     # First row as header
                 sep=',',      # Comma separator
                 index_col=0)  # First column as index
\end{lstlisting}

\textbf{Process:}
\begin{itemize}
    \item \textbf{Import}: Import pandas library
    \item \textbf{Read}: Use \code{pd.read\_csv()} function
    \item \textbf{Specify}: Add file path and parameters
    \item \textbf{Store}: Assign to DataFrame variable
\end{itemize}

\begin{mnemonicbox}IRSS - Import, Read, Specify, Store\end{mnemonicbox}
\end{solutionbox}

\questionmarks{5(c OR)}{7}{Explain features and applications of Scikit-Learn.}

\begin{solutionbox}
Scikit-Learn is a comprehensive machine learning library for Python.

\begin{center}
\captionof{table}{Key Features}
\begin{tabulary}{\linewidth}{|L|L|}
\hline
\textbf{Feature} & \textbf{Description} \\ \hline
\textbf{Algorithms} & Classification, regression, clustering \\ \hline
\textbf{Preprocessing} & Data scaling and transformation \\ \hline
\textbf{Model Selection} & Cross-validation and grid search \\ \hline
\textbf{Metrics} & Performance evaluation tools \\ \hline
\end{tabulary}
\end{center}

\textbf{Applications:}
\begin{itemize}
    \item \textbf{Healthcare}: Disease prediction
    \item \textbf{Finance}: Credit scoring
    \item \textbf{Marketing}: Customer segmentation
    \item \textbf{Technology}: Recommendation systems
\end{itemize}

\textbf{Algorithm Categories:}
\begin{itemize}
    \item \textbf{Supervised}: SVM, Random Forest, Linear Regression
    \item \textbf{Unsupervised}: K-means, DBSCAN, PCA
    \item \textbf{Ensemble}: Bagging, Boosting
\end{itemize}

\textbf{Workflow:}
\begin{enumerate}
    \item \textbf{Data preparation}: Preprocessing
    \item \textbf{Model selection}: Choose algorithm
    \item \textbf{Training}: Fit model to data
    \item \textbf{Evaluation}: Assess performance
    \item \textbf{Prediction}: Make forecasts
\end{enumerate}

\begin{mnemonicbox}APME - Algorithms, Preprocessing, Metrics, Evaluation\end{mnemonicbox}
\end{solutionbox}

\end{document}
