\documentclass{article}
\input{../../../../../../../latex-templates/gtu-solutions-short/preamble.tex}
\input{../../../../../../../latex-templates/gtu-solutions-short/gujarati-boxes.tex}
\input{../../../../../../../latex-templates/gtu-solutions-short/commands.tex}

\title{Foundation of AI and ML (4351601) - Summer 2024 Solution}
\date{May 16, 2024}

\begin{document}
\maketitle

\questionmarks{1(a)}{3}{Narrow AI અથવા Weak AI નો અર્થ શું છે?}
\begin{solutionbox}
\textbf{Narrow AI} અથવા \textbf{Weak AI} એ specific અને limited કાર્યો માટે બનાવેલ artificial intelligence systems છે.

\begin{center}
\captionof{table}{Narrow AI ની લાક્ષણિકતાઓ}
\begin{tabulary}{\linewidth}{L L}
\hline
\textbf{પાસું} & \textbf{વર્ણન} \\
\hline
\textbf{વ્યાપ્તિ} & ફક્ત specific કાર્યો માટે \\
\textbf{બુદ્ધિમત્તા} & કાર્ય-વિશિષ્ટ કુશળતા \\
\textbf{ઉદાહરણો} & Siri, chess programs, recommendation systems \\
\textbf{શીખવાની પ્રક્રિયા} & Domain માં pattern recognition \\
\hline
\end{tabulary}
\end{center}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{Narrow = ફક્ત વિશિષ્ટ કાર્યો}
\end{mnemonicbox}

\questionmarks{1(b)}{4}{વ્યાખ્યાયિત કરો: વર્ગીકરણ, રીગ્રેસન, ક્લસ્ટરિંગ, એસોસિએશન વિશ્લેષણ.}
\begin{solutionbox}
\begin{center}
\captionof{table}{Machine Learning ની તકનીકો}
\begin{tabulary}{\linewidth}{L L L L}
\hline
\textbf{તકનીક} & \textbf{વ્યાખ્યા} & \textbf{પ્રકાર} & \textbf{ઉદાહરણ} \\
\hline
\textbf{વર્ગીકરણ} & Discrete categories/classes predict કરે છે & Supervised & Email spam detection \\
\textbf{રીગ્રેસન} & Continuous numerical values predict કરે છે & Supervised & House price prediction \\
\textbf{ક્લસ્ટરિંગ} & Similar data points ને group કરે છે & Unsupervised & Customer segmentation \\
\textbf{એસોસિએશન વિશ્લેષણ} & Variables વચ્ચે relationships શોધે છે & Unsupervised & Market basket analysis \\
\hline
\end{tabulary}
\end{center}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{CRCA - Categories, Real numbers, Clusters, Associations}
\end{mnemonicbox}

\questionmarks{1(c)}{7}{ન્યુરોનના ત્રણ મુખ્ય ઘટકોને પ્રકાશિત કરો.}
\begin{solutionbox}
Biological neuron ના ત્રણ મુખ્ય ઘટકો જે artificial neural networks ને inspire કરે છે:

\begin{center}
\begin{tikzpicture}[node distance=1.5cm, auto]
    \node [gtu block] (Dendrites) {Dendrites};
    \node [gtu block, right=of Dendrites] (Soma) {Cell Body (Soma)};
    \node [gtu block, right=of Soma] (Axon) {Axon};
    \node [gtu state, below=0.5cm of Dendrites] {Input\\Receivers};
    \node [gtu state, below=0.5cm of Soma] {Processing\\Unit};
    \node [gtu state, below=0.5cm of Axon] {Output\\Transmitter};
    
    \path [gtu arrow] (Dendrites) -- node[above] {Signals} (Soma);
    \path [gtu arrow] (Soma) -- node[above] {Impulses} (Axon);
\end{tikzpicture}
\captionof{figure}{ન્યુરોન ઘટકો}
\end{center}

\begin{center}
\captionof{table}{ન્યુરોન ઘટકો}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{ઘટક} & \textbf{કાર્ય} & \textbf{AI માં સમકક્ષ} \\
\hline
\textbf{Dendrites} & અન્ય neurons થી input signals receive કરે છે & Input layer/weights \\
\textbf{Cell Body (Soma)} & Signals ને process અને integrate કરે છે & Activation function \\
\textbf{Axon} & અન્ય neurons ને output signals transmit કરે છે & Output connections \\
\hline
\end{tabulary}
\end{center}

\textbf{મુખ્ય મુદ્દાઓ:}
\begin{itemize}
    \item \textbf{Dendrites}: વિવિધ connection strengths સાથે input receivers તરીકે કામ કરે છે.
    \item \textbf{Cell Body}: Inputs ને sum કરે છે અને threshold function apply કરે છે.
    \item \textbf{Axon}: Processed signal ને આગળના neurons સુધી પહોંચાડે છે.
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{DCA - Dendrites Collect, Cell-body Calculates, Axon Announces}
\end{mnemonicbox}

\questionmarks{1(c) OR}{7}{Artificial Neural Network માં back propagation પદ્ધતિ સમજાવો.}
\begin{solutionbox}
\textbf{Back Propagation} એ supervised learning algorithm છે જે gradient descent દ્વારા error minimize કરીને multi-layer neural networks ને train કરે છે.

\begin{center}
\begin{tikzpicture}[node distance=1.5cm, auto]
    \node [gtu block] (Forward) {Forward Pass};
    \node [gtu block, right=of Forward] (Output) {Output Calculate કરો};
    \node [gtu block, below=1cm of Output] (Error) {Error Calculate કરો};
    \node [gtu block, left=of Error] (Backward) {Backward Pass};
    \node [gtu block, below=1cm of Backward] (Gradient) {Gradients Calculate કરો};
    \node [gtu block, right=of Gradient] (Update) {Weights Update કરો};
    \node [gtu decision, below=1cm of Update] (Check) {Error સ્વીકાર્ય છે?};
    \node [gtu state, right=of Check] (Done) {Training Complete};
    
    \path [gtu arrow] (Forward) -- (Output);
    \path [gtu arrow] (Output) -- (Error);
    \path [gtu arrow] (Error) -- (Backward);
    \path [gtu arrow] (Backward) -- (Gradient);
    \path [gtu arrow] (Gradient) -- (Update);
    \path [gtu arrow] (Update) -- (Check);
    \path [gtu arrow] (Check) -- node[above] {હા} (Done);
    \path [gtu arrow] (Check.west) -- node[above] {ના} ++(-1.5,0) |- (Forward.west);
\end{tikzpicture}
\captionof{figure}{Back Propagation ફ્લો}
\end{center}

\begin{center}
\captionof{table}{Back Propagation Steps}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{સ્ટેપ} & \textbf{પ્રક્રિયા} & \textbf{ફોર્મ્યુલા} \\
\hline
\textbf{Forward Pass} & Layer દ્વારા layer outputs calculate કરો & \(y = f(\Sigma(w_i x_i + b))\) \\
\textbf{Error Calculation} & Loss function compute કરો & \(E = \frac{1}{2}(target - output)^2\) \\
\textbf{Backward Pass} & Error gradients calculate કરો & \(\delta = \partial E/\partial w\) \\
\textbf{Weight Update} & Learning rate વાપરીને weights adjust કરો & \(w_{new} = w_{old} - \eta \cdot \delta\) \\
\hline
\end{tabulary}
\end{center}

\textbf{મુખ્ય લાક્ષણિકતાઓ:}
\begin{itemize}
    \item \textbf{Gradient Descent}: Minimum error શોધવા માટે calculus વાપરે છે.
    \item \textbf{Chain Rule}: Layers દ્વારા error ને backward propagate કરે છે.
    \item \textbf{Learning Rate}: Weight updates ની speed control કરે છે.
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{FEBU - Forward, Error, Backward, Update}
\end{mnemonicbox}

\questionmarks{2(a)}{3}{Machine Learning માં ઉપયોગમાં લેવાતા કોઈપણ પાંચ લોકપ્રિય algorithms ની સૂચિ બનાવો.}
\begin{solutionbox}
\begin{center}
\captionof{table}{લોકપ્રિય ML Algorithms}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{Algorithm} & \textbf{પ્રકાર} & \textbf{Application} \\
\hline
\textbf{Linear Regression} & Supervised & Continuous values નું prediction \\
\textbf{Decision Tree} & Supervised & Classification અને regression \\
\textbf{K-Means Clustering} & Unsupervised & Data grouping \\
\textbf{Support Vector Machine} & Supervised & Margins સાથે classification \\
\textbf{Random Forest} & Supervised & Ensemble learning \\
\hline
\end{tabulary}
\end{center}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{LDKSR - Learn Data, Keep Samples, Run}
\end{mnemonicbox}

\questionmarks{2(b)}{4}{નિષ્ણાત સિસ્ટમ શું છે? તેની મર્યાદાઓ અને applications ની યાદી બનાવો.}
\begin{solutionbox}
\textbf{Expert System} એ AI program છે જે specific domains માં complex problems solve કરવા માટે human expert knowledge ને mimic કરે છે.

\begin{center}
\captionof{table}{Expert System Overview}
\begin{tabulary}{\linewidth}{L L}
\hline
\textbf{પાસું} & \textbf{વિગતો} \\
\hline
\textbf{વ્યાખ્યા} & Domain-specific expertise સાથે AI system \\
\textbf{ઘટકો} & Knowledge base, inference engine, user interface \\
\hline
\end{tabulary}
\end{center}

\textbf{Applications:}
\begin{itemize}
    \item \textbf{Medical Diagnosis}: રોગ identification systems.
    \item \textbf{Financial Planning}: Investment advisory systems.
    \item \textbf{Fault Diagnosis}: Equipment troubleshooting.
\end{itemize}

\textbf{મર્યાદાઓ:}
\begin{itemize}
    \item \textbf{Limited Domain}: ફક્ત specific areas માં કામ કરે છે.
    \item \textbf{Knowledge Acquisition}: Expert knowledge extract કરવું મુશ્કેલ.
    \item \textbf{Maintenance}: Rules update અને modify કરવા મુશ્કેલ.
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{EXPERT - Explains Problems, Executes Rules, Tests}
\end{mnemonicbox}

\questionmarks{2(c)}{7}{ટોકનાઇઝેશન શું છે? યોગ્ય ઉદાહરણ સાથે સમજાવો.}
\begin{solutionbox}
\textbf{Tokenization} એ text ને smaller units (tokens) માં break down કરવાની process છે NLP processing માટે.

\begin{center}
\captionof{table}{Tokenization ના પ્રકારો}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{પ્રકાર} & \textbf{વર્ણન} & \textbf{ઉદાહરણ} \\
\hline
\textbf{Word Tokenization} & Words દ્વારા split કરે છે & "Hello world" $\to$ ["Hello", "world"] \\
\textbf{Sentence Tokenization} & Sentences દ્વારા split કરે છે & "Hi. How are you?" $\to$ ["Hi.", "How are you?"] \\
\textbf{Subword Tokenization} & Subwords માં split કરે છે & "unhappy" $\to$ ["un", "happy"] \\
\hline
\end{tabulary}
\end{center}

\textbf{Code ઉદાહરણ:}
\begin{lstlisting}[language=python]
import nltk
text = "Natural Language Processing is amazing!"
tokens = nltk.word_tokenize(text)
# Output: ['Natural', 'Language', 'Processing', 'is', 'amazing', '!']
\end{lstlisting}

\begin{center}
\begin{tikzpicture}[node distance=1.5cm, auto]
    \node [gtu block] (Raw) {Raw Text};
    \node [gtu block, right=of Raw] (Token) {Tokenization};
    \node [gtu block, right=of Token] (Clean) {Clean Tokens};
    \node [gtu block, right=of Clean] (Process) {Further Processing};
    
    \path [gtu arrow] (Raw) -- (Token);
    \path [gtu arrow] (Token) -- (Clean);
    \path [gtu arrow] (Clean) -- (Process);
\end{tikzpicture}
\captionof{figure}{Process Flow}
\end{center}

\textbf{મુખ્ય ફાયદા:}
\begin{itemize}
    \item \textbf{Standardization}: Text ને uniform format માં convert કરે છે.
    \item \textbf{Analysis Ready}: ML algorithms માટે text prepare કરે છે.
    \item \textbf{Feature Extraction}: Statistical analysis enable કરે છે.
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{TOKEN - Text Operations Keep Everything Normalized}
\end{mnemonicbox}

\questionmarks{2(a) OR}{3}{સુપરવાઇઝ્ડ અને અનસુપરવાઇઝ્ડ લર્નિંગની સરખામણી કરો.}
\begin{solutionbox}
\begin{center}
\captionof{table}{Supervised vs Unsupervised Learning}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{પાસું} & \textbf{Supervised Learning} & \textbf{Unsupervised Learning} \\
\hline
\textbf{Training Data} & Target outputs સાથે labeled data & Targets વિના unlabeled data \\
\textbf{લક્ષ્ય} & Specific outcomes predict કરવા & Hidden patterns discover કરવા \\
\textbf{ઉદાહરણો} & Classification, Regression & Clustering, Association rules \\
\textbf{મૂલ્યાંકન} & Accuracy, precision, recall & Silhouette score, elbow method \\
\textbf{Applications} & Email spam, price prediction & Customer segmentation, anomaly detection \\
\hline
\end{tabulary}
\end{center}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{SU - Supervised Uses labels, Unsupervised Uncovers patterns}
\end{mnemonicbox}

\questionmarks{2(b) OR}{4}{હેલ્થકેર, ફાઇનાન્સ અને મેન્યુફેક્ચરિંગમાં AI applications વિશે બધું સમજાવો.}
\begin{solutionbox}
\begin{center}
\captionof{table}{Industry પ્રમાણે AI Applications}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{Industry} & \textbf{Applications} & \textbf{ફાયદા} \\
\hline
\textbf{Healthcare} & Medical imaging, drug discovery, diagnosis & Improved accuracy, faster treatment \\
\textbf{Finance} & Fraud detection, algorithmic trading, credit scoring & Risk reduction, automated decisions \\
\textbf{Manufacturing} & Quality control, predictive maintenance, robotics & Efficiency, cost reduction \\
\hline
\end{tabulary}
\end{center}

\textbf{Healthcare ઉદાહરણો:}
\begin{itemize}
    \item \textbf{Medical Imaging}: X-rays અને MRIs માં AI cancer detect કરે છે.
    \item \textbf{Drug Discovery}: AI નવી medicine development ને accelerate કરે છે.
\end{itemize}

\textbf{Finance ઉદાહરણો:}
\begin{itemize}
    \item \textbf{Fraud Detection}: Real-time transaction monitoring.
    \item \textbf{Robo-advisors}: Automated investment management.
\end{itemize}

\textbf{Manufacturing ઉદાહરણો:}
\begin{itemize}
    \item \textbf{Quality Control}: Automated defect detection.
    \item \textbf{Predictive Maintenance}: Equipment failure prediction.
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{HFM - Health, Finance, Manufacturing benefit from AI}
\end{mnemonicbox}

\questionmarks{2(c) OR}{7}{સિન્ટેક્ટિક વિશ્લેષણ શું છે અને તે લેક્સિકલ વિશ્લેષણથી કેવી રીતે અલગ છે?}
\begin{solutionbox}
\textbf{Syntactic Analysis} sentences ના grammatical structure ને examine કરે છે, જ્યારે \textbf{Lexical Analysis} text ને meaningful tokens માં break કરે છે.

\begin{center}
\captionof{table}{Lexical vs Syntactic Analysis}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{પાસું} & \textbf{Lexical Analysis} & \textbf{Syntactic Analysis} \\
\hline
\textbf{હેતુ} & Text ને words માં tokenize કરવા & Grammatical structure parse કરવા \\
\textbf{Input} & Raw text & Lexical analysis થી tokens \\
\textbf{Output} & Tokens, part-of-speech tags & Parse trees, grammar rules \\
\textbf{ધ્યાન} & Individual words & Sentence structure \\
\textbf{ઉદાહરણ} & "The cat runs" $\to$ [The, cat, runs] & Noun-verb relationship દર્શાવતું parse tree બનાવે છે \\
\hline
\end{tabulary}
\end{center}

\begin{center}
\begin{tikzpicture}[node distance=1.5cm, auto]
    \node [gtu block] (Raw) {Raw Text};
    \node [gtu block, right=of Raw] (Lex) {Lexical Analysis};
    \node [gtu block, right=of Lex] (Tok) {Tokens};
    \node [gtu block, below=1cm of Raw] (Syn) {Syntactic Analysis};
    \node [gtu block, right=of Syn] (Tree) {Parse Tree};
    
    \path [gtu arrow] (Raw) -- (Lex);
    \path [gtu arrow] (Lex) -- (Tok);
    \path [gtu arrow] (Tok) |- (Syn);
    \path [gtu arrow] (Syn) -- (Tree);
\end{tikzpicture}
\captionof{figure}{Process Flow}
\end{center}

\textbf{ઉદાહરણ:}
\begin{itemize}
    \item \textbf{Lexical}: "She reads books" $\to$ ["She", "reads", "books"]
    \item \textbf{Syntactic}: "She" ને subject, "reads" ને verb, "books" ને object તરીકે identify કરે છે.
\end{itemize}

\textbf{મુખ્ય તફાવતો:}
\begin{itemize}
    \item \textbf{Scope}: Lexical words પર કામ કરે છે, Syntactic sentence structure પર.
    \item \textbf{જટિલતા}: Syntactic analysis lexical કરતાં વધુ complex છે.
    \item \textbf{Dependencies}: Syntactic analysis lexical analysis પર depend કરે છે.
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{LEX-SYN: LEXical extracts, SYNtactic structures}
\end{mnemonicbox}

\questionmarks{3(a)}{3}{પ્રતિક્રિયાશીલ મશીનોની વિવિધ લાક્ષણિકતાઓની યાદી બનાવો.}
\begin{solutionbox}
\begin{center}
\captionof{table}{Reactive Machines ની લાક્ષણિકતાઓ}
\begin{tabulary}{\linewidth}{L L}
\hline
\textbf{લાક્ષણિકતા} & \textbf{વર્ણન} \\
\hline
\textbf{કોઈ મેમરી નથી} & Past experiences store કરી શકતા નથી \\
\textbf{વર્તમાન-કેન્દ્રિત} & ફક્ત current input ને respond કરે છે \\
\textbf{નિર્ધારિત} & Same input માટે same output આપે છે \\
\textbf{કાર્ય-વિશિષ્ટ} & Particular functions માટે design કરેલ \\
\textbf{કોઈ શીખવું નથી} & Experience થી improve કરી શકતા નથી \\
\hline
\end{tabulary}
\end{center}

\textbf{ઉદાહરણો:}
\begin{itemize}
    \item \textbf{Deep Blue}: IBM નું chess computer.
    \item \textbf{Game AI}: Tic-tac-toe programs.
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{REACT - Responds Exactly, Always Consistent Tasks}
\end{mnemonicbox}

\questionmarks{3(b)}{4}{તફાવત કરો: હકારાત્મક મજબૂતીકરણ v/s નકારાત્મક મજબૂતીકરણ}
\begin{solutionbox}
\begin{center}
\captionof{table}{Positive vs Negative Reinforcement}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{પાસું} & \textbf{Positive Reinforcement} & \textbf{Negative Reinforcement} \\
\hline
\textbf{વ્યાખ્યા} & Good behavior માટે reward add કરવું & Good behavior માટે penalty remove કરવું \\
\textbf{Action} & કંઈક desirable આપવું & કંઈક undesirable દૂર કરવું \\
\textbf{લક્ષ્ય} & Desired behavior increase કરવું & Desired behavior increase કરવું \\
\textbf{ઉદાહરણ} & Correct answer માટે treat આપવું & Good performance માટે extra work દૂર કરવું \\
\hline
\end{tabulary}
\end{center}

\begin{center}
\begin{tikzpicture}[node distance=1.5cm, auto]
    \node [gtu state] (Good) {Good Behavior};
    
    \node [gtu block, left=of Good, fill=green!10] (Pos) {Positive};
    \node [gtu state, below=0.5cm of Pos] {Add Reward};
    
    \node [gtu block, right=of Good, fill=red!10] (Neg) {Negative};
    \node [gtu state, below=0.5cm of Neg] {Remove Penalty};
    
    \node [gtu block, below=2cm of Good] (Result) {Behavior Increases};
    
    \path [gtu arrow] (Pos) -- (Good);
    \path [gtu arrow] (Neg) -- (Good);
    \path [gtu arrow] (Good) -- (Result);
\end{tikzpicture}
\captionof{figure}{Reinforcement પ્રકારો}
\end{center}

\textbf{મુખ્ય મુદ્દાઓ:}
\begin{itemize}
    \item \textbf{બંને behavior increase કરે છે} પરંતુ વિવિધ mechanisms દ્વારા.
    \item \textbf{Positive કંઈક pleasant add કરે છે}.
    \item \textbf{Negative કંઈક unpleasant remove કરે છે}.
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{PN - Positive adds Nice things, Negative removes Nasty things}
\end{mnemonicbox}

\questionmarks{3(c)}{7}{ટર્મ-ફ્રીક્વન્સી-ઇનવર્સ ડોક્યુમેન્ટ ફ્રીક્વન્સી (TF-IDF) word embedding technique વિશે બધું સમજાવો.}
\begin{solutionbox}
\textbf{TF-IDF} એ numerical statistic છે જે documents ના collection માં કોઈ document માટે word કેટલું important છે તે reflect કરે છે.

\textbf{ફોર્મ્યુલા:}
\[ \text{TF-IDF} = \text{TF}(t,d) \times \text{IDF}(t) \]
જ્યાં:
\begin{itemize}
    \item \(\text{TF}(t,d) = \frac{\text{Document } d \text{ માં term } t \text{ કેટલી વાર આવે છે}}{\text{Document } d \text{ માં total terms}}\)
    \item \(\text{IDF}(t) = \log\left(\frac{\text{Total documents}}{\text{Term } t \text{ ધરાવતા documents}}\right)\)
\end{itemize}

\begin{center}
\captionof{table}{TF-IDF ઘટકો}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{ઘટક} & \textbf{ફોર્મ્યુલા} & \textbf{હેતુ} \\
\hline
\textbf{Term Frequency (TF)} & \(tf(t,d) = count(t,d) / |d|\) & Document માં word frequency measure કરે છે \\
\textbf{Inverse Document Frequency (IDF)} & \(idf(t) = \log(N / df(t))\) & Corpus માં word importance measure કરે છે \\
\textbf{TF-IDF Score} & \(tf\text{-}idf(t,d) = tf(t,d) \times idf(t)\) & Final word importance score \\
\hline
\end{tabulary}
\end{center}

\textbf{ઉદાહરણ Calculation:}
\begin{itemize}
    \item Document: "cat sat on mat"
    \item Term: "cat"
    \item TF = 1/4 = 0.25
    \item જો "cat" 10 માંથી 2 documents માં આવે છે: IDF = \(\log(10/2) = 0.699\)
    \item TF-IDF = \(0.25 \times 0.699 = 0.175\)
\end{itemize}

\textbf{Applications:}
\begin{itemize}
    \item \textbf{Information Retrieval}: Search engines.
    \item \textbf{Text Mining}: Document similarity.
    \item \textbf{Feature Extraction}: ML preprocessing.
\end{itemize}

\textbf{ફાયદા:}
\begin{itemize}
    \item \textbf{Common words ને low scores મળે છે} (the, and, is).
    \item \textbf{Rare પરંતુ important words ને high scores મળે છે}.
    \item \textbf{સરળ અને અસરકારક} text analysis માટે.
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{TF-IDF - Term Frequency x Inverse Document Frequency}
\end{mnemonicbox}

\questionmarks{3(a) OR}{3}{ફઝી લોજિક સિસ્ટમ્સ વ્યાખ્યાયિત કરો. તેના મુખ્ય ઘટકોની ચર્ચા કરો.}
\begin{solutionbox}
\textbf{Fuzzy Logic Systems} uncertainty અને partial truth handle કરે છે, completely true અને completely false વચ્ચે values allow કરે છે.

\begin{center}
\captionof{table}{Fuzzy Logic ઘટકો}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{ઘટક} & \textbf{કાર્ય} & \textbf{ઉદાહરણ} \\
\hline
\textbf{Fuzzifier} & Crisp inputs ને fuzzy sets માં convert કરે છે & Temperature 75°F $\to$ "Warm" (0.7) \\
\textbf{Rule Base} & If-then fuzzy rules ધરાવે છે & IF temp is warm THEN fan is medium \\
\textbf{Inference Engine} & Inputs પર fuzzy rules apply કરે છે & Multiple rules combine કરે છે \\
\textbf{Defuzzifier} & Fuzzy output ને crisp value માં convert કરે છે & "Medium speed" $\to$ 60\% fan speed \\
\hline
\end{tabulary}
\end{center}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{FRID - Fuzzifier, Rules, Inference, Defuzzifier}
\end{mnemonicbox}

\questionmarks{3(b) OR}{4}{મજબૂતીકરણ શિક્ષણના ઘટકો સમજાવો: નીતિ, પુરસ્કાર સંકેત, મૂલ્ય કાર્ય, મોડેલ}
\begin{solutionbox}
\begin{center}
\captionof{table}{Reinforcement Learning ઘટકો}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{ઘટક} & \textbf{વ્યાખ્યા} & \textbf{હેતુ} \\
\hline
\textbf{Policy (નીતિ)} & Actions select કરવાની strategy & Agent ના behavior ને define કરે છે \\
\textbf{Reward Signal (પુરસ્કાર સંકેત)} & Environment તરફથી feedback & Good/bad actions indicate કરે છે \\
\textbf{Value Function (મૂલ્ય કાર્ય)} & Expected future rewards & Long-term benefit estimate કરે છે \\
\textbf{Model (મોડેલ)} & Environment નું agent representation & Next state અને reward predict કરે છે \\
\hline
\end{tabulary}
\end{center}

\textbf{વિગતવાર સમજૂતી:}
\begin{itemize}
    \item \textbf{Policy ($\pi$)}: Deterministic અથવા stochastic હોઈ શકે.
    \item \textbf{Reward Signal (R)}: Immediate feedback (positive/negative).
    \item \textbf{Value Function (V)}: Expected long-term return.
    \item \textbf{Model}: Environment dynamics predict કરે છે.
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{PRVM - Policy chooses, Reward judges, Value estimates, Model predicts}
\end{mnemonicbox}

\questionmarks{3(c) OR}{7}{તફાવત કરો: આવૃત્તિ-આધારિત v/s આગાહી-આધારિત word embedding તકનીકો.}
\begin{solutionbox}
\begin{center}
\captionof{table}{Frequency-based vs Prediction-based Word Embeddings}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{પાસું} & \textbf{Frequency-based} & \textbf{Prediction-based} \\
\hline
\textbf{Approach} & Count-based statistics & Neural network prediction \\
\textbf{ઉદાહરણો} & TF-IDF, Co-occurrence Matrix & Word2Vec, GloVe \\
\textbf{Computation} & Matrix factorization & Gradient descent \\
\textbf{Context} & Global statistics & Local context windows \\
\textbf{Scalability} & Matrix size દ્વારા limited & Vocabulary સાથે scales \\
\textbf{Quality} & Basic semantic relationships & Rich semantic relationships \\
\hline
\end{tabulary}
\end{center}

\textbf{Frequency-based Methods:}
\begin{itemize}
    \item \textbf{TF-IDF}: Term frequency $\times$ Inverse document frequency.
    \item \textbf{Co-occurrence Matrix}: Word pair frequency counts.
\end{itemize}

\textbf{Prediction-based Methods:}
\begin{itemize}
    \item \textbf{Word2Vec}: Skip-gram અને CBOW models.
    \item \textbf{GloVe}: Global Vectors for Word Representation.
\end{itemize}

\textbf{ફાયદા:}
\begin{itemize}
    \item \textbf{Frequency-based}: સરળ, ઝડપી (small data), basic similarity માટે સારું.
    \item \textbf{Prediction-based}: Dense vectors, બહેતર semantics, scalable.
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{FP - Frequency counts, Prediction learns}
\end{mnemonicbox}

\questionmarks{4(a)}{3}{પ્રતિક્રિયાશીલ મશીનની મુખ્ય લાક્ષણિકતાઓની યાદી બનાવો.}
\begin{solutionbox}
\begin{center}
\captionof{table}{Reactive Machine મુખ્ય લાક્ષણિકતાઓ}
\begin{tabulary}{\linewidth}{L L}
\hline
\textbf{લાક્ષણિકતા} & \textbf{વર્ણન} \\
\hline
\textbf{Stateless} & Past interactions ની કોઈ memory નથી \\
\textbf{Reactive} & ફક્ત current inputs ને respond કરે છે \\
\textbf{Deterministic} & Same inputs માટે consistent outputs \\
\textbf{Specialized} & Specific tasks માટે designed \\
\textbf{Real-time} & Stimuli ને immediate response \\
\hline
\end{tabulary}
\end{center}

\textbf{ઉદાહરણો:}
\begin{itemize}
    \item \textbf{Deep Blue}: Chess-playing computer.
    \item \textbf{Google AlphaGo}: Go-playing system (early version).
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{SRDSR - Stateless, Reactive, Deterministic, Specialized, Real-time}
\end{mnemonicbox}

\questionmarks{4(b)}{4}{વિવિધ પૂર્વ-પ્રોસેસિંગ તકનીકોની સૂચિ બનાવો. તેમાંથી કોઈપણ એકને python code વડે સમજાવો.}
\begin{solutionbox}
\begin{center}
\captionof{table}{Text Pre-processing તકનીકો}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{તકનીક} & \textbf{હેતુ} & \textbf{ઉદાહરણ} \\
\hline
\textbf{Tokenization} & Text ને words માં split કરવું & "Hello world" $\to$ ["Hello", "world"] \\
\textbf{Stop Word Removal} & Common words remove કરવા & "the", "and", "is" remove કરવા \\
\textbf{Stemming} & Words ને root form માં reduce કરવા & "running" $\to$ "run" \\
\textbf{Lemmatization} & Dictionary form માં convert કરવા & "better" $\to$ "good" \\
\hline
\end{tabulary}
\end{center}

\textbf{Stemming સમજૂતી:}
Stemming suffixes remove કરીને words ને root form માં reduce કરે છે.

\textbf{Stemming માટે Python Code:}
\begin{lstlisting}[language=python]
import nltk
from nltk.stem import PorterStemmer

# Stemmer initialize કરો
stemmer = PorterStemmer()

# Example words
words = ["running", "flies", "dogs", "churches", "studying"]

# Stemming apply કરો
stemmed_words = [stemmer.stem(word) for word in words]
print(stemmed_words)
# Output: ['run', 'fli', 'dog', 'church', 'studi']
\end{lstlisting}

\textbf{Stemming ના ફાયદા:}
\begin{itemize}
    \item ML models માટે vocabulary size reduce કરે છે.
    \item Related words ને together group કરે છે.
    \item Text analysis efficiency improve કરે છે.
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{TSSL - Tokenize, Stop-words, Stem, Lemmatize}
\end{mnemonicbox}

\questionmarks{4(c)}{7}{Word2vec તકનીકને વિગતવાર પ્રકાશિત કરો.}
\begin{solutionbox}
\textbf{Word2Vec} એ neural network-based તકનીક છે જે context predict કરીને words ના dense vector representations શીખે છે.

\begin{center}
\captionof{table}{Word2Vec Architectures}
\begin{tabulary}{\linewidth}{L L L L}
\hline
\textbf{Architecture} & \textbf{Approach} & \textbf{Input} & \textbf{Output} \\
\hline
\textbf{Skip-gram} & Center word થી context predict કરે છે & Center word & Context words \\
\textbf{CBOW} & Context થી center word predict કરે છે & Context words & Center word \\
\hline
\end{tabulary}
\end{center}

\begin{center}
\begin{tikzpicture}[node distance=1.5cm, auto]
    \node [gtu state] (Input) {Input: Center Word};
    \node [gtu block, right=of Input] (Hidden) {Hidden Layer};
    \node [gtu state, right=of Hidden] (Output) {Output: Context Words};
    \node [gtu block, below=1cm of Output] (Softmax) {Softmax Layer};
    \node [gtu state, left=of Softmax] (Prob) {Prob. Dist.};
    
    \path [gtu arrow] (Input) -- (Hidden);
    \path [gtu arrow] (Hidden) -- (Output);
    \path [gtu arrow] (Output) -- (Softmax);
    \path [gtu arrow] (Softmax) -- (Prob);
\end{tikzpicture}
\captionof{figure}{Skip-gram Model}
\end{center}

\textbf{Training Process:}
\begin{itemize}
    \item \textbf{Sliding Window}: Text પર window move કરો.
    \item \textbf{Word Pairs}: (center, context) pairs બનાવો.
    \item \textbf{Neural Network}: Context predict કરવા માટે train કરો.
    \item \textbf{Weight Matrix}: Word vectors extract કરો.
\end{itemize}

\textbf{Mathematical Concept:}
\[ \text{Objective} = \max \sum \log P(\text{context}|\text{center}) \]
\[ P(\text{context}|\text{center}) = \frac{\exp(v_{context} \cdot v_{center})}{\sum \exp(v_w \cdot v_{center})} \]

\textbf{Applications:}
\begin{itemize}
    \item \textbf{Similarity}: Similar words શોધવા.
    \item \textbf{Analogies}: King - Man + Woman = Queen.
    \item \textbf{Feature Engineering}: ML input features.
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{W2V - Words to Vectors via neural networks}
\end{mnemonicbox}

\questionmarks{4(a) OR}{3}{નેચરલ લેંગ્વેજ પ્રોસેસિંગની કોઈપણ ચાર applications ની યાદી બનાવો. સ્પામ શોધને વિગતવાર સમજાવો.}
\begin{solutionbox}
\begin{center}
\captionof{table}{NLP Applications}
\begin{tabulary}{\linewidth}{L L}
\hline
\textbf{Application} & \textbf{વર્ણન} \\
\hline
\textbf{Spam Detection} & Unwanted emails identify કરવા \\
\textbf{Sentiment Analysis} & Emotional tone determine કરવા \\
\textbf{Machine Translation} & Languages વચ્ચે translate કરવા \\
\textbf{Chatbots} & Automated conversation systems \\
\hline
\end{tabulary}
\end{center}

\textbf{Spam Detection વિગતો:}
\begin{center}
\begin{tikzpicture}[node distance=1.5cm, auto]
    \node [gtu state] (Email) {Email Text};
    \node [gtu block, below=0.7cm of Email] (Features) {Feature\\Extraction};
    \node [gtu block, below=0.7cm of Features] (Classify) {ML Classifier\\(Naive Bayes)};
    \node [gtu decision, below=0.7cm of Classify] (Decision) {Spam?};
    \node [gtu state, left=of Decision, fill=red!10] (Yes) {Junk Folder};
    \node [gtu state, right=of Decision, fill=green!10] (No) {Inbox};
    
    \path [gtu arrow] (Email) -- (Features);
    \path [gtu arrow] (Features) -- (Classify);
    \path [gtu arrow] (Classify) -- (Decision);
    \path [gtu arrow] (Decision) -- node[above] {Yes} (Yes);
    \path [gtu arrow] (Decision) -- node[above] {No} (No);
\end{tikzpicture}
\captionof{figure}{Spam Detection}
\end{center}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{SMTP - Spam, Machine Translation, Sentiment, Phishing detection}
\end{mnemonicbox}

\questionmarks{4(b) OR}{4}{પ્રવચન સંકલન અને વ્યવહારિક વિશ્લેષણ વિશે સમજાવો.}
\begin{solutionbox}
\begin{center}
\captionof{table}{Discourse Integration vs Pragmatic Analysis}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{પાસું} & \textbf{Discourse Integration} & \textbf{Pragmatic Analysis} \\
\hline
\textbf{ધ્યાન} & Text coherence અને structure & Context અને intention \\
\textbf{વ્યાપ્તિ} & Multiple sentences/paragraphs & Speaker નો intended meaning \\
\textbf{ઘટકો} & Anaphora, cataphora, connectives & Implicature, speech acts \\
\textbf{લક્ષ્ય} & Text flow understand કરવું & Real meaning understand કરવું \\
\hline
\end{tabulary}
\end{center}

\textbf{ઉદાહરણો:}
\begin{itemize}
    \item \textbf{Discourse}: "Mary owns a car. The vehicle is red." ("vehicle" refers to "car").
    \item \textbf{Pragmatic}: "Can you pass the salt?" (Request, ability વિશે question નથી).
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{DP - Discourse connects, Pragmatics interprets context}
\end{mnemonicbox}

\questionmarks{4(c) OR}{7}{બેગ ઓફ વર્ડ્સ word embedding technique વિશે વિગતવાર ચર્ચા કરો.}
\begin{solutionbox}
\textbf{Bag of Words (BoW)} એ simple text representation method છે જે documents ને unordered collections of words તરીકે treat કરે છે.

\begin{center}
\captionof{table}{BoW Process}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{Step} & \textbf{Description} & \textbf{Example} \\
\hline
\textbf{Vocabulary} & Collect unique words & ["cat", "sat", "mat"] \\
\textbf{Vector} & Count occurrences & [1, 1, 1] for "cat sat mat" \\
\textbf{Representation} & Document $\to$ Vector & Vectors form Matrix \\
\hline
\end{tabulary}
\end{center}

\textbf{ફાયદા:}
\begin{itemize}
    \item \textbf{સરળતા}: Understand અને implement કરવા માટે સરળ.
    \item \textbf{અસરકારકતા}: Spam detection જેવા tasks માટે સારું.
\end{itemize}

\textbf{ગેરફાયદા:}
\begin{itemize}
    \item \textbf{કોઈ Word Order નથી}.
    \item \textbf{Sparse Vectors}.
    \item \textbf{કોઈ Semantics નથી}.
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{BOW - Bag Of Words counts occurrences}
\end{mnemonicbox}

\questionmarks{5(a)}{3}{ન્યુરલ નેટવર્કમાં સક્રિયકરણ કાર્યોની ભૂમિકા શું છે?}
\begin{solutionbox}
\begin{center}
\captionof{table}{Activation Function ભૂમિકાઓ}
\begin{tabulary}{\linewidth}{L L}
\hline
\textbf{ભૂમિકા} & \textbf{વર્ણન} \\
\hline
\textbf{બિન-રેખીયતા} & Complex patterns શીખવાને enable કરે છે \\
\textbf{આઉટપુટ નિયંત્રણ} & Neuron firing threshold determine કરે છે \\
\textbf{Gradient Flow} & Backpropagation efficiency ને affect કરે છે \\
\textbf{રેન્જ મર્યાદા} & Output values ને bounds કરે છે \\
\hline
\end{tabulary}
\end{center}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{NOGL - Non-linearity, Output control, Gradient flow, Limiting range}
\end{mnemonicbox}

\questionmarks{5(b)}{4}{ન્યુરલ નેટવર્કના આર્કિટેક્ચરનું વિગતવાર વર્ણન કરો.}
\begin{solutionbox}
\begin{center}
\captionof{table}{Neural Network Architecture ઘટકો}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{ઘટક} & \textbf{કાર્ય} & \textbf{ઉદાહરણ} \\
\hline
\textbf{Input Layer} & Input data receive કરે છે & Features/pixels \\
\textbf{Hidden Layers} & Information process કરે છે & Pattern recognition \\
\textbf{Output Layer} & Final result produce કરે છે & Classification/prediction \\
\textbf{Connections} & Layers વચ્ચે neurons ને link કરે છે & Weighted edges \\
\hline
\end{tabulary}
\end{center}

\begin{center}
\begin{tikzpicture}[node distance=1.5cm, auto]
    % Input Layer
    \foreach \i in {1,2,3}
        \node[circle, draw, minimum size=0.8cm] (I\i) at (0,-\i) {$I_\i$};
    
    % Hidden Layer
    \foreach \h in {1,2,3,4}
        \node[circle, draw, minimum size=0.8cm] (H\h) at (3, -0.5-\h) {$H_\h$};
        
    % Output Layer
    \foreach \o in {1,2}
        \node[circle, draw, minimum size=0.8cm] (O\o) at (6,-1.5-\o) {$O_\o$};
        
    % Connections
    \foreach \i in {1,2,3}
        \foreach \h in {1,2,3,4}
            \draw [->] (I\i) -- (H\h);
            
    \foreach \h in {1,2,3,4}
        \foreach \o in {1,2}
            \draw [->] (H\h) -- (O\o);
            
    \node[above] at (0,0) {Input Layer};
    \node[above] at (3,-0.5) {Hidden Layer};
    \node[above] at (6,-1.5) {Output Layer};
\end{tikzpicture}
\captionof{figure}{Neural Network Architecture}
\end{center}

\textbf{Information Flow:}
\begin{itemize}
    \item \textbf{Forward Pass}: Input $\to$ Hidden $\to$ Output.
    \item \textbf{Weighted Sum}: $\Sigma(w_i \times x_i + bias)$.
    \item \textbf{Activation}: Activation function apply કરો.
\end{itemize}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{IHOC - Input, Hidden, Output, Connections}
\end{mnemonicbox}

\questionmarks{5(c)}{7}{નેચરલ લેંગ્વેજ પ્રોસેસિંગમાં અસ્પષ્ટતાના પ્રકારોની યાદી બનાવો અને સમજાવો.}
\begin{solutionbox}
\textbf{Ambiguity} NLP માં ત્યારે થાય છે જ્યારે text ના multiple possible interpretations હોય છે.

\begin{center}
\captionof{table}{NLP Ambiguities ના પ્રકારો}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{પ્રકાર} & \textbf{વ્યાખ્યા} & \textbf{ઉકેલ} \\
\hline
\textbf{Lexical} & Word ના multiple meanings & Context analysis \\
\textbf{Syntactic} & Multiple parse structures & Grammar rules \\
\textbf{Semantic} & Multiple sentence meanings & Semantic analysis \\
\textbf{Pragmatic} & Context-dependent meaning & Intent recognition \\
\textbf{Referential} & Unclear pronoun reference & Anaphora resolution \\
\hline
\end{tabulary}
\end{center}

\begin{center}
\begin{tikzpicture}[node distance=1.5cm, auto]
    \node [gtu state] (Ambig) {Ambiguous Text};
    \node [gtu block, right=of Ambig] (Context) {Context Analysis};
    \node [gtu block, right=of Context] (Disambig) {Disambiguation};
    \node [gtu state, right=of Disambig] (Clear) {Clear Interpretation};
    
    \node [gtu block, below=0.5cm of Context] (Knowledge) {Knowledge Bases};
    
    \path [gtu arrow] (Ambig) -- (Context);
    \path [gtu arrow] (Context) -- (Disambig);
    \path [gtu arrow] (Disambig) -- (Clear);
    \path [gtu arrow] (Knowledge) -| (Disambig);
\end{tikzpicture}
\captionof{figure}{Resolution Process}
\end{center}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{LSSPR - Lexical, Syntactic, Semantic, Pragmatic, Referential}
\end{mnemonicbox}

\questionmarks{5(a) OR}{3}{ન્યુરલ નેટવર્કમાં ઉપયોગમાં લેવાતા કેટલાક લોકપ્રિય સક્રિયકરણ કાર્યોના નામોની સૂચિ બનાવો.}
\begin{solutionbox}
\begin{center}
\captionof{table}{લોકપ્રિય Activation Functions}
\begin{tabulary}{\linewidth}{L L L L}
\hline
\textbf{Function} & \textbf{ફોર્મ્યુલા} & \textbf{Range} & \textbf{વપરાશ} \\
\hline
\textbf{ReLU} & \(f(x) = \max(0, x)\) & \([0, \infty)\) & Hidden layers \\
\textbf{Sigmoid} & \(f(x) = 1/(1 + e^{-x})\) & \((0, 1)\) & Binary classification \\
\textbf{Tanh} & \(f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}\) & \((-1, 1)\) & Hidden layers \\
\textbf{Softmax} & \(f(x_i) = e^{x_i} / \Sigma e^{x_j}\) & \((0, 1)\) & Multi-class output \\
\hline
\end{tabulary}
\end{center}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{RSTSL - ReLU, Sigmoid, Tanh, Softmax, Leaky ReLU}
\end{mnemonicbox}

\questionmarks{5(b) OR}{4}{કૃત્રિમ ન્યુરલ નેટવર્કમાં શીખવાની પ્રક્રિયા સમજાવો.}
\begin{solutionbox}
\textbf{Learning Process} neural networks માં iterative training દ્વારા error minimize કરવા માટે weights અને biases ને adjust કરવાનો સમાવેશ કરે છે.

\begin{center}
\captionof{table}{Learning Process Steps}
\begin{tabulary}{\linewidth}{L L L}
\hline
\textbf{સ્ટેપ} & \textbf{પ્રક્રિયા} & \textbf{વર્ણન} \\
\hline
\textbf{Initialize} & Random weights & Small random values સાથે start કરો \\
\textbf{Forward Pass} & Output calculate કરો & Network દ્વારા input propagate કરો \\
\textbf{Calculate Error} & Target સાથે compare કરો & Loss function વાપરો \\
\textbf{Backward Pass} & Gradients calculate કરો & Backpropagation વાપરો \\
\textbf{Update Weights} & Parameters adjust કરો & Gradient descent apply કરો \\
\hline
\end{tabulary}
\end{center}

\begin{center}
\begin{tikzpicture}[node distance=1.5cm, auto]
    \node [gtu block] (Init) {Weights Initialize કરો};
    \node [gtu block, right=of Init] (Forward) {Forward Pass};
    \node [gtu block, right=of Forward] (Loss) {Loss Calculate કરો};
    \node [gtu block, below=1cm of Loss] (Back) {Backward Pass};
    \node [gtu block, left=of Back] (Update) {Weights Update કરો};
    \node [gtu decision, left=of Update] (Conv) {Converged?};
    \node [gtu state, below=1cm of Conv] (Done) {Training Complete};
    
    \path [gtu arrow] (Init) -- (Forward);
    \path [gtu arrow] (Forward) -- (Loss);
    \path [gtu arrow] (Loss) -- (Back);
    \path [gtu arrow] (Back) -- (Update);
    \path [gtu arrow] (Update) -- (Conv);
    \path [gtu arrow] (Conv) -- node[left] {હા} (Done);
    \path [gtu arrow] (Conv.north) |- node[left] {ના} (Forward.south);
\end{tikzpicture}
\captionof{figure}{Learning Flow}
\end{center}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{IFCBU - Initialize, Forward, Calculate, Backward, Update}
\end{mnemonicbox}

\questionmarks{5(c) OR}{7}{નેચરલ લેંગ્વેજ પ્રોસેસિંગના વિવિધ ફાયદા અને ગેરફાયદાની યાદી બનાવો.}
\begin{solutionbox}
\begin{center}
\captionof{table}{NLP ફાયદા અને ગેરફાયદા}
\begin{tabulary}{\linewidth}{L L}
\hline
\textbf{ફાયદા} & \textbf{ગેરફાયદા} \\
\hline
\textbf{સ્વચાલિત ટેક્સ્ટ વિશ્લેષણ} & \textbf{અસ્પષ્ટતા હેન્ડલિંગ} \\
\textbf{ભાષા અનુવાદ} & \textbf{સંદર્ભ સમજ} \\
\textbf{માનવ-કમ્પ્યુટર ક્રિયાપ્રતિક્રિયા} & \textbf{સાંસ્કૃતિક સૂત્રધારતા} \\
\textbf{માહિતી નિષ્કર્ષણ} & \textbf{કોમ્પ્યુટેશનલ જટિલતા} \\
\textbf{ભાવના વિશ્લેષણ} & \textbf{ડેટા આવશ્યકતાઓ} \\
\hline
\end{tabulary}
\end{center}

\begin{center}
\begin{tikzpicture}[node distance=1.5cm]
    \node [gtu block] (NLP) {NLP};
    
    \node [gtu state, below left=1.5cm and 0.5cm of NLP, align=center] (Apps) {\textbf{Applications}\\Translation\\Sentiment\\Extraction};
    \node [gtu state, below right=1.5cm and 0.5cm of NLP, align=center] (Challenges) {\textbf{Challenges}\\Ambiguity\\Context\\Nuances};
    
    \path [gtu arrow] (NLP) -- (Apps);
    \path [gtu arrow] (NLP) -- (Challenges);
\end{tikzpicture}
\captionof{figure}{Applications vs Challenges}
\end{center}
\end{solutionbox}

\begin{mnemonicbox}
\mnemonic{ALICE vs ACHDR - Automated, Language, Interaction, Content, Extraction vs Ambiguity, Context, Human-nuances}
\end{mnemonicbox}

\end{document}
