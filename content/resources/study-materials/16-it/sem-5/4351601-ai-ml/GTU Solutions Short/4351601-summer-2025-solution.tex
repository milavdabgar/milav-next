\documentclass{article}
\input{../../../../../../../latex-templates/gtu-solutions-short/preamble.tex}
\input{../../../../../../../latex-templates/gtu-solutions-short/english-boxes.tex}
\input{../../../../../../../latex-templates/gtu-solutions-short/commands.tex}

\title{Foundation of AI and ML (4351601) - Summer 2025 Solution}
\date{May 12, 2025}

\begin{document}
\maketitle

\questionmarks{1}{a}{3}
\textbf{What is Word Embedding technique? List out different word embedding techniques.}

\textbf{Answer:}

\textbf{Word Embedding} is a technique that converts words into numerical vectors while preserving semantic relationships between words. It represents words as dense vectors in a high-dimensional space where similar words are closer together.

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{Technique} & \textbf{Description} & \textbf{Key Feature} \\
\hline
\textbf{TF-IDF} & Term Frequency-Inverse Document Frequency & Statistical measure \\
\hline
\textbf{Bag of Words (BoW)} & Frequency-based representation & Simple counting method \\
\hline
\textbf{Word2Vec} & Neural network-based embedding & Captures semantic relationships \\
\hline
\textbf{GloVe} & Global Vectors for word representation & Combines global and local statistics \\
\hline
\end{tabulary}

\begin{itemize}
    \item \textbf{TF-IDF}: Measures word importance in documents
    \item \textbf{BoW}: Creates vocabulary-based vectors
    \item \textbf{Word2Vec}: Uses CBOW and Skip-gram models
    \item \textbf{GloVe}: Pre-trained embeddings with global context
\end{itemize}

\begin{mnemonicbox}
"TB-WG" (TF-IDF, BoW, Word2Vec, GloVe)
\end{mnemonicbox}

\questionmarks{1}{b}{4}
\textbf{Categorize the different types of Artificial Intelligence and demonstrate it with a diagram.}

\textbf{Answer:}

AI can be categorized based on \textbf{capabilities} and \textbf{functionality}.

\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=1.5cm,
    level 1/.style={sibling distance=7cm},
    level 2/.style={sibling distance=2.5cm}, 
    edge from parent/.style={draw, -latex}]
    
    \node [gtu block] {Artificial Intelligence}
        child {node [gtu block] {Based on Capabilities}
            child {node [gtu block] {Narrow AI/\\Weak AI}}
            child {node [gtu block] {General AI/\\Strong AI}}
            child {node [gtu block] {Super AI}}
        }
        child {node [gtu block] {Based on Functionality}
            child {node [gtu block] {Reactive\\Machines}}
            child {node [gtu block] {Limited\\Memory}}
            child {node [gtu block] {Theory of\\Mind}}
            child {node [gtu block] {Self-\\Awareness}}
        };
\end{tikzpicture}
\caption{Classification of AI}
\end{figure}

\begin{tabulary}{\linewidth}{|L|L|L|L|}
\hline
\textbf{Category} & \textbf{Type} & \textbf{Description} & \textbf{Example} \\
\hline
\textbf{Capabilities} & Narrow AI & Task-specific intelligence & Siri, Chess programs \\
\cline{2-4}
 & General AI & Human-level intelligence & Not yet achieved \\
\cline{2-4}
 & Super AI & Beyond human intelligence & Theoretical concept \\
\hline
\textbf{Functionality} & Reactive & No memory, responds to stimuli & Deep Blue \\
\cline{2-4}
 & Limited Memory & Uses past data & Self-driving cars \\
\hline
\end{tabulary}

\begin{mnemonicbox}
"NGS-RLT" (Narrow-General-Super, Reactive-Limited-Theory)
\end{mnemonicbox}

\questionmarks{1}{c}{7}
\textbf{Explain NLU and NLG by giving difference.}

\textbf{Answer:}

\textbf{Natural Language Understanding (NLU)} and \textbf{Natural Language Generation (NLG)} are two key components of Natural Language Processing.

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{Aspect} & \textbf{NLU} & \textbf{NLG} \\
\hline
\textbf{Purpose} & Understands human language & Generates human language \\
\hline
\textbf{Direction} & Input processing & Output generation \\
\hline
\textbf{Function} & Interprets meaning & Creates text \\
\hline
\textbf{Process} & Analysis and comprehension & Synthesis and creation \\
\hline
\textbf{Examples} & Intent recognition, sentiment analysis & Chatbot responses, report generation \\
\hline
\textbf{Challenges} & Ambiguity resolution & Natural text generation \\
\hline
\end{tabulary}

\textbf{Detailed Explanation:}

\begin{itemize}
    \item \textbf{NLU (Natural Language Understanding)}:
    \begin{itemize}
        \item Converts unstructured text into structured data
        \item Performs semantic analysis and intent extraction
        \item Handles ambiguity and context understanding
    \end{itemize}
    \item \textbf{NLG (Natural Language Generation)}:
    \begin{itemize}
        \item Converts structured data into natural language
        \item Creates coherent and contextually appropriate text
        \item Ensures grammatical correctness and fluency
    \end{itemize}
\end{itemize}

\begin{mnemonicbox}
"UI-OG" (Understanding Input, Output Generation)
\end{mnemonicbox}

\questionmarks{1}{c}{7}
\textbf{List out various Industries where Artificial Intelligence is used and explain any two.}

\textbf{Answer:}

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{Industry} & \textbf{AI Applications} & \textbf{Benefits} \\
\hline
\textbf{Healthcare} & Diagnosis, drug discovery & Improved accuracy \\
\hline
\textbf{Finance} & Fraud detection, trading & Risk management \\
\hline
\textbf{Manufacturing} & Quality control, predictive maintenance & Efficiency \\
\hline
\textbf{Transportation} & Autonomous vehicles, route optimization & Safety \\
\hline
\textbf{Retail} & Recommendation systems, inventory & Personalization \\
\hline
\textbf{Education} & Personalized learning, assessment & Adaptive teaching \\
\hline
\end{tabulary}

\textbf{Detailed Explanation of Two Industries:}

\textbf{1. Healthcare Industry:}
\begin{itemize}
    \item \textbf{Medical Diagnosis}: AI analyzes medical images and patient data
    \item \textbf{Drug Discovery}: Accelerates identification of potential medicines
    \item \textbf{Personalized Treatment}: Tailors therapy based on patient genetics
    \item \textbf{Benefits}: Faster diagnosis, reduced errors, improved outcomes
\end{itemize}

\textbf{2. Finance Industry:}
\begin{itemize}
    \item \textbf{Fraud Detection}: Identifies suspicious transactions in real-time
    \item \textbf{Algorithmic Trading}: Automated trading based on market patterns
    \item \textbf{Credit Scoring}: Assesses loan default risk accurately
    \item \textbf{Benefits}: Enhanced security, faster processing, better risk management
\end{itemize}

\begin{mnemonicbox}
"HF-MR-TE" (Healthcare-Finance, Manufacturing-Retail-Transportation-Education)
\end{mnemonicbox}

\questionmarks{2}{a}{3}
\textbf{Define the term Machine Learning. Draw the classification diagram of Machine Learning.}

\textbf{Answer:}

\textbf{Machine Learning} is a subset of AI that enables computers to learn and improve from experience without being explicitly programmed. It uses algorithms to analyze data, identify patterns, and make predictions.

\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=1.5cm,
    level 1/.style={sibling distance=4.5cm},
    level 2/.style={sibling distance=2.2cm}, 
    edge from parent/.style={draw, -latex}]
    
    \node [gtu block] {Machine Learning}
        child {node [gtu block] {Supervised\\Learning}
            child {node [gtu block] {Classification}}
            child {node [gtu block] {Regression}}
        }
        child {node [gtu block] {Unsupervised\\Learning}
            child {node [gtu block] {Clustering}}
            child {node [gtu block] {Association}}
        }
        child {node [gtu block] {Reinforcement\\Learning}
            child {node [gtu block] {Model-based}}
            child {node [gtu block] {Model-free}}
        };
\end{tikzpicture}
\caption{Classification of Machine Learning}
\end{figure}

\begin{itemize}
    \item \textbf{Supervised}: Uses labeled training data
    \item \textbf{Unsupervised}: Finds patterns in unlabeled data
    \item \textbf{Reinforcement}: Learns through rewards and penalties
\end{itemize}

\begin{mnemonicbox}
"SUR" (Supervised-Unsupervised-Reinforcement)
\end{mnemonicbox}

\questionmarks{2}{b}{4}
\textbf{Differentiate Positive reinforcement and Negative reinforcement.}

\textbf{Answer:}

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{Aspect} & \textbf{Positive Reinforcement} & \textbf{Negative Reinforcement} \\
\hline
\textbf{Definition} & Adding reward for good behavior & Removing unpleasant stimulus \\
\hline
\textbf{Action} & Gives something pleasant & Takes away something unpleasant \\
\hline
\textbf{Purpose} & Increase desired behavior & Increase desired behavior \\
\hline
\textbf{Example} & Bonus for good performance & Removing alarm after waking up \\
\hline
\textbf{Effect} & Motivation through rewards & Motivation through relief \\
\hline
\textbf{Agent Response} & Seeks to repeat action & Avoids negative consequences \\
\hline
\end{tabulary}

\textbf{Key Points:}
\begin{itemize}
    \item \textbf{Positive Reinforcement}: Strengthens behavior by adding positive stimulus
    \item \textbf{Negative Reinforcement}: Strengthens behavior by removing negative stimulus
    \item \textbf{Both types}: Aim to increase the likelihood of desired behavior
    \item \textbf{Difference}: Method of encouragement (add vs remove)
\end{itemize}

\begin{mnemonicbox}
"AR-RN" (Add Reward, Remove Negative)
\end{mnemonicbox}

\questionmarks{2}{c}{7}
\textbf{Compare Supervised and Unsupervised learning.}

\textbf{Answer:}

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{Parameter} & \textbf{Supervised Learning} & \textbf{Unsupervised Learning} \\
\hline
\textbf{Data Type} & Labeled data (input-output pairs) & Unlabeled data (only inputs) \\
\hline
\textbf{Learning Goal} & Predict outcomes & Find hidden patterns \\
\hline
\textbf{Feedback} & Has correct answers & No correct answers \\
\hline
\textbf{Algorithms} & SVM, Decision Trees, Neural Networks & K-means, Hierarchical clustering \\
\hline
\textbf{Applications} & Classification, Regression & Clustering, Association rules \\
\hline
\textbf{Accuracy} & Can be measured & Difficult to measure \\
\hline
\textbf{Complexity} & Less complex & More complex \\
\hline
\textbf{Examples} & Email spam detection, Price prediction & Customer segmentation, Market basket analysis \\
\hline
\end{tabulary}

\textbf{Detailed Comparison:}
\begin{itemize}
    \item \textbf{Supervised Learning}:
    \begin{itemize}
        \item Requires training data with known outcomes
        \item Performance can be easily evaluated
        \item Used for prediction tasks
    \end{itemize}
    \item \textbf{Unsupervised Learning}:
    \begin{itemize}
        \item Works with data without predefined labels
        \item Discovers hidden structures in data
        \item Used for exploratory data analysis
    \end{itemize}
\end{itemize}

\begin{mnemonicbox}
"LP-PF" (Labeled Prediction, Pattern Finding)
\end{mnemonicbox}

\questionmarks{2}{a}{3}
\textbf{Define: Classification, Regression, and clustering.}

\textbf{Answer:}

\begin{tabulary}{\linewidth}{|L|L|L|L|}
\hline
\textbf{Task} & \textbf{Definition} & \textbf{Output Type} & \textbf{Example} \\
\hline
\textbf{Classification} & Predicts discrete categories/classes & Categorical & Email: Spam/Not Spam \\
\hline
\textbf{Regression} & Predicts continuous numerical values & Numerical & House price prediction \\
\hline
\textbf{Clustering} & Groups similar data points & Groups/Clusters & Customer segmentation \\
\hline
\end{tabulary}

\textbf{Detailed Definitions:}
\begin{itemize}
    \item \textbf{Classification}: Assigns input data to predefined categories based on learned patterns
    \item \textbf{Regression}: Estimates relationships between variables to predict continuous values
    \item \textbf{Clustering}: Discovers natural groupings in data without prior knowledge of groups
\end{itemize}

\begin{mnemonicbox}
"CRC" (Categories, Real numbers, Clusters)
\end{mnemonicbox}

\questionmarks{2}{b}{4}
\textbf{Compare Artificial Neural Network and Biological Neural Network.}

\textbf{Answer:}

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{Aspect} & \textbf{Artificial Neural Network} & \textbf{Biological Neural Network} \\
\hline
\textbf{Processing} & Digital/Binary & Analog \\
\hline
\textbf{Speed} & Fast processing & Slower processing \\
\hline
\textbf{Learning} & Backpropagation algorithm & Synaptic plasticity \\
\hline
\textbf{Memory} & Separate storage & Distributed in connections \\
\hline
\textbf{Structure} & Layered architecture & Complex 3D structure \\
\hline
\textbf{Fault Tolerance} & Low & High \\
\hline
\textbf{Energy} & High power consumption & Low energy consumption \\
\hline
\textbf{Parallelism} & Limited parallel processing & Massive parallel processing \\
\hline
\end{tabulary}

\textbf{Key Differences:}
\begin{itemize}
    \item \textbf{ANN}: Mathematical model inspired by brain
    \item \textbf{Biological}: Actual brain neural networks
    \item \textbf{Purpose}: ANN for computation, Biological for cognition
    \item \textbf{Adaptability}: Biological networks more flexible
\end{itemize}

\begin{mnemonicbox}
"DSML-CFEP" (Digital-Speed-Memory-Layer vs Complex-Fault-Energy-Parallel)
\end{mnemonicbox}

\questionmarks{2}{c}{7}
\textbf{List out various applications of supervised, unsupervised and reinforcement learning.}

\textbf{Answer:}

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{Learning Type} & \textbf{Applications} & \textbf{Real-world Examples} \\
\hline
\textbf{Supervised} & Email classification, Medical diagnosis, Stock prediction, Credit scoring & Gmail spam filter, X-ray analysis, Trading algorithms \\
\hline
\textbf{Unsupervised} & Customer segmentation, Anomaly detection, Data compression & Market research, Fraud detection, Image compression \\
\hline
\textbf{Reinforcement} & Game playing, Robotics, Autonomous vehicles, Resource allocation & AlphaGo, Robot navigation, Self-driving cars \\
\hline
\end{tabulary}

\textbf{Detailed Applications:}

\textbf{Supervised Learning:}
\begin{itemize}
    \item \textbf{Classification}: Spam detection, sentiment analysis, image recognition
    \item \textbf{Regression}: Price forecasting, weather prediction, sales estimation
\end{itemize}

\textbf{Unsupervised Learning:}
\begin{itemize}
    \item \textbf{Clustering}: Market segmentation, gene sequencing, recommendation systems
    \item \textbf{Association}: Market basket analysis, web usage patterns
\end{itemize}

\textbf{Reinforcement Learning:}
\begin{itemize}
    \item \textbf{Control Systems}: Robot control, traffic management
    \item \textbf{Optimization}: Resource scheduling, portfolio management
\end{itemize}

\begin{mnemonicbox}
"SCR-CRO" (Supervised-Classification-Regression, Unsupervised-Clustering-Association, Reinforcement-Control-Optimization)
\end{mnemonicbox}

\questionmarks{3}{a}{3}
\textbf{Explain Single Layer Forward Network with proper diagram.}

\textbf{Answer:}

A \textbf{Single Layer Forward Network} (Perceptron) is the simplest neural network with one layer of weights between input and output.

\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=2cm, auto, >=latex]
    % Input nodes
    \node [gtu state] (x1) {$X_1$};
    \node [gtu state, below of=x1] (x2) {$X_2$};
    \node [gtu state, below of=x2] (x3) {$X_3$};
    \node [gtu state, below of=x3] (b) {$b$};
    
    % Summation node
    \node [gtu state, right=3cm of x2] (sum) {$\Sigma$};
    
    % Activation node
    \node [gtu state, right=2cm of sum] (act) {$\phi$};
    
    % Output node
    \node [right=2cm of act] (y) {$Y$};
    
    % Edges
    \path [gtu arrow] (x1) -- node [above] {$W_1$} (sum);
    \path [gtu arrow] (x2) -- node [above] {$W_2$} (sum);
    \path [gtu arrow] (x3) -- node [above] {$W_3$} (sum);
    \path [gtu arrow] (b) -- (sum);
    
    \path [gtu arrow] (sum) -- (act);
    \path [gtu arrow] (act) -- (y);
    
    % Labels
    \node [left of=x2, node distance=1.5cm, rotate=90] {Input Layer};
    \node [below of=sum, node distance=1cm] {Summation};
    \node [below of=act, node distance=1cm] {Activation};
\end{tikzpicture}
\caption{Single Layer Forward Network}
\end{figure}

\textbf{Components:}
\begin{itemize}
    \item \textbf{Inputs}: X1, X2, X3 (feature values)
    \item \textbf{Weights}: W1, W2, W3 (connection strengths)
    \item \textbf{Bias}: Additional parameter for threshold adjustment
    \item \textbf{Summation}: Weighted sum of inputs
    \item \textbf{Activation}: Function to produce output
\end{itemize}

\textbf{Mathematical Formula:}
$$Y = f(\Sigma(W_i \times X_i) + b)$$

\begin{mnemonicbox}
"IWSA" (Input-Weight-Sum-Activation)
\end{mnemonicbox}

\questionmarks{3}{b}{4}
\textbf{Write a short note on Backpropagation.}

\textbf{Answer:}

\textbf{Backpropagation} is a supervised learning algorithm used to train neural networks by adjusting weights based on error calculation.

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{Phase} & \textbf{Description} & \textbf{Action} \\
\hline
\textbf{Forward Pass} & Input propagates through network & Calculate output \\
\hline
\textbf{Error Calculation} & Compare output with target & Find error/loss \\
\hline
\textbf{Backward Pass} & Error propagates backward & Update weights \\
\hline
\textbf{Weight Update} & Adjust weights using gradient & Minimize error \\
\hline
\end{tabulary}

\textbf{Key Features:}
\begin{itemize}
    \item \textbf{Gradient Descent}: Uses calculus to find optimal weights
    \item \textbf{Chain Rule}: Calculates error contribution of each weight
    \item \textbf{Iterative Process}: Repeats until convergence
    \item \textbf{Learning Rate}: Controls speed of weight updates
\end{itemize}

\textbf{Steps:}
\begin{enumerate}
    \item Initialize random weights
    \item Forward propagation to get output
    \item Calculate error between actual and predicted
    \item Backward propagation to update weights
\end{enumerate}

\begin{mnemonicbox}
"FCBU" (Forward-Calculate-Backward-Update)
\end{mnemonicbox}

\questionmarks{3}{c}{7}
\textbf{Explain the components of architecture of Feed Forward Neuron Network.}

\textbf{Answer:}

\textbf{Feed Forward Neural Network} consists of multiple layers where information flows in one direction from input to output.

\begin{figure}[H]
\centering
\begin{tikzpicture}[x=1.5cm, y=1.2cm, >=latex]
    % Input Layer
    \foreach \i in {1,2,3}
        \node[gtu state] (I\i) at (0,-\i) {$X_\i$};

    % Hidden Layer
    \foreach \i in {1,2,3}
        \node[gtu state] (H\i) at (2,-\i) {$N_\i$};

    % Output Layer
    \foreach \i in {1,2}
        \node[gtu state] (O\i) at (4,-{\i-0.5}) {$Y_\i$};

    % Connections Input -> Hidden
    \foreach \i in {1,2,3}
        \foreach \j in {1,2,3}
            \draw[gtu arrow] (I\i) -- (H\j);

    % Connections Hidden -> Output
    \foreach \i in {1,2,3}
        \foreach \j in {1,2}
            \draw[gtu arrow] (H\i) -- (O\j);

    % Labels
    \node at (0,0) {\textbf{Input Layer}};
    \node at (2,0) {\textbf{Hidden Layer}};
    \node at (4,0) {\textbf{Output Layer}};
\end{tikzpicture}
\caption{Feed Forward Neural Network Architecture}
\end{figure}

\textbf{Components:}

\textbf{1. Input Layer:}
\begin{itemize}
    \item Receives raw data
    \item No processing, just distribution
    \item Number of neurons = number of features
\end{itemize}

\textbf{2. Hidden Layer(s):}
\begin{itemize}
    \item Performs computation and transformation
    \item Contains activation functions
    \item Can have multiple hidden layers
\end{itemize}

\textbf{3. Output Layer:}
\begin{itemize}
    \item Produces final results
    \item Number of neurons = number of outputs
    \item Uses appropriate activation for task type
\end{itemize}

\textbf{4. Weights and Biases:}
\begin{itemize}
    \item \textbf{Weights}: Connection strengths between neurons
    \item \textbf{Biases}: Threshold adjustment parameters
\end{itemize}

\textbf{5. Activation Functions:}
\begin{itemize}
    \item Introduce non-linearity
    \item Common types: ReLU, Sigmoid, Tanh
\end{itemize}

\begin{mnemonicbox}
"IHO-WA" (Input-Hidden-Output, Weights-Activation)
\end{mnemonicbox}

\questionmarks{3}{a}{3}
\textbf{Explain Multilayer Feed Forward ANN with diagram.}

\textbf{Answer:}

\textbf{Multilayer Feed Forward ANN} contains multiple hidden layers between input and output layers, enabling complex pattern recognition.

\begin{figure}[H]
\centering
\begin{tikzpicture}[x=1.5cm, y=1.2cm, >=latex]
    % Input Layer
    \foreach \i in {1,2}
        \node[gtu state] (I\i) at (0,-\i) {$X_\i$};

    % Hidden Layer 1
    \foreach \i in {1,2}
        \node[gtu state] (H1\i) at (2,-\i) {$H1_\i$};

    % Hidden Layer 2
    \foreach \i in {1,2}
        \node[gtu state] (H2\i) at (4,-\i) {$H2_\i$};
        
    % Output
    \node[gtu state] (O1) at (6,-1.5) {$Y$};

    % Connections
    \foreach \i in {1,2}
        \foreach \j in {1,2}
            \draw[gtu arrow] (I\i) -- (H1\j);
            
    \foreach \i in {1,2}
        \foreach \j in {1,2}
            \draw[gtu arrow] (H1\i) -- (H2\j);
            
    \foreach \i in {1,2}
        \draw[gtu arrow] (H2\i) -- (O1);

    % Labels
    \node at (0,0) {Input};
    \node at (2,0) {Hidden 1};
    \node at (4,0) {Hidden 2};
    \node at (6,0) {Output};
\end{tikzpicture}
\caption{Multilayer Feed Forward ANN}
\end{figure}

\textbf{Characteristics:}
\begin{itemize}
    \item \textbf{Deep Architecture}: Multiple hidden layers
    \item \textbf{Complex Patterns}: Can learn non-linear relationships
    \item \textbf{Universal Approximator}: Can approximate any continuous function
\end{itemize}

\begin{mnemonicbox}
"MDC" (Multiple layers, Deep learning, Complex patterns)
\end{mnemonicbox}

\questionmarks{3}{b}{4}
\textbf{Explain 'ReLU is the most commonly used Activation function.'}

\textbf{Answer:}

\textbf{ReLU (Rectified Linear Unit)} is widely used due to its simplicity and effectiveness in deep networks.

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{Advantage} & \textbf{Description} & \textbf{Benefit} \\
\hline
\textbf{Computational Efficiency} & Simple max(0,x) operation & Fast processing \\
\hline
\textbf{Gradient Flow} & No vanishing gradient for positive values & Better learning \\
\hline
\textbf{Sparsity} & Outputs zero for negative inputs & Efficient representation \\
\hline
\textbf{Non-linearity} & Introduces non-linear behavior & Complex pattern learning \\
\hline
\end{tabulary}

\textbf{Mathematical Definition:}
$$f(x) = \max(0, x)$$

\textbf{Comparison with Other Functions:}
\begin{itemize}
    \item \textbf{vs Sigmoid}: No saturation problem, faster computation
    \item \textbf{vs Tanh}: Simpler calculation, better gradient flow
    \item \textbf{Limitations}: Dead neurons problem for negative inputs
\end{itemize}

\textbf{Why Most Common:}
\begin{itemize}
    \item Solves vanishing gradient problem
    \item Computationally efficient
    \item Works well in practice
    \item Default choice for hidden layers
\end{itemize}

\begin{mnemonicbox}
"CGSN" (Computational, Gradient, Sparsity, Non-linear)
\end{mnemonicbox}

\questionmarks{3}{c}{7}
\textbf{Explain step by step learning process of Artificial Neural Network.}

\textbf{Answer:}

\textbf{ANN Learning Process} involves iterative weight adjustment to minimize prediction error.

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{Step} & \textbf{Process} & \textbf{Description} \\
\hline
\textbf{1. Initialization} & Set random weights & Small random values \\
\hline
\textbf{2. Forward Propagation} & Calculate output & Input $\rightarrow$ Hidden $\rightarrow$ Output \\
\hline
\textbf{3. Error Calculation} & Compare with target & Loss function computation \\
\hline
\textbf{4. Backward Propagation} & Calculate gradients & Error $\rightarrow$ Hidden $\leftarrow$ Input \\
\hline
\textbf{5. Weight Update} & Adjust parameters & Gradient descent \\
\hline
\textbf{6. Iteration} & Repeat process & Until convergence \\
\hline
\end{tabulary}

\textbf{Detailed Steps:}

\textbf{Step 1: Initialize Weights}
\begin{itemize}
    \item Assign small random values to all weights and biases
    \item Prevents symmetry breaking problem
\end{itemize}

\textbf{Step 2: Forward Propagation}
\begin{itemize}
    \item Input data flows through network layers
    \item Each neuron computes weighted sum + activation
\end{itemize}

\textbf{Step 3: Calculate Error}
\begin{itemize}
    \item Compare network output with desired output
    \item Use loss functions like MSE or Cross-entropy
\end{itemize}

\textbf{Step 4: Backward Propagation}
\begin{itemize}
    \item Calculate error gradient for each weight
    \item Use chain rule to propagate error backward
\end{itemize}

\textbf{Step 5: Update Weights}
\begin{itemize}
    \item Adjust weights using gradient descent
    \item New\_weight = Old\_weight - (learning\_rate $\times$ gradient)
\end{itemize}

\textbf{Step 6: Repeat Process}
\begin{itemize}
    \item Continue until error converges or maximum epochs reached
    \item Monitor validation performance to avoid overfitting
\end{itemize}

\begin{mnemonicbox}
"IFEBWI" (Initialize-Forward-Error-Backward-Weight-Iterate)
\end{mnemonicbox}

\questionmarks{4}{a}{3}
\textbf{List out various advantages and disadvantages of Natural Language Processing.}

\textbf{Answer:}

\begin{tabulary}{\linewidth}{|L|L|}
\hline
\textbf{Advantages} & \textbf{Disadvantages} \\
\hline
\textbf{Automation} of text processing & \textbf{Ambiguity} in human language \\
\hline
\textbf{24/7 Availability} for customer service & \textbf{Context Understanding} challenges \\
\hline
\textbf{Multilingual Support} capabilities & \textbf{Cultural Nuances} difficulty \\
\hline
\textbf{Scalability} for large datasets & \textbf{High Computational} requirements \\
\hline
\textbf{Consistency} in responses & \textbf{Data Privacy} concerns \\
\hline
\textbf{Cost Reduction} in operations & \textbf{Limited Creativity} in responses \\
\hline
\end{tabulary}

\textbf{Key Points:}
\begin{itemize}
    \item \textbf{Advantages}: Efficiency, accessibility, consistency
    \item \textbf{Disadvantages}: Complexity, resource requirements, limitations
    \item \textbf{Balance}: Benefits outweigh challenges in many applications
\end{itemize}

\begin{mnemonicbox}
"AMS-ACC" (Automation-Multilingual-Scalability vs Ambiguity-Context-Computational)
\end{mnemonicbox}

\questionmarks{4}{b}{4}
\textbf{List out preprocessing techniques in NLP and demonstrate any one with a python program.}

\textbf{Answer:}

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{Technique} & \textbf{Purpose} & \textbf{Example} \\
\hline
\textbf{Tokenization} & Split text into words/sentences & "Hello world" $\rightarrow$ ["Hello", "world"] \\
\hline
\textbf{Stop Words Removal} & Remove common words & Remove "the", "is", "and" \\
\hline
\textbf{Stemming} & Reduce words to root form & "running" $\rightarrow$ "run" \\
\hline
\textbf{Lemmatization} & Convert to dictionary form & "better" $\rightarrow$ "good" \\
\hline
\textbf{POS Tagging} & Identify parts of speech & "run" $\rightarrow$ verb \\
\hline
\textbf{Named Entity Recognition} & Identify entities & "Apple" $\rightarrow$ Organization \\
\hline
\end{tabulary}

\textbf{Python Program - Tokenization:}

\begin{lstlisting}[language=Python]
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize

# Sample text
text = "Natural Language Processing is amazing. It helps computers understand human language."

# Word tokenization
words = word_tokenize(text)
print("Words:", words)

# Sentence tokenization  
sentences = sent_tokenize(text)
print("Sentences:", sentences)
\end{lstlisting}

\begin{mnemonicbox}
"TSSL-PN" (Tokenization-Stop-Stemming-Lemmatization, POS-NER)
\end{mnemonicbox}

\questionmarks{4}{c}{7}
\textbf{Explain the phases of NLP.}

\textbf{Answer:}

\textbf{NLP Phases} represent the systematic approach to process and understand natural language.

\begin{tabulary}{\linewidth}{|L|L|L|L|}
\hline
\textbf{Phase} & \textbf{Description} & \textbf{Process} & \textbf{Example} \\
\hline
\textbf{Lexical Analysis} & Tokenization and word identification & Break text into tokens & "I am happy" $\rightarrow$ ["I", "am", "happy"] \\
\hline
\textbf{Syntactic Analysis} & Grammar and sentence structure & Parse trees, POS tagging & Identify noun, verb, adjective \\
\hline
\textbf{Semantic Analysis} & Meaning extraction & Word sense disambiguation & "Bank" $\rightarrow$ financial vs river \\
\hline
\textbf{Discourse Integration} & Context across sentences & Resolve pronouns, references & "He" refers to "John" \\
\hline
\textbf{Pragmatic Analysis} & Intent and context understanding & Consider situation/culture & Sarcasm, idioms interpretation \\
\hline
\end{tabulary}

\textbf{Detailed Explanation:}

\textbf{1. Lexical Analysis:}
\begin{itemize}
    \item First phase of NLP pipeline
    \item Converts character stream into tokens
    \item Removes punctuation and special characters
\end{itemize}

\textbf{2. Syntactic Analysis:}
\begin{itemize}
    \item Analyzes grammatical structure
    \item Creates parse trees
    \item Identifies sentence components
\end{itemize}

\textbf{3. Semantic Analysis:}
\begin{itemize}
    \item Extracts meaning from text
    \item Handles word ambiguity
    \item Maps words to concepts
\end{itemize}

\textbf{4. Discourse Integration:}
\begin{itemize}
    \item Analyzes text beyond sentence level
    \item Maintains context across sentences
    \item Resolves references and connections
\end{itemize}

\textbf{5. Pragmatic Analysis:}
\begin{itemize}
    \item Considers real-world context
    \item Understands speaker's intent
    \item Handles figurative language
\end{itemize}

\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=1.8cm, auto, >=latex]
    \node [gtu block] (raw) {Raw Text};
    \node [gtu block, right of=raw, node distance=2.5cm] (lex) {Lexical\\Analysis};
    \node [gtu block, right of=lex, node distance=2.5cm] (syn) {Syntactic\\Analysis};
    \node [gtu block, right of=syn, node distance=2.5cm] (sem) {Semantic\\Analysis};
    
    \node [gtu block, below of=syn] (dis) {Discourse\\Integration};
    \node [gtu block, left of=dis, node distance=2.5cm] (pra) {Pragmatic\\Analysis};
    \node [gtu block, left of=pra, node distance=2.5cm] (und) {Understanding};
    
    \path [gtu arrow] (raw) -- (lex);
    \path [gtu arrow] (lex) -- (syn);
    \path [gtu arrow] (syn) -- (sem);
    \path [gtu arrow] (sem) |- (dis);
    \path [gtu arrow] (dis) -- (pra);
    \path [gtu arrow] (pra) -- (und);
\end{tikzpicture}
\caption{Phases of NLP}
\end{figure}

\begin{mnemonicbox}
"LSSDP" (Lexical-Syntactic-Semantic-Discourse-Pragmatic)
\end{mnemonicbox}

\questionmarks{4}{a}{3}
\textbf{What is Natural Language Processing? List out its applications.}

\textbf{Answer:}

\textbf{Natural Language Processing (NLP)} is a branch of AI that enables computers to understand, interpret, and generate human language in a meaningful way.

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{Category} & \textbf{Applications} & \textbf{Examples} \\
\hline
\textbf{Communication} & Chatbots, Virtual assistants & Siri, Alexa, ChatGPT \\
\hline
\textbf{Translation} & Language translation & Google Translate \\
\hline
\textbf{Analysis} & Sentiment analysis, Text mining & Social media monitoring \\
\hline
\textbf{Search} & Information retrieval & Search engines \\
\hline
\textbf{Writing} & Grammar checking, Auto-complete & Grammarly, predictive text \\
\hline
\textbf{Business} & Document processing, Spam detection & Email filtering \\
\hline
\end{tabulary}

\textbf{Key Applications:}
\begin{itemize}
    \item \textbf{Machine Translation}: Converting text between languages
    \item \textbf{Speech Recognition}: Converting speech to text
    \item \textbf{Text Summarization}: Creating concise summaries
    \item \textbf{Question Answering}: Providing answers to queries
\end{itemize}

\begin{mnemonicbox}
"CTAS-WB" (Communication-Translation-Analysis-Search, Writing-Business)
\end{mnemonicbox}

\questionmarks{4}{b}{4}
\textbf{List out the tasks performed with WordNet in NLTK and demonstrate anyone with a python code.}

\textbf{Answer:}

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{Task} & \textbf{Description} & \textbf{Purpose} \\
\hline
\textbf{Synsets} & Find synonymous words & Word similarity \\
\hline
\textbf{Definitions} & Get word meanings & Understanding context \\
\hline
\textbf{Examples} & Usage examples & Practical application \\
\hline
\textbf{Hyponyms} & Find specific terms & Hierarchical relationships \\
\hline
\textbf{Hypernyms} & Find general terms & Category identification \\
\hline
\textbf{Antonyms} & Find opposite words & Contrast analysis \\
\hline
\end{tabulary}

\textbf{Python Code - Synsets and Definitions:}

\begin{lstlisting}[language=Python]
from nltk.corpus import wordnet

# Get synsets for word 'good'
synsets = wordnet.synsets('good')
print("Synsets:", synsets)

# Get definition
definition = synsets[0].definition()
print("Definition:", definition)

# Get examples
examples = synsets[0].examples()
print("Examples:", examples)
\end{lstlisting}

\begin{mnemonicbox}
"SDEHA" (Synsets-Definitions-Examples-Hyponyms-Antonyms)
\end{mnemonicbox}

\questionmarks{4}{c}{7}
\textbf{Explain the types of ambiguities in NLP.}

\textbf{Answer:}

\textbf{NLP Ambiguities} occur when text can be interpreted in multiple ways, creating challenges for automated understanding.

\begin{tabulary}{\linewidth}{|L|L|L|L|}
\hline
\textbf{Type} & \textbf{Description} & \textbf{Example} & \textbf{Resolution} \\
\hline
\textbf{Lexical} & Multiple meanings of single word & "Bank" (financial/river) & Context analysis \\
\hline
\textbf{Syntactic} & Multiple grammatical interpretations & "Flying planes can be dangerous" & Parse trees \\
\hline
\textbf{Semantic} & Multiple meanings at sentence level & "Time flies like an arrow" & Semantic analysis \\
\hline
\textbf{Pragmatic} & Context-dependent interpretation & "Can you pass the salt?" & Situational context \\
\hline
\textbf{Referential} & Unclear pronoun references & "John told Bob he was wrong" & Discourse analysis \\
\hline
\end{tabulary}

\textbf{Detailed Explanation:}

\textbf{1. Lexical Ambiguity:}
\begin{itemize}
    \item Same word, different meanings
    \item Homonyms and polysemes
    \item Example: "Bat" (animal/sports equipment)
\end{itemize}

\textbf{2. Syntactic Ambiguity:}
\begin{itemize}
    \item Multiple grammatical structures
    \item Different parse trees possible
    \item Example: "I saw a man with a telescope"
\end{itemize}

\textbf{3. Semantic Ambiguity:}
\begin{itemize}
    \item Sentence-level meaning confusion
    \item Multiple interpretations possible
    \item Example: "Visiting relatives can be boring"
\end{itemize}

\textbf{4. Pragmatic Ambiguity:}
\begin{itemize}
    \item Context and intent dependent
    \item Cultural and situational factors
    \item Example: Sarcasm and indirect requests
\end{itemize}

\textbf{5. Referential Ambiguity:}
\begin{itemize}
    \item Unclear references to entities
    \item Pronoun resolution challenges
    \item Example: Multiple possible antecedents
\end{itemize}

\textbf{Resolution Strategies:}
\begin{itemize}
    \item Context analysis and machine learning
    \item Statistical disambiguation methods
    \item Knowledge bases and ontologies
\end{itemize}

\begin{mnemonicbox}
"LSSPR" (Lexical-Syntactic-Semantic-Pragmatic-Referential)
\end{mnemonicbox}

\questionmarks{5}{a}{3}
\textbf{Explain Bag of Words with example.}

\textbf{Answer:}

\textbf{Bag of Words (BoW)} is a text representation method that converts text into numerical vectors based on word frequency, ignoring grammar and word order.

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{Step} & \textbf{Process} & \textbf{Description} \\
\hline
\textbf{1. Tokenization} & Split text into words & Create vocabulary \\
\hline
\textbf{2. Vocabulary Creation} & Unique words collection & Dictionary of terms \\
\hline
\textbf{3. Vector Creation} & Count word frequencies & Numerical representation \\
\hline
\end{tabulary}

\textbf{Example:}

Documents:
\begin{itemize}
    \item Doc1: "I love machine learning"
    \item Doc2: "Machine learning is amazing"
\end{itemize}

\textbf{Vocabulary:} [I, love, machine, learning, is, amazing]

\textbf{BoW Vectors:}
\begin{itemize}
    \item Doc1: [1, 1, 1, 1, 0, 0]
    \item Doc2: [0, 0, 1, 1, 1, 1]
\end{itemize}

\textbf{Characteristics:}
\begin{itemize}
    \item \textbf{Order Independent}: Word sequence ignored
    \item \textbf{Frequency Based}: Counts word occurrences
    \item \textbf{Sparse Representation}: Many zero values
\end{itemize}

\begin{mnemonicbox}
"TVC" (Tokenize-Vocabulary-Count)
\end{mnemonicbox}

\questionmarks{5}{b}{4}
\textbf{What is Word2Vec? Explain its steps.}

\textbf{Answer:}

\textbf{Word2Vec} is a neural network-based technique that creates dense vector representations of words by learning from their context in large text corpora.

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{Model} & \textbf{Approach} & \textbf{Prediction} \\
\hline
\textbf{CBOW} & Continuous Bag of Words & Context $\rightarrow$ Target word \\
\hline
\textbf{Skip-gram} & Skip-gram with Negative Sampling & Target word $\rightarrow$ Context \\
\hline
\end{tabulary}

\textbf{Steps of Word2Vec:}

\textbf{1. Data Preparation:}
\begin{itemize}
    \item Collect large text corpus
    \item Clean and preprocess text
    \item Create training pairs
\end{itemize}

\textbf{2. Model Architecture:}
\begin{itemize}
    \item Input layer (one-hot encoded words)
    \item Hidden layer (embedding layer)
    \item Output layer (softmax for prediction)
\end{itemize}

\textbf{3. Training Process:}
\begin{itemize}
    \item \textbf{CBOW}: Predict target word from context
    \item \textbf{Skip-gram}: Predict context from target word
    \item Use backpropagation to update weights
\end{itemize}

\textbf{4. Vector Extraction:}
\begin{itemize}
    \item Extract weight matrix from hidden layer
    \item Each row represents word embedding
    \item Typically 100-300 dimensions
\end{itemize}

\textbf{Benefits:}
\begin{itemize}
    \item Captures semantic relationships
    \item Similar words have similar vectors
    \item Supports arithmetic operations (King - Man + Woman = Queen)
\end{itemize}

\begin{mnemonicbox}
"DMAT" (Data-Model-Architecture-Training)
\end{mnemonicbox}

\questionmarks{5}{c}{7}
\textbf{List out applications of NLP and explain any one in detail.}

\textbf{Answer:}

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{Application} & \textbf{Description} & \textbf{Industry Use} \\
\hline
\textbf{Machine Translation} & Language conversion & Global communication \\
\hline
\textbf{Sentiment Analysis} & Opinion mining & Social media monitoring \\
\hline
\textbf{Chatbots} & Conversational AI & Customer service \\
\hline
\textbf{Text Summarization} & Content condensation & News, research \\
\hline
\textbf{Speech Recognition} & Voice to text & Virtual assistants \\
\hline
\textbf{Information Extraction} & Data mining from text & Business intelligence \\
\hline
\textbf{Question Answering} & Automated responses & Search engines \\
\hline
\textbf{Spam Detection} & Email filtering & Cybersecurity \\
\hline
\end{tabulary}

\textbf{Detailed Explanation: Sentiment Analysis}

\textbf{Sentiment Analysis} is the process of determining emotional tone and opinions expressed in text data.

\textbf{Components:}
\begin{itemize}
    \item \textbf{Text Preprocessing}: Cleaning and tokenization
    \item \textbf{Feature Extraction}: TF-IDF, word embeddings
    \item \textbf{Classification}: Positive, negative, neutral
    \item \textbf{Confidence Scoring}: Strength of sentiment
\end{itemize}

\textbf{Process Steps:}
\begin{enumerate}
    \item \textbf{Data Collection}: Gather text from reviews, social media
    \item \textbf{Preprocessing}: Remove noise, normalize text
    \item \textbf{Feature Engineering}: Convert text to numerical features
    \item \textbf{Model Training}: Use ML algorithms for classification
    \item \textbf{Prediction}: Classify new text sentiment
    \item \textbf{Evaluation}: Measure accuracy and performance
\end{enumerate}

\textbf{Applications:}
\begin{itemize}
    \item \textbf{Brand Monitoring}: Track customer opinions
    \item \textbf{Product Reviews}: Analyze customer feedback
    \item \textbf{Social Media}: Monitor public sentiment
    \item \textbf{Market Research}: Understand consumer preferences
\end{itemize}

\begin{mnemonicbox}
"MSCTSIQ-S" (Machine-Sentiment-Chatbot-Text-Speech-Information-Question-Spam)
\end{mnemonicbox}

\questionmarks{5}{a}{3}
\textbf{Explain TFIDF with example.}

\textbf{Answer:}

\textbf{TF-IDF (Term Frequency-Inverse Document Frequency)} measures word importance in a document relative to a collection of documents.

\textbf{Formula:}
$$TF-IDF = TF(t,d) \times IDF(t)$$

Where:
\begin{itemize}
    \item TF(t,d) = (Number of times term t appears in document d) / (Total terms in document d)
    \item IDF(t) = log(Total documents / Documents containing term t)
\end{itemize}

\textbf{Example:}

Documents:
\begin{itemize}
    \item Doc1: "machine learning is good"
    \item Doc2: "learning algorithms are good"
    \item Doc3: "machine algorithms work well"
\end{itemize}

\textbf{Table: TF-IDF Calculation for "machine"}

\begin{tabulary}{\linewidth}{|L|L|L|L|}
\hline
\textbf{Document} & \textbf{TF} & \textbf{IDF} & \textbf{TF-IDF} \\
\hline
Doc1 & 1/4 = 0.25 & log(3/2) = 0.18 & 0.25 $\times$ 0.18 = 0.045 \\
\hline
Doc2 & 0/4 = 0 & log(3/2) = 0.18 & 0 $\times$ 0.18 = 0 \\
\hline
Doc3 & 1/4 = 0.25 & log(3/2) = 0.18 & 0.25 $\times$ 0.18 = 0.045 \\
\hline
\end{tabulary}

\textbf{Key Points:}
\begin{itemize}
    \item \textbf{High TF-IDF}: Important word in specific document
    \item \textbf{Low TF-IDF}: Common word across documents
    \item \textbf{Applications}: Information retrieval, text mining
\end{itemize}

\begin{mnemonicbox}
"TI-FD" (Term frequency, Inverse Document frequency)
\end{mnemonicbox}

\questionmarks{5}{b}{4}
\textbf{Explain about challenges with TFIDF and BOW.}

\textbf{Answer:}

\begin{tabulary}{\linewidth}{|L|L|L|L|}
\hline
\textbf{Challenge} & \textbf{TF-IDF} & \textbf{BOW} & \textbf{Impact} \\
\hline
\textbf{Semantic Understanding} & Cannot capture meaning & Ignores word relationships & Poor context understanding \\
\hline
\textbf{Word Order} & Position ignored & Sequence lost & Grammar meaning lost \\
\hline
\textbf{Sparsity} & High-dimensional vectors & Many zero values & Memory inefficient \\
\hline
\textbf{Vocabulary Size} & Large feature space & Grows with corpus & Computational complexity \\
\hline
\textbf{Out-of-Vocabulary} & Unknown words ignored & New words not handled & Limited generalization \\
\hline
\textbf{Polysemy} & Multiple meanings & Same treatment for different senses & Ambiguity issues \\
\hline
\end{tabulary}

\textbf{Detailed Challenges:}

\textbf{1. Lack of Semantic Understanding:}
\begin{itemize}
    \item Words treated as independent features
    \item Cannot understand synonyms or related concepts
    \item "Good" and "excellent" treated differently
\end{itemize}

\textbf{2. Loss of Word Order:}
\begin{itemize}
    \item "Dog bites man" vs "Man bites dog" same representation
    \item Context and grammar information lost
    \item Sentence structure ignored
\end{itemize}

\textbf{3. High Dimensionality:}
\begin{itemize}
    \item Vector size equals vocabulary size
    \item Sparse matrices with mostly zeros
    \item Storage and computation problems
\end{itemize}

\textbf{4. Context Insensitivity:}
\begin{itemize}
    \item Same word different contexts treated equally
    \item "Apple" company vs fruit same representation
    \item Polysemy and homonymy issues
\end{itemize}

\textbf{Solutions:}
\begin{itemize}
    \item \textbf{Word Embeddings}: Word2Vec, GloVe
    \item \textbf{Contextual Models}: BERT, GPT
    \item \textbf{N-grams}: Capture some word order
    \item \textbf{Dimensionality Reduction}: PCA, SVD
\end{itemize}

\begin{mnemonicbox}
"SSVO-CP" (Semantic-Sequence-Vocabulary-OOV, Context-Polysemy)
\end{mnemonicbox}

\questionmarks{5}{c}{7}
\textbf{Explain the working of GloVe.}

\textbf{Answer:}

\textbf{GloVe (Global Vectors for Word Representation)} combines global statistical information with local context windows to create word embeddings.

\begin{tabulary}{\linewidth}{|L|L|L|L|}
\hline
\textbf{Aspect} & \textbf{GloVe} & \textbf{Word2Vec} & \textbf{Traditional Methods} \\
\hline
\textbf{Approach} & Global + Local statistics & Local context windows & Frequency-based \\
\hline
\textbf{Training} & Matrix factorization & Neural networks & Counting methods \\
\hline
\textbf{Efficiency} & Fast training & Slower training & Very fast \\
\hline
\textbf{Performance} & High accuracy & Good accuracy & Limited performance \\
\hline
\end{tabulary}

\textbf{Working Process:}

\textbf{1. Co-occurrence Matrix Construction:}
\begin{itemize}
    \item Count word co-occurrences in context windows
    \item Create global statistics matrix
    \item $X_{ij}$ = number of times word j appears in context of word i
\end{itemize}

\textbf{2. Ratio Calculation:}
\begin{itemize}
    \item Calculate probability ratios
    \item $P(k|i) = X_{ik} / X_i$ (probability of word k given word i)
    \item Focus on meaningful ratios between probabilities
\end{itemize}

\textbf{3. Objective Function:}
\begin{itemize}
    \item Minimize weighted least squares objective
    \item $J = \sum f(X_{ij})(w_i^T w_j + b_i + b_j - \log X_{ij})^2$
    \item Where $f(x)$ is weighting function
\end{itemize}

\textbf{4. Vector Learning:}
\begin{itemize}
    \item Use gradient descent to optimize objective
    \item Learn word vectors $w_i$ and context vectors $w_j$
    \item Final representation combines both vectors
\end{itemize}

\textbf{Key Features:}

\textbf{Global Statistics:}
\begin{itemize}
    \item Uses entire corpus information
    \item Captures global word relationships
    \item More stable than local methods
\end{itemize}

\textbf{Efficiency:}
\begin{itemize}
    \item Trains on co-occurrence statistics
    \item Faster than neural network methods
    \item Scalable to large corpora
\end{itemize}

\textbf{Performance:}
\begin{itemize}
    \item Performs well on analogy tasks
    \item Captures both semantic and syntactic relationships
    \item Good performance on similarity tasks
\end{itemize}

\textbf{Mathematical Foundation:}
$$J = \sum_{i,j=1}^{V} f(X_{ij})(w_i^T w_j + b_i + b_j - \log X_{ij})^2$$

Where:
\begin{itemize}
    \item V = vocabulary size
    \item $X_{ij}$ = co-occurrence count
    \item $w_i, w_j$ = word vectors
    \item $b_i, b_j$ = bias terms
    \item $f(x)$ = weighting function
\end{itemize}

\textbf{Advantages:}
\begin{itemize}
    \item \textbf{Combines Benefits}: Global statistics + local context
    \item \textbf{Interpretable}: Clear mathematical foundation
    \item \textbf{Efficient}: Faster training than Word2Vec
    \item \textbf{Effective}: Good performance on various tasks
\end{itemize}

\textbf{Applications:}
\begin{itemize}
    \item \textbf{Word Similarity}: Find related words
    \item \textbf{Analogy Tasks}: King - Man + Woman = Queen
    \item \textbf{Text Classification}: Feature representation
    \item \textbf{Machine Translation}: Cross-lingual mappings
\end{itemize}

\begin{mnemonicbox}
"CROF-PGAE" (Co-occurrence-Ratio-Objective-Function, Performance-Global-Advantage-Efficiency)
\end{mnemonicbox}

\end{document}
