\documentclass{article}
\input{../../../../../../../latex-templates/gtu-solutions-short/preamble.tex}
\input{../../../../../../../latex-templates/gtu-solutions-short/gujarati-boxes.tex}
\input{../../../../../../../latex-templates/gtu-solutions-short/commands.tex}

\title{Foundation of AI and ML (4351601) - Winter 2023 Solution (Gujarati)}
\author{Milav Dabgar}
\date{December 04, 2023}

\begin{document}
\maketitle

\questionmarks{1}{a}{3}
\textbf{નીચેની terms ની વ્યાખ્યા આપો: (1) Artificial Intelligence (2) Expert System.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L}
        \hline
        \textbf{Term} & \textbf{વ્યાખ્યા} \\
        \hline
        \textbf{Artificial Intelligence} & AI એ computer science ની એક શાખા છે જે એવા machines બનાવે છે જે સામાન્ય રીતે માનવ બુદ્ધિની જરૂર પડતા કાર્યો કરી શકે છે, જેમ કે learning, reasoning અને problem-solving. \\
        \textbf{Expert System} & Expert system એ એક computer program છે જે knowledge અને inference rules નો ઉપયોગ કરીને એવી problems solve કરે છે જેમાં સામાન્ય રીતે ચોક્કસ ક્ષેત્રમાં માનવ expertise ની જરૂર પડે છે. \\
        \hline
    \end{tabulary}

    \begin{itemize}
        \item \textbf{AI ની વિશેષતાઓ}: Learning, reasoning, perception
        \item \textbf{Expert system ના ભાગો}: Knowledge base, inference engine
    \end{itemize}

    \begin{mnemonicbox}
    "AI શીખે છે, Expert સલાહ આપે છે"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{1}{b}{4}
\textbf{Biological Neural Network અને Artificial Neural Network ની સરખામણી કરો.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L L}
        \hline
        \textbf{પાસું} & \textbf{Biological Neural Network} & \textbf{Artificial Neural Network} \\
        \hline
        \textbf{Processing} & Parallel processing & Sequential/parallel processing \\
        \textbf{ઝડપ} & ધીમી (milliseconds) & ઝડપી (nanoseconds) \\
        \textbf{શીખવું} & સતત શીખવું & Batch/online learning \\
        \textbf{Storage} & વિતરિત storage & કેન્દ્રિય storage \\
        \hline
    \end{tabulary}

    \begin{itemize}
        \item \textbf{Biological}: જટિલ, fault-tolerant, સ્વ-સુધારણા કરે છે
        \item \textbf{Artificial}: સરળ, ચોક્કસ, programmable
    \end{itemize}

    \begin{mnemonicbox}
    "Bio જટિલ છે, AI સરળ છે"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{1}{c}{7}
\textbf{AI ના પ્રકારો તેની applications સાથે સમજાવો.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L L}
        \hline
        \textbf{AI નો પ્રકાર} & \textbf{વર્ણન} & \textbf{Applications} \\
        \hline
        \textbf{Narrow AI} & ચોક્કસ કાર્યો માટે design કરેલ AI & Voice assistants, recommendation systems \\
        \textbf{General AI} & માનવ સ્તરની intelligence વાળી AI & હજુ સુધી પ્રાપ્ત નથી \\
        \textbf{Super AI} & માનવ intelligence કરતાં વધારે AI & સૈદ્ધાંતિક વિભાવના \\
        \hline
    \end{tabulary}

    \begin{figure}[H]
        \centering
        \begin{tikzpicture}[node distance=2cm, auto]
            \node [gtu block] (types) {AI ના પ્રકારો};
            \node [gtu block, below left=of types] (narrow) {Narrow AI};
            \node [gtu block, below=of types] (general) {General AI};
            \node [gtu block, below right=of types] (super) {Super AI};
            
            \node [gtu state, below=0.5cm of narrow] (narrow_app) {Siri, Alexa};
            \node [gtu state, below=0.5cm of general] (general_app) {માનવ સ્તરના કાર્યો};
            \node [gtu state, below=0.5cm of super] (super_app) {માનવ Intelligence કરતાં વધારે};

            \path [gtu arrow] (types) -- (narrow);
            \path [gtu arrow] (types) -- (general);
            \path [gtu arrow] (types) -- (super);
            \path [gtu arrow] (narrow) -- (narrow_app);
            \path [gtu arrow] (general) -- (general_app);
            \path [gtu arrow] (super) -- (super_app);
        \end{tikzpicture}
        \caption{Artificial Intelligence ના પ્રકારો}
    \end{figure}

    \begin{itemize}
        \item \textbf{હાલનું focus}: Narrow AI આજના applications પર પ્રભુત્વ ધરાવે છે
        \item \textbf{ભવિષ્યનું લક્ષ્ય}: General AI ને સુરક્ષિત રીતે પ્રાપ્ત કરવું
    \end{itemize}

    \begin{mnemonicbox}
    "હવે Narrow, લક્ષ્ય General, Super ડરામણી"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{1}{c}{7}
\textbf{AI ethics અને limitations સમજાવો.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L}
        \hline
        \textbf{Ethics નું પાસું} & \textbf{વર્ણન} \\
        \hline
        \textbf{Privacy} & વ્યક્તિગત data અને user information ની સુરક્ષા \\
        \textbf{Bias} & વિવિધ જૂથોમાં નિષ્પક્ષતા સુનિશ્ચિત કરવી \\
        \textbf{Transparency} & AI નિર્ણયોને સમજાવી શકાય તેવા બનાવવા \\
        \textbf{Accountability} & AI actions માટે જવાબદારી નક્કી કરવી \\
        \hline
    \end{tabulary}

    \textbf{મર્યાદાઓ:}
    \begin{itemize}
        \item \textbf{Data dependency}: મોટા, ગુણવત્તાવાળા datasets ની જરૂર
        \item \textbf{Computational power}: નોંધપાત્ર processing resources ની જરૂર
        \item \textbf{Creativity નો અભાવ}: ખરેખર મૌલિક concepts બનાવી શકતી નથી
    \end{itemize}

    \begin{mnemonicbox}
    "Privacy, Bias, Transparency, Accountability"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{2}{a}{3}
\textbf{નીચેની terms ની વ્યાખ્યા આપો: (1) Well posed Learning Problem (2) Machine Learning.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L}
        \hline
        \textbf{Term} & \textbf{વ્યાખ્યા} \\
        \hline
        \textbf{Well posed Learning Problem} & એક learning problem જેમાં સ્પષ્ટ રીતે વ્યાખ્યાયિત task (T), performance measure (P), અને experience (E) હોય જ્યાં experience સાથે performance સુધરે છે. \\
        \textbf{Machine Learning} & AI નો એક ભાગ જે computers ને experience થી આપોઆપ શીખવા અને સુધારવા માટે સક્ષમ બનાવે છે, સ્પષ્ટ રીતે program કર્યા વગર. \\
        \hline
    \end{tabulary}

    \begin{itemize}
        \item \textbf{Well posed formula}: T + P + E = Learning
        \item \textbf{ML નો ફાયદો}: Data થી આપોઆપ સુધારો
    \end{itemize}

    \begin{mnemonicbox}
    "Task, Performance, Experience"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{2}{b}{4}
\textbf{Reinforcement Learning તેમાં ઉપયોગ થતાં terms સાથે સમજાવો.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L}
        \hline
        \textbf{Term} & \textbf{વર્ણન} \\
        \hline
        \textbf{Agent} & શીખનાર અથવા નિર્ણય લેનાર \\
        \textbf{Environment} & જે દુનિયામાં agent કામ કરે છે \\
        \textbf{Action} & દરેક state માં agent શું કરી શકે છે \\
        \textbf{State} & Agent ની હાલની સ્થિતિ \\
        \textbf{Reward} & Environment તરફથી feedback \\
        \hline
    \end{tabulary}

    \begin{figure}[H]
        \centering
        \begin{tikzpicture}[node distance=2.5cm, auto]
            \node [gtu block] (agent) {Agent};
            \node [gtu block, right=4cm of agent] (env) {Environment};

            \path [gtu arrow, bend left] (agent) edge node {Action} (env);
            \path [gtu arrow, bend left] (env) edge node {State, Reward} (agent);
        \end{tikzpicture}
        \caption{Reinforcement Learning Cycle}
    \end{figure}

    \begin{itemize}
        \item \textbf{શીખવાની પ્રક્રિયા}: Trial and error approach
        \item \textbf{લક્ષ્ય}: કુલ reward વધારવું
    \end{itemize}

    \begin{mnemonicbox}
    "Agent કરે છે, Environment State અને Reward આપે છે"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{2}{c}{7}
\textbf{Supervised, Unsupervised અને Reinforcement Learning ની સરખામણી કરો.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L L L}
        \hline
        \textbf{પાસું} & \textbf{Supervised} & \textbf{Unsupervised} & \textbf{Reinforcement} \\
        \hline
        \textbf{Data} & Labeled data & Unlabeled data & Interactive data \\
        \textbf{લક્ષ્ય} & Output predict કરવું & Patterns શોધવા & Reward વધારવું \\
        \textbf{Feedback} & તુરંત & કોઈ નહીં & વિલંબિત \\
        \textbf{ઉદાહરણો} & Classification & Clustering & Game playing \\
        \hline
    \end{tabulary}

    \begin{itemize}
        \item \textbf{Supervised}: શિક્ષક-માર્ગદર્શિત learning
        \item \textbf{Unsupervised}: સ્વ-શોધ learning
        \item \textbf{Reinforcement}: Trial-and-error learning
    \end{itemize}

    \begin{mnemonicbox}
    "Supervised પાસે શિક્ષક, Unsupervised શોધે છે, Reinforcement પ્રયત્ન કરે છે"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{2}{a}{3}
\textbf{Reinforcement Learning ના key features લખો.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L}
        \hline
        \textbf{Feature} & \textbf{વર્ણન} \\
        \hline
        \textbf{Trial and Error} & પ્રયોગ દ્વારા શીખવું \\
        \textbf{Delayed Reward} & Actions પછી feedback મળે છે \\
        \textbf{Sequential Decision} & Actions ભવિષ્યના states ને અસર કરે છે \\
        \hline
    \end{tabulary}

    \begin{itemize}
        \item \textbf{કોઈ supervisor નથી}: Agent સ્વતંત્ર રીતે શીખે છે
        \item \textbf{Exploration vs Exploitation}: નવા actions અજમાવવા અને જાણીતા સારા actions વાપરવા વચ્ચે સંતુલન
    \end{itemize}

    \begin{mnemonicbox}
    "પ્રયત્ન, વિલંબ, ક્રમ"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{2}{b}{4}
\textbf{Reinforcement Learning ના પ્રકારો સમજાવો.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L}
        \hline
        \textbf{પ્રકાર} & \textbf{વર્ણન} \\
        \hline
        \textbf{Positive RL} & વર્તણૂક વધારવા માટે positive stimulus ઉમેરવું \\
        \textbf{Negative RL} & વર્તણૂક વધારવા માટે negative stimulus દૂર કરવું \\
        \hline
    \end{tabulary}

    \textbf{Learning આધારિત:}
    \begin{itemize}
        \item \textbf{Model-based}: Agent environment model શીખે છે
        \item \textbf{Model-free}: Agent સીધો experience થી શીખે છે
    \end{itemize}

    \begin{mnemonicbox}
    "Positive ઉમેરે, Negative દૂર કરે"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{2}{c}{7}
\textbf{Reinforcement Learning implement કરવા માટેના approaches સમજાવો.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L L}
        \hline
        \textbf{Approach} & \textbf{વર્ણન} & \textbf{ઉદાહરણ} \\
        \hline
        \textbf{Value-based} & States/actions ના value શીખવા & Q-Learning \\
        \textbf{Policy-based} & Policy સીધી શીખવી & Policy Gradient \\
        \textbf{Model-based} & Environment model શીખવું & Dynamic Programming \\
        \hline
    \end{tabulary}

    \begin{figure}[H]
        \centering
        \begin{tikzpicture}[node distance=1.5cm, auto]
            \node [gtu block] (rl) {RL Approaches};
            \node [gtu block, below left=of rl] (value) {Value-based};
            \node [gtu block, below=of rl] (policy) {Policy-based};
            \node [gtu block, below right=of rl] (model) {Model-based};
            
            \node [gtu state, below=0.5cm of value] (qlearn) {Q-Learning};
            \node [gtu state, below=0.5cm of policy] (pgrad) {Policy Gradient};
            \node [gtu state, below=0.5cm of model] (dp) {Dynamic Prog.};

            \path [gtu arrow] (rl) -- (value);
            \path [gtu arrow] (rl) -- (policy);
            \path [gtu arrow] (rl) -- (model);
            \path [gtu arrow] (value) -- (qlearn);
            \path [gtu arrow] (policy) -- (pgrad);
            \path [gtu arrow] (model) -- (dp);
        \end{tikzpicture}
        \caption{Reinforcement Learning Approaches}
    \end{figure}

    \begin{itemize}
        \item \textbf{Value-based}: Value functions estimate કરે છે
        \item \textbf{Policy-based}: Policy parameters optimize કરે છે
        \item \textbf{Model-based}: Environment model વાપરે છે
    \end{itemize}

    \begin{mnemonicbox}
    "Value, Policy, Model"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{3}{a}{3}
\textbf{Activation functions ReLU અને sigmoid વર્ણવો.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L L}
        \hline
        \textbf{Function} & \textbf{Formula} & \textbf{Range} \\
        \hline
        \textbf{ReLU} & f(x) = max(0, x) & [0, $\infty$) \\
        \textbf{Sigmoid} & f(x) = 1/(1 + $e^{-x}$) & (0, 1) \\
        \hline
    \end{tabulary}

    \begin{itemize}
        \item \textbf{ReLU નો ફાયદો}: Vanishing gradient problem નથી
        \item \textbf{Sigmoid નો ફાયદો}: Smooth gradient, probabilistic output
    \end{itemize}

    \begin{mnemonicbox}
    "ReLU સુધારે છે, Sigmoid દબાવે છે"
\end{mnemonicbox}
\end{solutionbox}


\questionmarks{3}{b}{4}
\textbf{Multi-layer feed forward ANN સમજાવો.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L}
        \hline
        \textbf{Component} & \textbf{વર્ણન} \\
        \hline
        \textbf{Input Layer} & Input data receive કરે છે \\
        \textbf{Hidden Layers} & Information process કરે છે (multiple layers) \\
        \textbf{Output Layer} & Final result બનાવે છે \\
        \textbf{Connections} & ફક્ત forward direction માં \\
        \hline
    \end{tabulary}

    \begin{itemize}
        \item \textbf{Information flow}: Input થી output સુધી એક દિશામાં
        \item \textbf{કોઈ cycles નથી}: કોઈ feedback connections નથી
    \end{itemize}

    \begin{mnemonicbox}
    "Input \textrightarrow Hidden \textrightarrow Output (ફક્ત આગળ)"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{3}{c}{7}
\textbf{ANN નું structure દોરો અને તેના દરેક components ની functionality સમજાવો.}

\begin{solutionbox}
    \begin{figure}[H]
        \centering
        \begin{tikzpicture}[
            neuron/.style={circle, fill=blue!10, draw=blue, thick, minimum size=0.8cm}
        ]
            % Input Layer
            \node[neuron] (x1) at (0,2) {$x_1$};
            \node[neuron] (x2) at (0,1) {$x_2$};
            \node[neuron] (x3) at (0,0) {$x_3$};
            
            % Hidden Layer
            \node[neuron] (h1) at (3,2.5) {$h_1$};
            \node[neuron] (h2) at (3,1.5) {$h_2$};
            \node[neuron] (h3) at (3,0.5) {$h_3$};
            \node[neuron] (h4) at (3,-0.5) {$h_4$};
            
            % Output Layer
            \node[neuron] (y1) at (6,1.5) {$y_1$};
            \node[neuron] (y2) at (6,0.5) {$y_2$};
            
            % Connections from input to hidden
            \foreach \i in {1,2,3}
                \foreach \j in {1,2,3,4}
                    \draw[->] (x\i) -- (h\j);
            
            % Connections from hidden to output
            \foreach \i in {1,2,3,4}
                \foreach \j in {1,2}
                    \draw[->] (h\i) -- (y\j);
            
            % Labels
            \node[above=0.3cm of x1] {Input Layer};
            \node[above=0.3cm of h1] {Hidden Layer};
            \node[above=0.3cm of y1] {Output Layer};
        \end{tikzpicture}
        \caption{Artificial Neural Network Structure}
    \end{figure}

    \begin{tabulary}{\linewidth}{L L}
        \hline
        \textbf{Component} & \textbf{Functionality} \\
        \hline
        \textbf{Neurons} & Processing units જે inputs receive કરે છે અને outputs બનાવે છે \\
        \textbf{Weights} & Neurons વચ્ચેની connection strengths \\
        \textbf{Bias} & Activation function ને shift કરવા માટે વધારાનું parameter \\
        \textbf{Activation Function} & Network માં non-linearity લાવે છે \\
        \hline
    \end{tabulary}

    \begin{itemize}
        \item \textbf{Input layer}: Input data receive કરે છે અને વિતરિત કરે છે
        \item \textbf{Hidden layers}: Features અને patterns extract કરે છે
        \item \textbf{Output layer}: Final classification અથવા prediction બનાવે છે
        \item \textbf{Connections}: Neurons વચ્ચેની weighted links
    \end{itemize}

    \begin{mnemonicbox}
    "Neurons સાથે Weights, Bias, અને Activation"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{3}{a}{3}
\textbf{Backpropagation પર ટૂંક નોંધ લખો.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L}
        \hline
        \textbf{પાસું} & \textbf{વર્ણન} \\
        \hline
        \textbf{હેતુ} & Neural networks માટે training algorithm \\
        \textbf{પદ્ધતિ} & Chain rule સાથે gradient descent \\
        \textbf{દિશા} & પાછળની તરફ error propagation \\
        \hline
    \end{tabulary}

    \begin{itemize}
        \item \textbf{પ્રક્રિયા}: Network દ્વારા પાછળની તરફ error gradients calculate કરવા
        \item \textbf{Update}: Error ઘટાડવા માટે weights adjust કરવા
    \end{itemize}

    \begin{mnemonicbox}
    "પાછળની તરફ Error Propagation"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{3}{b}{4}
\textbf{Single-layer feed forward network સમજાવો.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L}
        \hline
        \textbf{Feature} & \textbf{વર્ણન} \\
        \hline
        \textbf{Structure} & Input layer સીધી output layer સાથે connected \\
        \textbf{Layers} & ફક્ત input અને output layers \\
        \textbf{મર્યાદાઓ} & ફક્ત linearly separable problems solve કરી શકે \\
        \textbf{ઉદાહરણ} & Perceptron \\
        \hline
    \end{tabulary}

    \begin{itemize}
        \item \textbf{ક્ષમતા}: Linear decision boundaries સુધી મર્યાદિત
        \item \textbf{Applications}: સરળ classification tasks
    \end{itemize}

    \begin{mnemonicbox}
    "Single Layer, Linear મર્યાદાઓ"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{3}{c}{7}
\textbf{Recurrent neural network નું architecture દોરો અને સમજાવો.}

\begin{solutionbox}
    \begin{figure}[H]
        \centering
        \begin{tikzpicture}[node distance=2cm, auto]
            \node [gtu block] (input) {Input};
            \node [gtu block, right=of input] (hidden) {Hidden State};
            \node [gtu block, right=of hidden] (output) {Output};
            
            % Feedback loop
            \path [gtu arrow] (input) -- (hidden);
            \path [gtu arrow] (hidden) -- (output);
            
            \draw [gtu arrow] (hidden.south) to [out=270,in=270,looseness=4] node[below]{Recurrent Connection} (hidden.south);

        \end{tikzpicture}
        \caption{Recurrent Neural Network Architecture}
    \end{figure}

    \begin{tabulary}{\linewidth}{L L}
        \hline
        \textbf{Component} & \textbf{Function} \\
        \hline
        \textbf{Hidden State} & પાછલા inputs ની memory રાખે છે \\
        \textbf{Recurrent Connection} & Hidden state થી તે જ તરફ feedback \\
        \textbf{Sequence Processing} & Sequential data handle કરે છે \\
        \hline
    \end{tabulary}

    \begin{itemize}
        \item \textbf{Memory}: પાછલા time steps ની information રાખે છે
        \item \textbf{Applications}: Language modeling, speech recognition
        \item \textbf{ફાયદો}: Variable-length sequences process કરી શકે છે
    \end{itemize}

    \begin{mnemonicbox}
    "Recurrent યાદ રાખે છે, પાછળ Loop કરે છે"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{4}{a}{3}
\textbf{NLP ની વ્યાખ્યા આપો અને તેના advantages લખો.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L}
        \hline
        \textbf{Term} & \textbf{વ્યાખ્યા} \\
        \hline
        \textbf{NLP} & Natural Language Processing - computers ને માનવ ભાષા સમજવા, interpret કરવા અને generate કરવા માટે સક્ષમ બનાવે છે \\
        \hline
    \end{tabulary}

    \textbf{Advantages:}
    \begin{itemize}
        \item \textbf{Human-computer interaction}: કુદરતી communication
        \item \textbf{Automation}: આપોઆપ text processing અને analysis
        \item \textbf{Accessibility}: વિકલાંગ વપરાશકર્તાઓ માટે voice interfaces
    \end{itemize}

    \begin{mnemonicbox}
    "કુદરતી ભાષા, કુદરતી Interaction"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{4}{b}{4}
\textbf{NLU અને NLG ની સરખામણી કરો.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L L}
        \hline
        \textbf{પાસું} & \textbf{NLU (Understanding)} & \textbf{NLG (Generation)} \\
        \hline
        \textbf{હેતુ} & માનવ ભાષા interpret કરવી & માનવ ભાષા generate કરવી \\
        \textbf{Input} & Text/Speech & Structured data \\
        \textbf{Output} & Structured data & Text/Speech \\
        \textbf{ઉદાહરણો} & Sentiment analysis & Text summarization \\
        \hline
    \end{tabulary}

    \begin{itemize}
        \item \textbf{NLU}: Unstructured text ને structured data માં convert કરે છે
        \item \textbf{NLG}: Structured data ને natural text માં convert કરે છે
    \end{itemize}

    \begin{mnemonicbox}
    "NLU સમજે છે, NLG બનાવે છે"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{4}{c}{7}
\textbf{Word tokenization અને frequency distribution of words યોગ્ય ઉદાહરણ સાથે સમજાવો.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L L}
        \hline
        \textbf{પ્રક્રિયા} & \textbf{વર્ણન} & \textbf{ઉદાહરણ} \\
        \hline
        \textbf{Tokenization} & Text ને individual words/tokens માં તોડવું & "Hello world" \textrightarrow ["Hello", "world"] \\
        \textbf{Frequency Distribution} & દરેક token ની occurrence count કરવી & \{"Hello": 1, "world": 1\} \\
        \hline
    \end{tabulary}

    \textbf{ઉદાહરણ:}
    \begin{lstlisting}
Text: "The cat sat on the mat"
Tokens: ["The", "cat", "sat", "on", "the", "mat"]
Frequency: {"The": 1, "cat": 1, "sat": 1, "on": 1, "the": 1, "mat": 1}
    \end{lstlisting}

    \begin{itemize}
        \item \textbf{Case sensitivity}: "The" અને "the" અલગ અલગ count થાય છે
        \item \textbf{Applications}: Text analysis, search engines
        \item \textbf{Preprocessing}: NLP tasks માટે આવશ્યક step
    \end{itemize}

    \begin{mnemonicbox}
    "Tokenize પછી Count"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{4}{a}{3}
\textbf{NLP ના disadvantages ની યાદી આપો.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L}
        \hline
        \textbf{Disadvantage} & \textbf{વર્ણન} \\
        \hline
        \textbf{Ambiguity} & Words/sentences ના multiple meanings \\
        \textbf{Context dependency} & Context સાથે meaning બદલાય છે \\
        \textbf{Language complexity} & Grammar rules અને exceptions \\
        \hline
    \end{tabulary}

    \begin{itemize}
        \item \textbf{સાંસ્કૃતિક variations}: અલગ ભાષાઓ, dialects
        \item \textbf{Computational cost}: Resource-intensive processing
    \end{itemize}

    \begin{mnemonicbox}
    "અસ્પષ્ટ, Contextual, જટિલ"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{4}{b}{4}
\textbf{NLP માં ambiguities ના પ્રકારો સમજાવો.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L L}
        \hline
        \textbf{પ્રકાર} & \textbf{વર્ણન} & \textbf{ઉદાહરણ} \\
        \hline
        \textbf{Lexical} & Word ના multiple meanings & "Bank" (financial/river) \\
        \textbf{Syntactic} & Multiple parse trees possible & "I saw a man with a telescope" \\
        \textbf{Semantic} & Multiple interpretations & "Flying planes can be dangerous" \\
        \hline
    \end{tabulary}

    \begin{itemize}
        \item \textbf{Resolution}: Context analysis, statistical models
        \item \textbf{Challenge}: NLP systems માં મુખ્ય અવરોધ
    \end{itemize}

    \begin{mnemonicbox}
    "Lexical words, Syntactic structure, Semantic meaning"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{4}{c}{7}
\textbf{Stemming words અને parts of speech(POS) tagging યોગ્ય ઉદાહરણ સાથે સમજાવો.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L L}
        \hline
        \textbf{પ્રક્રિયા} & \textbf{વર્ણન} & \textbf{ઉદાહરણ} \\
        \hline
        \textbf{Stemming} & Words ને root/stem form માં ઘટાડવા & "running" \textrightarrow "run", "flies" \textrightarrow "fli" \\
        \textbf{POS Tagging} & Grammatical categories assign કરવા & "The/DT cat/NN runs/VB fast/RB" \\
        \hline
    \end{tabulary}

    \textbf{Stemming ઉદાહરણ:}
    \begin{lstlisting}
Original: ["running", "runs", "runner"]
Stemmed: ["run", "run", "runner"]
    \end{lstlisting}

    \textbf{POS Tagging ઉદાહરણ:}
    \begin{lstlisting}
Sentence: "The quick brown fox jumps"
Tagged: "The/DT quick/JJ brown/JJ fox/NN jumps/VB"
    \end{lstlisting}

    \begin{itemize}
        \item \textbf{Stemming નો હેતુ}: Vocabulary size ઘટાડવું, સંબંધિત words ને group કરવા
        \item \textbf{POS નો હેતુ}: Grammatical structure સમજવું
        \item \textbf{Applications**: Information retrieval, grammar checking
    \end{itemize}

    \begin{mnemonicbox}
    "Root સુધી Stem, Grammar પ્રમાણે Tag"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{5}{a}{3}
\textbf{Word embedding વ્યાખ્યા આપો અને word embedding ની various techniques ની યાદી આપો.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L}
        \hline
        \textbf{Term} & \textbf{વ્યાખ્યા} \\
        \hline
        \textbf{Word Embedding} & Words ના dense vector representations જે semantic relationships capture કરે છે \\
        \hline
    \end{tabulary}

    \textbf{Techniques:}
    \begin{itemize}
        \item \textbf{TF-IDF}: Term Frequency-Inverse Document Frequency
        \item \textbf{Bag of Words (BoW)}: સરળ word occurrence counting
        \item \textbf{Word2Vec}: Neural network-based embeddings
    \end{itemize}

    \begin{mnemonicbox}
    "TF-IDF counts, BoW bags, Word2Vec vectorizes"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{5}{b}{4}
\textbf{TF-IDF and BoW માટે Challenges સમજાવો.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L}
        \hline
        \textbf{પદ્ધતિ} & \textbf{Challenges} \\
        \hline
        \textbf{TF-IDF} & Sparse vectors, કોઈ semantic similarity નથી, high dimensionality \\
        \textbf{BoW} & Order ignore થાય છે, context ખોવાય છે, sparse representation \\
        \hline
    \end{tabulary}

    \textbf{સામાન્ય સમસ્યાઓ:}
    \begin{itemize}
        \item \textbf{Sparsity}: મોટાભાગના vector elements zero છે
        \item \textbf{કોઈ semantics નથી}: સમાન words ના અલગ vectors
        \item \textbf{High dimensions}: Memory અને computation intensive
    \end{itemize}

    \begin{mnemonicbox}
    "Sparse, કોઈ Semantics નથી, High Dimensions"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{5}{c}{7}
\textbf{NLP ની ઉપયોગીતાઓ યોગ્ય ઉદાહરણ સાથે સમજાવો.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L L}
        \hline
        \textbf{Application} & \textbf{વર્ણન} & \textbf{ઉદાહરણ} \\
        \hline
        \textbf{Machine Translation} & ભાષાઓ વચ્ચે translate કરવું & Google Translate \\
        \textbf{Sentiment Analysis} & Emotional tone નક્કી કરવું & Product review analysis \\
        \textbf{Question Answering} & Text માંથી પ્રશ્નોના જવાબ આપવા & Chatbots, virtual assistants \\
        \textbf{Spam Detection} & અનિચ્છિત emails identify કરવા & Email filters \\
        \textbf{Spelling Correction} & Spelling errors ઠીક કરવા & Text editors માં auto-correct \\
        \hline
    \end{tabulary}

    \begin{figure}[H]
        \centering
        \begin{tikzpicture}[node distance=1.5cm, auto]
            \node [gtu block] (nlp) {NLP Applications};
            \node [gtu block, below left=of nlp] (trans) {Translation};
            \node [gtu block, below=of nlp] (sent) {Sentiment};
            \node [gtu block, below right=of nlp] (qa) {QA};
            \node [gtu block, below of=trans] (spam) {Spam};
            \node [gtu block, below of=qa] (spell) {Spelling};

            \path [gtu arrow] (nlp) -- (trans);
            \path [gtu arrow] (nlp) -- (sent);
            \path [gtu arrow] (nlp) -- (qa);
            \path [gtu arrow] (nlp) -- (spam);
            \path [gtu arrow] (nlp) -- (spell);
        \end{tikzpicture}
        \caption{NLP Applications}
    \end{figure}

    \begin{itemize}
        \item \textbf{Real-world impact}: Human-computer interaction સુધારે છે
        \item \textbf{Business value}: Text processing tasks automate કરે છે
        \item \textbf{વધતું ક્ષેત્ર}: નવા applications સતત આવતા રહે છે
    \end{itemize}

    \begin{mnemonicbox}
    "Translate, Sentiment, Question, Spam, Spell"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{5}{a}{3}
\textbf{Glove(Global Vector for word representation) ને વર્ણવો.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L}
        \hline
        \textbf{પાસું} & \textbf{વર્ણન} \\
        \hline
        \textbf{હેતુ} & Global corpus statistics વાપરીને word vectors બનાવવા \\
        \textbf{પદ્ધતિ} & Global matrix factorization અને local context combine કરે છે \\
        \textbf{ફાયદો} & Global અને local બંને statistical information capture કરે છે \\
        \hline
    \end{tabulary}

    \begin{itemize}
        \item \textbf{Global statistics}: Word co-occurrence information વાપરે છે
        \item \textbf{Pre-trained}: સામાન્ય ઉપયોગ માટે trained vectors ઉપલબ્ધ છે
    \end{itemize}

    \begin{mnemonicbox}
    "Global Vectors, Local Context"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{5}{b}{4}
\textbf{Inverse Document Frequency (IDF) સમજાવો.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L L}
        \hline
        \textbf{Component} & \textbf{Formula} & \textbf{હેતુ} \\
        \hline
        \textbf{IDF} & log(N/df) & Documents માં word importance measure કરવું \\
        \textbf{N} & Total documents & Corpus size \\
        \textbf{df} & Document frequency & Term containing documents \\
        \hline
    \end{tabulary}

    \begin{itemize}
        \item \textbf{High IDF}: દુર્લભ words (વધુ informative)
        \item \textbf{Low IDF}: સામાન્ય words (ઓછા informative)
        \item \textbf{Application}: TF-IDF weighting scheme નો ભાગ
    \end{itemize}

    \begin{mnemonicbox}
    "Inverse Document, દુર્લભ મહત્વપૂર્ણ છે"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{5}{c}{7}
\textbf{Document માટે TF(Term Frequency) ગણવાનું યોગ્ય ઉદાહરણ સાથે સમજાવો.}

\begin{solutionbox}
    \textbf{Methods:}
    \begin{itemize}
        \item \textbf{Raw TF}: Simple count f(t,d)
        \item \textbf{Normalized TF}: f(t,d)/max(f(w,d))
        \item \textbf{Log TF}: 1 + log(f(t,d))
    \end{itemize}

    \textbf{Example Document:} ``The cat sat on the mat. The mat was soft.''

    \textbf{Steps:}
    \begin{enumerate}
        \item દરેક term ની occurrence count કરો
        \item પસંદ કરેલું TF formula લાગુ કરો
        \item TF-IDF calculation માં વાપરો
    \end{enumerate}

    \begin{mnemonicbox}
    "Count, Normalize, Log"
\end{mnemonicbox}
\end{solutionbox}


\questionmarks{3}{b}{4}
\textbf{Multi-layer feed forward ANN સમજાવો.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L}
        \hline
        \textbf{Component} & \textbf{વર્ણન} \\
        \hline
        \textbf{Input Layer} & Input data receive કરે છે \\
        \textbf{Hidden Layers} & Information process કરે છે (multiple layers) \\
        \textbf{Output Layer} & Final result બનાવે છે \\
        \textbf{Connections} & ફક્ત forward direction માં \\
        \hline
    \end{tabulary}

    \begin{itemize}
        \item \textbf{Information flow}: Input થી output સુધી એક દિશામાં
        \item \textbf{કોઈ cycles નથી}: કોઈ feedback connections નથી
    \end{itemize}

    \begin{mnemonicbox}
    "Input \textrightarrow Hidden \textrightarrow Output (ફક્ત આગળ)"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{3}{c}{7}
\textbf{ANN નું structure દોરો અને તેના દરેક components ની functionality સમજાવો.}

\begin{solutionbox}
    \begin{figure}[H]
        \centering
        \begin{tikzpicture}[
            neuron/.style={circle, fill=blue!10, draw=blue, thick, minimum size=0.8cm},
            bias/.style={circle, fill=red!10, draw=red, thick, minimum size=0.6cm},
            layer/.style={matrix, row sep=0.5cm, column sep=0.5cm, nodes={neuron}}
        ]
            % Input Layer
            \node[layer] (input) {
                $x_1$ \\
                $x_2$ \\
                $x_3$ \\
            };
            
            % Hidden Layer 1
            \node[layer, right=2cm of input] (hidden) {
                $h_1$ \\
                $h_2$ \\
                $h_3$ \\
                $h_4$ \\
            };
            
            % Output Layer
            \node[layer, right=2cm of hidden] (output) {
                $y_1$ \\
                $y_2$ \\
            };
            
            % Connections
            \foreach \i in {1,2,3}
                \foreach \j in {1,2,3,4}
                    \draw[->] (input-\i-1) -- (hidden-\j-1);
            
            \foreach \i in {1,2,3,4}
                \foreach \j in {1,2}
                    \draw[->] (hidden-\i-1) -- (output-\j-1);
            
            % Labels
            \node[above=0.2cm of input] {Input Layer};
            \node[above=0.2cm of hidden] {Hidden Layer};
            \node[above=0.2cm of output] {Output Layer};

        \end{tikzpicture}
        \caption{Artificial Neural Network Structure}
    \end{figure}

    \begin{tabulary}{\linewidth}{L L}
        \hline
        \textbf{Component} & \textbf{Functionality} \\
        \hline
        \textbf{Neurons} & Processing units જે inputs receive કરે છે અને outputs બનાવે છે \\
        \textbf{Weights} & Neurons વચ્ચેની connection strengths \\
        \textbf{Bias} & Activation function ને shift કરવા માટે વધારાનું parameter \\
        \textbf{Activation Function} & Network માં non-linearity લાવે છે \\
        \hline
    \end{tabulary}

    \begin{itemize}
        \item \textbf{Input layer}: Input data receive કરે છે અને વિતરિત કરે છે
        \item \textbf{Hidden layers}: Features અને patterns extract કરે છે
        \item \textbf{Output layer}: Final classification અથવા prediction બનાવે છે
        \item \textbf{Connections}: Neurons વચ્ચેની weighted links
    \end{itemize}

    \begin{mnemonicbox}
    "Neurons સાથે Weights, Bias, અને Activation"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{3}{a}{3}
\textbf{Backpropagation પર ટૂંક નોંધ લખો.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L}
        \hline
        \textbf{પાસું} & \textbf{વર્ણન} \\
        \hline
        \textbf{હેતુ} & Neural networks માટે training algorithm \\
        \textbf{પદ્ધતિ} & Chain rule સાથે gradient descent \\
        \textbf{દિશા} & પાછળની તરફ error propagation \\
        \hline
    \end{tabulary}

    \begin{itemize}
        \item \textbf{પ્રક્રિયા}: Network દ્વારા પાછળની તરફ error gradients calculate કરવા
        \item \textbf{Update}: Error ઘટાડવા માટે weights adjust કરવા
    \end{itemize}

    \begin{mnemonicbox}
    "પાછળની તરફ Error Propagation"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{3}{b}{4}
\textbf{Single-layer feed forward network સમજાવો.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L}
        \hline
        \textbf{Feature} & \textbf{વર્ણન} \\
        \hline
        \textbf{Structure} & Input layer સીધી output layer સાથે connected \\
        \textbf{Layers} & ફક્ત input અને output layers \\
        \textbf{મર્યાદાઓ} & ફક્ત linearly separable problems solve કરી શકે \\
        \textbf{ઉદાહરણ} & Perceptron \\
        \hline
    \end{tabulary}

    \begin{itemize}
        \item \textbf{ક્ષમતા}: Linear decision boundaries સુધી મર્યાદિત
        \item \textbf{Applications}: સરળ classification tasks
    \end{itemize}

    \begin{mnemonicbox}
    "Single Layer, Linear મર્યાદાઓ"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{3}{c}{7}
\textbf{Recurrent neural network નું architecture દોરો અને સમજાવો.}

\begin{solutionbox}
    \begin{figure}[H]
        \centering
        \begin{tikzpicture}[node distance=2cm, auto]
            \node [gtu block] (input) {Input};
            \node [gtu block, right=of input] (hidden) {Hidden State};
            \node [gtu block, right=of hidden] (output) {Output};
            
            % Feedback loop
            \path [gtu arrow] (input) -- (hidden);
            \path [gtu arrow] (hidden) -- (output);
            
            \draw [gtu arrow] (hidden.south) to [out=270,in=270,looseness=4] node[below]{Recurrent Connection} (hidden.south);

        \end{tikzpicture}
        \caption{Recurrent Neural Network Architecture}
    \end{figure}

    \begin{tabulary}{\linewidth}{L L}
        \hline
        \textbf{Component} & \textbf{Function} \\
        \hline
        \textbf{Hidden State} & પાછલા inputs ની memory રાખે છે \\
        \textbf{Recurrent Connection} & Hidden state થી તે જ તરફ feedback \\
        \textbf{Sequence Processing} & Sequential data handle કરે છે \\
        \hline
    \end{tabulary}

    \begin{itemize}
        \item \textbf{Memory}: પાછલા time steps ની information રાખે છે
        \item \textbf{Applications}: Language modeling, speech recognition
        \item \textbf{ફાયદો}: Variable-length sequences process કરી શકે છે
    \end{itemize}

    \begin{mnemonicbox}
    "Recurrent યાદ રાખે છે, પાછળ Loop કરે છે"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{4}{a}{3}
\textbf{NLP ની વ્યાખ્યા આપો અને તેના advantages લખો.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L}
        \hline
        \textbf{Term} & \textbf{વ્યાખ્યા} \\
        \hline
        \textbf{NLP} & Natural Language Processing - computers ને માનવ ભાષા સમજવા, interpret કરવા અને generate કરવા માટે સક્ષમ બનાવે છે \\
        \hline
    \end{tabulary}

    \textbf{Advantages:}
    \begin{itemize}
        \item \textbf{Human-computer interaction}: કુદરતી communication
        \item \textbf{Automation}: આપોઆપ text processing અને analysis
        \item \textbf{Accessibility}: વિકલાંગ વપરાશકર્તાઓ માટે voice interfaces
    \end{itemize}

    \begin{mnemonicbox}
    "કુદરતી ભાષા, કુદરતી Interaction"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{4}{b}{4}
\textbf{NLU અને NLG ની સરખામણી કરો.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L L}
        \hline
        \textbf{પાસું} & \textbf{NLU (Understanding)} & \textbf{NLG (Generation)} \\
        \hline
        \textbf{હેતુ} & માનવ ભાષા interpret કરવી & માનવ ભાષા generate કરવી \\
        \textbf{Input} & Text/Speech & Structured data \\
        \textbf{Output} & Structured data & Text/Speech \\
        \textbf{ઉદાહરણો} & Sentiment analysis & Text summarization \\
        \hline
    \end{tabulary}

    \begin{itemize}
        \item \textbf{NLU}: Unstructured text ને structured data માં convert કરે છે
        \item \textbf{NLG}: Structured data ને natural text માં convert કરે છે
    \end{itemize}

    \begin{mnemonicbox}
    "NLU સમજે છે, NLG બનાવે છે"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{4}{c}{7}
\textbf{Word tokenization અને frequency distribution of words યોગ્ય ઉદાહરણ સાથે સમજાવો.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L L}
        \hline
        \textbf{પ્રક્રિયા} & \textbf{વર્ણન} & \textbf{ઉદાહરણ} \\
        \hline
        \textbf{Tokenization} & Text ને individual words/tokens માં તોડવું & "Hello world" \textrightarrow ["Hello", "world"] \\
        \textbf{Frequency Distribution} & દરેક token ની occurrence count કરવી & \{"Hello": 1, "world": 1\} \\
        \hline
    \end{tabulary}

    \textbf{ઉદાહરણ:}
    \begin{lstlisting}
Text: "The cat sat on the mat"
Tokens: ["The", "cat", "sat", "on", "the", "mat"]
Frequency: {"The": 1, "cat": 1, "sat": 1, "on": 1, "the": 1, "mat": 1}
    \end{lstlisting}

    \begin{itemize}
        \item \textbf{Case sensitivity}: "The" અને "the" અલગ અલગ count થાય છે
        \item \textbf{Applications}: Text analysis, search engines
        \item \textbf{Preprocessing}: NLP tasks માટે આવશ્યક step
    \end{itemize}

    \begin{mnemonicbox}
    "Tokenize પછી Count"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{4}{a}{3}
\textbf{NLP ના disadvantages ની યાદી આપો.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L}
        \hline
        \textbf{Disadvantage} & \textbf{વર્ણન} \\
        \hline
        \textbf{Ambiguity} & Words/sentences ના multiple meanings \\
        \textbf{Context dependency} & Context સાથે meaning બદલાય છે \\
        \textbf{Language complexity} & Grammar rules અને exceptions \\
        \hline
    \end{tabulary}

    \begin{itemize}
        \item \textbf{સાંસ્કૃતિક variations}: અલગ ભાષાઓ, dialects
        \item \textbf{Computational cost}: Resource-intensive processing
    \end{itemize}

    \begin{mnemonicbox}
    "અસ્પષ્ટ, Contextual, જટિલ"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{4}{b}{4}
\textbf{NLP માં ambiguities ના પ્રકારો સમજાવો.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L L}
        \hline
        \textbf{પ્રકાર} & \textbf{વર્ણન} & \textbf{ઉદાહરણ} \\
        \hline
        \textbf{Lexical} & Word ના multiple meanings & "Bank" (financial/river) \\
        \textbf{Syntactic} & Multiple parse trees possible & "I saw a man with a telescope" \\
        \textbf{Semantic} & Multiple interpretations & "Flying planes can be dangerous" \\
        \hline
    \end{tabulary}

    \begin{itemize}
        \item \textbf{Resolution}: Context analysis, statistical models
        \item \textbf{Challenge}: NLP systems માં મુખ્ય અવરોધ
    \end{itemize}

    \begin{mnemonicbox}
    "Lexical words, Syntactic structure, Semantic meaning"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{4}{c}{7}
\textbf{Stemming words અને parts of speech(POS) tagging યોગ્ય ઉદાહરણ સાથે સમજાવો.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L L}
        \hline
        \textbf{પ્રક્રિયા} & \textbf{વર્ણન} & \textbf{ઉદાહરણ} \\
        \hline
        \textbf{Stemming} & Words ને root/stem form માં ઘટાડવા & "running" \textrightarrow "run", "flies" \textrightarrow "fli" \\
        \textbf{POS Tagging} & Grammatical categories assign કરવા & "The/DT cat/NN runs/VB fast/RB" \\
        \hline
    \end{tabulary}

    \textbf{Stemming ઉદાહરણ:}
    \begin{lstlisting}
Original: ["running", "runs", "runner"]
Stemmed: ["run", "run", "runner"]
    \end{lstlisting}

    \textbf{POS Tagging ઉદાહરણ:}
    \begin{lstlisting}
Sentence: "The quick brown fox jumps"
Tagged: "The/DT quick/JJ brown/JJ fox/NN jumps/VB"
    \end{lstlisting}

    \begin{itemize}
        \item \textbf{Stemming નો હેતુ}: Vocabulary size ઘટાડવું, સંબંધિત words ને group કરવા
        \item \textbf{POS નો હેતુ}: Grammatical structure સમજવું
        \item \textbf{Applications**: Information retrieval, grammar checking
    \end{itemize}

    \begin{mnemonicbox}
    "Root સુધી Stem, Grammar પ્રમાણે Tag"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{5}{a}{3}
\textbf{Word embedding વ્યાખ્યા આપો અને word embedding ની various techniques ની યાદી આપો.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L}
        \hline
        \textbf{Term} & \textbf{વ્યાખ્યા} \\
        \hline
        \textbf{Word Embedding} & Words ના dense vector representations જે semantic relationships capture કરે છે \\
        \hline
    \end{tabulary}

    \textbf{Techniques:}
    \begin{itemize}
        \item \textbf{TF-IDF}: Term Frequency-Inverse Document Frequency
        \item \textbf{Bag of Words (BoW)}: સરળ word occurrence counting
        \item \textbf{Word2Vec}: Neural network-based embeddings
    \end{itemize}

    \begin{mnemonicbox}
    "TF-IDF counts, BoW bags, Word2Vec vectorizes"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{5}{b}{4}
\textbf{TF-IDF and BoW માટે Challenges સમજાવો.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L}
        \hline
        \textbf{પદ્ધતિ} & \textbf{Challenges} \\
        \hline
        \textbf{TF-IDF} & Sparse vectors, કોઈ semantic similarity નથી, high dimensionality \\
        \textbf{BoW} & Order ignore થાય છે, context ખોવાય છે, sparse representation \\
        \hline
    \end{tabulary}

    \textbf{સામાન્ય સમસ્યાઓ:}
    \begin{itemize}
        \item \textbf{Sparsity}: મોટાભાગના vector elements zero છે
        \item \textbf{કોઈ semantics નથી}: સમાન words ના અલગ vectors
        \item \textbf{High dimensions}: Memory અને computation intensive
    \end{itemize}

    \begin{mnemonicbox}
    "Sparse, કોઈ Semantics નથી, High Dimensions"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{5}{c}{7}
\textbf{NLP ની ઉપયોગીતાઓ યોગ્ય ઉદાહરણ સાથે સમજાવો.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L L}
        \hline
        \textbf{Application} & \textbf{વર્ણન} & \textbf{ઉદાહરણ} \\
        \hline
        \textbf{Machine Translation} & ભાષાઓ વચ્ચે translate કરવું & Google Translate \\
        \textbf{Sentiment Analysis} & Emotional tone નક્કી કરવું & Product review analysis \\
        \textbf{Question Answering} & Text માંથી પ્રશ્નોના જવાબ આપવા & Chatbots, virtual assistants \\
        \textbf{Spam Detection} & અનિચ્છિત emails identify કરવા & Email filters \\
        \textbf{Spelling Correction} & Spelling errors ઠીક કરવા & Text editors માં auto-correct \\
        \hline
    \end{tabulary}

    \begin{figure}[H]
        \centering
        \begin{tikzpicture}[node distance=1.5cm, auto]
            \node [gtu block] (nlp) {NLP Applications};
            \node [gtu block, below left=of nlp] (trans) {Translation};
            \node [gtu block, below=of nlp] (sent) {Sentiment};
            \node [gtu block, below right=of nlp] (qa) {QA};
            \node [gtu block, below of=trans] (spam) {Spam};
            \node [gtu block, below of=qa] (spell) {Spelling};

            \path [gtu arrow] (nlp) -- (trans);
            \path [gtu arrow] (nlp) -- (sent);
            \path [gtu arrow] (nlp) -- (qa);
            \path [gtu arrow] (nlp) -- (spam);
            \path [gtu arrow] (nlp) -- (spell);
        \end{tikzpicture}
        \caption{NLP Applications}
    \end{figure}

    \begin{itemize}
        \item \textbf{Real-world impact}: Human-computer interaction સુધારે છે
        \item \textbf{Business value}: Text processing tasks automate કરે છે
        \item \textbf{વધતું ક્ષેત્ર}: નવા applications સતત આવતા રહે છે
    \end{itemize}

    \begin{mnemonicbox}
    "Translate, Sentiment, Question, Spam, Spell"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{5}{a}{3}
\textbf{Glove(Global Vector for word representation) ને વર્ણવો.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L}
        \hline
        \textbf{પાસું} & \textbf{વર્ણન} \\
        \hline
        \textbf{હેતુ} & Global corpus statistics વાપરીને word vectors બનાવવા \\
        \textbf{પદ્ધતિ} & Global matrix factorization અને local context combine કરે છે \\
        \textbf{ફાયદો} & Global અને local બંને statistical information capture કરે છે \\
        \hline
    \end{tabulary}

    \begin{itemize}
        \item \textbf{Global statistics}: Word co-occurrence information વાપરે છે
        \item \textbf{Pre-trained}: સામાન્ય ઉપયોગ માટે trained vectors ઉપલબ્ધ છે
    \end{itemize}

    \begin{mnemonicbox}
    "Global Vectors, Local Context"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{5}{b}{4}
\textbf{Inverse Document Frequency (IDF) સમજાવો.}

\begin{solutionbox}
    \begin{tabulary}{\linewidth}{L L L}
        \hline
        \textbf{Component} & \textbf{Formula} & \textbf{હેતુ} \\
        \hline
        \textbf{IDF} & log(N/df) & Documents માં word importance measure કરવું \\
        \textbf{N} & Total documents & Corpus size \\
        \textbf{df} & Document frequency & Term containing documents \\
        \hline
    \end{tabulary}

    \begin{itemize}
        \item \textbf{High IDF}: દુર્લભ words (વધુ informative)
        \item \textbf{Low IDF}: સામાન્ય words (ઓછા informative)
        \item \textbf{Application}: TF-IDF weighting scheme નો ભાગ
    \end{itemize}

    \begin{mnemonicbox}
    "Inverse Document, દુર્લભ મહત્વપૂર્ણ છે"
\end{mnemonicbox}
\end{solutionbox}

\questionmarks{5}{c}{7}
\textbf{Document માટે TF(Term Frequency) ગણવાનું યોગ્ય ઉદાહરણ સાથે સમજાવો.}

\begin{solutionbox}
    \textbf{Methods:}
    \begin{itemize}
        \item \textbf{Raw TF}: Simple count f(t,d)
        \item \textbf{Normalized TF}: f(t,d)/max(f(w,d))
        \item \textbf{Log TF}: 1 + log(f(t,d))
    \end{itemize}

    \textbf{Example Document:} ``The cat sat on the mat. The mat was soft.''

    \textbf{Steps:}
    \begin{enumerate}
        \item દરેક term ની occurrence count કરો
        \item પસંદ કરેલું TF formula લાગુ કરો
        \item TF-IDF calculation માં વાપરો
    \end{enumerate}

    \begin{mnemonicbox}
    "Count, Normalize, Log"
\end{mnemonicbox}
\end{solutionbox}

\end{document}


