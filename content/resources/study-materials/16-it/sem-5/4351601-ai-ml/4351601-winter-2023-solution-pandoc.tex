\documentclass[10pt,a4paper]{article}
\input{../../../../../../latex-templates/gtu-solutions/preamble.tex}
\input{../../../../../../latex-templates/gtu-solutions/english-boxes.tex}

\begin{document}

\begin{center}
{\Huge\bfseries\color{headcolor} Subject Name Solutions}\\[5pt]
{\LARGE 4351601 -- Winter 2023}\\[3pt]
{\large Semester 1 Study Material}\\[3pt]
{\normalsize\textit{Detailed Solutions and Explanations}}
\end{center}

\vspace{10pt}

\subsection*{Question 1(a) [3 marks]}\label{q1a}

\textbf{Define the following terms: (1) Artificial Intelligence (2)
Expert System.}

\begin{solutionbox}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.6667}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Term
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Artificial Intelligence} & AI is a branch of computer science
that creates machines capable of performing tasks that typically require
human intelligence, such as learning, reasoning, and problem-solving. \\
\textbf{Expert System} & An expert system is a computer program that
uses knowledge and inference rules to solve problems that normally
require human expertise in a specific domain. \\
\end{longtable}
}

\begin{itemize}
\tightlist
\item
  \textbf{AI characteristics}: Learning, reasoning, perception
\item
  \textbf{Expert system components}: Knowledge base, inference engine
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``AI Learns, Expert Advises''

\end{mnemonicbox}
\subsection*{Question 1(b) [4 marks]}\label{q1b}

\textbf{Compare Biological Neural Network and Artificial Neural
Network.}

\begin{solutionbox}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1290}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4355}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4355}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Biological Neural Network
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Artificial Neural Network
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Processing} & Parallel processing & Sequential/parallel
processing \\
\textbf{Speed} & Slow (milliseconds) & Fast (nanoseconds) \\
\textbf{Learning} & Continuous learning & Batch/online learning \\
\textbf{Storage} & Distributed storage & Centralized storage \\
\end{longtable}
}

\begin{itemize}
\tightlist
\item
  \textbf{Biological}: Complex, fault-tolerant, self-repairing
\item
  \textbf{Artificial}: Simple, precise, programmable
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``Bio is Complex, AI is Simple''

\end{mnemonicbox}
\subsection*{Question 1(c) [7 marks]}\label{q1c}

\textbf{Explain types of AI with its applications.}

\begin{solutionbox}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3077}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3590}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type of AI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Applications
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Narrow AI} & AI designed for specific tasks & Voice assistants,
recommendation systems \\
\textbf{General AI} & AI with human-level intelligence & Not yet
achieved \\
\textbf{Super AI} & AI exceeding human intelligence & Theoretical
concept \\
\end{longtable}
}

\begin{center}
\textbf{Mermaid Diagram (Code)}
\begin{verbatim}
{Shaded}
{Highlighting}[]
graph TD
    A[Types of AI] {-{-}{} B[Narrow AI]}
    A {-{-}{} C[General AI]}
    A {-{-}{} D[Super AI]}
    B {-{-}{} E[Siri, Alexa]}
    B {-{-}{} F[Netflix Recommendations]}
    C {-{-}{} G[Human{-}level Tasks]}
    D {-{-}{} H[Beyond Human Intelligence]}
{Highlighting}
{Shaded}
\end{verbatim}
\end{center}

\begin{itemize}
\tightlist
\item
  \textbf{Current focus}: Narrow AI dominates today's applications
\item
  \textbf{Future goal}: Achieving General AI safely
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``Narrow Now, General Goal, Super Scary''

\end{mnemonicbox}
\subsection*{Question 1(c) OR [7
marks]}\label{q1c}

\textbf{Explain AI ethics and limitations.}

\begin{solutionbox}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Ethics Aspect & Description \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Privacy} & Protecting personal data and user information \\
\textbf{Bias} & Ensuring fairness across different groups \\
\textbf{Transparency} & Making AI decisions explainable \\
\textbf{Accountability} & Determining responsibility for AI actions \\
\end{longtable}
}

\textbf{Limitations:}

\begin{itemize}
\tightlist
\item
  \textbf{Data dependency}: Requires large, quality datasets
\item
  \textbf{Computational power}: Needs significant processing resources
\item
  \textbf{Lack of creativity}: Cannot truly create original concepts
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``Privacy, Bias, Transparency, Accountability''

\end{mnemonicbox}
\subsection*{Question 2(a) [3 marks]}\label{q2a}

\textbf{Define the following terms: (1) Well posed Learning Problem (2)
Machine Learning.}

\begin{solutionbox}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.6667}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Term
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Well posed Learning Problem} & A learning problem with clearly
defined task (T), performance measure (P), and experience (E) where
performance improves with experience. \\
\textbf{Machine Learning} & A subset of AI that enables computers to
learn and improve automatically from experience without being explicitly
programmed. \\
\end{longtable}
}

\begin{itemize}
\tightlist
\item
  \textbf{Well posed formula}: T + P + E = Learning
\item
  \textbf{ML advantage}: Automatic improvement from data
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``Task, Performance, Experience''

\end{mnemonicbox}
\subsection*{Question 2(b) [4 marks]}\label{q2b}

\textbf{Explain Reinforcement Learning along with terms used in it.}

\begin{solutionbox}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Term & Description \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Agent} & The learner or decision maker \\
\textbf{Environment} & The world in which agent operates \\
\textbf{Action} & What agent can do in each state \\
\textbf{State} & Current situation of the agent \\
\textbf{Reward} & Feedback from environment \\
\end{longtable}
}

\begin{center}
\textbf{Mermaid Diagram (Code)}
\begin{verbatim}
{Shaded}
{Highlighting}[]
graph LR
    A[Agent] {-{-}{} B[Action]}
    B {-{-}{} C[Environment]}
    C {-{-}{} D[State]}
    C {-{-}{} E[Reward]}
    D {-{-}{} A}
    E {-{-}{} A}
{Highlighting}
{Shaded}
\end{verbatim}
\end{center}

\begin{itemize}
\tightlist
\item
  \textbf{Learning process}: Trial and error approach
\item
  \textbf{Goal}: Maximize cumulative reward
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``Agent Acts, Environment States and Rewards''

\end{mnemonicbox}
\subsection*{Question 2(c) [7 marks]}\label{q2c}

\textbf{Compare Supervised, Unsupervised and Reinforcement Learning.}

\begin{solutionbox}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Aspect & Supervised & Unsupervised & Reinforcement \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Data} & Labeled data & Unlabeled data & Interactive data \\
\textbf{Goal} & Predict output & Find patterns & Maximize reward \\
\textbf{Feedback} & Immediate & None & Delayed \\
\textbf{Examples} & Classification & Clustering & Game playing \\
\end{longtable}
}

\begin{itemize}
\tightlist
\item
  \textbf{Supervised}: Teacher-guided learning
\item
  \textbf{Unsupervised}: Self-discovery learning\\
\item
  \textbf{Reinforcement}: Trial-and-error learning
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``Supervised has Teacher, Unsupervised Discovers,
Reinforcement Tries''

\end{mnemonicbox}
\subsection*{Question 2(a) OR [3
marks]}\label{q2a}

\textbf{Write Key features of Reinforcement Learning.}

\begin{solutionbox}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Feature & Description \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Trial and Error} & Learning through experimentation \\
\textbf{Delayed Reward} & Feedback comes after actions \\
\textbf{Sequential Decision} & Actions affect future states \\
\end{longtable}
}

\begin{itemize}
\tightlist
\item
  \textbf{No supervisor}: Agent learns independently
\item
  \textbf{Exploration vs Exploitation}: Balance between trying new
  actions and using known good actions
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``Try, Delay, Sequence''

\end{mnemonicbox}
\subsection*{Question 2(b) OR [4
marks]}\label{q2b}

\textbf{Explain Types of Reinforcement learning.}

\begin{solutionbox}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Type & Description \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Positive RL} & Adding positive stimulus to increase behavior \\
\textbf{Negative RL} & Removing negative stimulus to increase
behavior \\
\end{longtable}
}

\textbf{Based on Learning:}

\begin{itemize}
\tightlist
\item
  \textbf{Model-based}: Agent learns environment model
\item
  \textbf{Model-free}: Agent learns directly from experience
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``Positive Adds, Negative Removes''

\end{mnemonicbox}
\subsection*{Question 2(c) OR [7
marks]}\label{q2c}

\textbf{Explain approaches to implement Reinforcement Learning.}

\begin{solutionbox}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Approach & Description & Example \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Value-based} & Learn value of states/actions & Q-Learning \\
\textbf{Policy-based} & Learn policy directly & Policy Gradient \\
\textbf{Model-based} & Learn environment model & Dynamic Programming \\
\end{longtable}
}

\begin{center}
\textbf{Mermaid Diagram (Code)}
\begin{verbatim}
{Shaded}
{Highlighting}[]
graph TD
    A[RL Approaches] {-{-}{} B[Value{-}based]}
    A {-{-}{} C[Policy{-}based]}
    A {-{-}{} D[Model{-}based]}
    B {-{-}{} E[Q{-}Learning]}
    C {-{-}{} F[Policy Gradient]}
    D {-{-}{} G[Dynamic Programming]}
{Highlighting}
{Shaded}
\end{verbatim}
\end{center}

\begin{itemize}
\tightlist
\item
  \textbf{Value-based}: Estimates value functions
\item
  \textbf{Policy-based}: Optimizes policy parameters
\item
  \textbf{Model-based}: Uses environment model
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``Value, Policy, Model''

\end{mnemonicbox}
\subsection*{Question 3(a) [3 marks]}\label{q3a}

\textbf{Describe the activation functions ReLU and sigmoid.}

\begin{solutionbox}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Function & Formula & Range \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{ReLU} & f(x) = max(0, x) & [0, \infty) \\
\textbf{Sigmoid} & f(x) = 1/(1 + e\^{}(-x)) & (0, 1) \\
\end{longtable}
}

\begin{itemize}
\tightlist
\item
  \textbf{ReLU advantage}: No vanishing gradient problem
\item
  \textbf{Sigmoid advantage}: Smooth gradient, probabilistic output
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``ReLU Rectifies, Sigmoid Squashes''

\end{mnemonicbox}
\subsection*{Question 3(b) [4 marks]}\label{q3b}

\textbf{Explain Multi-layer feed forward ANN.}

\begin{solutionbox}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Component & Description \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Input Layer} & Receives input data \\
\textbf{Hidden Layers} & Process information (multiple layers) \\
\textbf{Output Layer} & Produces final result \\
\textbf{Connections} & Forward direction only \\
\end{longtable}
}

\begin{itemize}
\tightlist
\item
  \textbf{Information flow}: Unidirectional from input to output
\item
  \textbf{No cycles}: No feedback connections
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``Input \rightarrow Hidden \rightarrow Output (Forward Only)''

\end{mnemonicbox}
\subsection*{Question 3(c) [7 marks]}\label{q3c}

\textbf{Draw the structure of ANN and explain functionality of each of
its components.}

\begin{solutionbox}

\begin{center}
\textbf{Mermaid Diagram (Code)}
\begin{verbatim}
{Shaded}
{Highlighting}[]
graph LR
    A[Input Layer] {-{-}{} B[Hidden Layer 1]}
    B {-{-}{} C[Hidden Layer 2]}
    C {-{-}{} D[Output Layer]}
    
    subgraph "Components"
        E[Neurons]
        F[Weights]
        G[Bias]
        H[Activation Function]
    end
{Highlighting}
{Shaded}
\end{verbatim}
\end{center}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.4231}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.5769}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Functionality
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Neurons} & Processing units that receive inputs and produce
outputs \\
\textbf{Weights} & Connection strengths between neurons \\
\textbf{Bias} & Additional parameter to shift activation function \\
\textbf{Activation Function} & Introduces non-linearity to the
network \\
\end{longtable}
}

\begin{itemize}
\tightlist
\item
  \textbf{Input layer}: Receives and distributes input data
\item
  \textbf{Hidden layers}: Extract features and patterns
\item
  \textbf{Output layer}: Produces final classification or prediction
\item
  \textbf{Connections}: Weighted links between neurons
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``Neurons with Weights, Bias, and Activation''

\end{mnemonicbox}
\subsection*{Question 3(a) OR [3
marks]}\label{q3a}

\textbf{Write a short note on Backpropagation.}

\begin{solutionbox}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Aspect & Description \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Purpose} & Training algorithm for neural networks \\
\textbf{Method} & Gradient descent with chain rule \\
\textbf{Direction} & Backward error propagation \\
\end{longtable}
}

\begin{itemize}
\tightlist
\item
  \textbf{Process}: Calculate error gradients backwards through network
\item
  \textbf{Update}: Adjust weights to minimize error
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``Back-ward Error Propagation''

\end{mnemonicbox}
\subsection*{Question 3(b) OR [4
marks]}\label{q3b}

\textbf{Explain Single-layer feed forward network.}

\begin{solutionbox}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Feature & Description \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Structure} & Input layer directly connected to output layer \\
\textbf{Layers} & Only input and output layers \\
\textbf{Limitations} & Can only solve linearly separable problems \\
\textbf{Example} & Perceptron \\
\end{longtable}
}

\begin{itemize}
\tightlist
\item
  \textbf{Capability}: Limited to linear decision boundaries
\item
  \textbf{Applications}: Simple classification tasks
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``Single Layer, Linear Limits''

\end{mnemonicbox}
\subsection*{Question 3(c) OR [7
marks]}\label{q3c}

\textbf{Draw and explain the architecture of Recurrent neural network.}

\begin{solutionbox}

\begin{center}
\textbf{Mermaid Diagram (Code)}
\begin{verbatim}
{Shaded}
{Highlighting}[]
graph LR
    A[Input] {-{-}{} B[Hidden State]}
    B {-{-}{} C[Output]}
    B {-{-}{} B[Self{-}loop]}
    D[Previous State] {-{-}{} B}
{Highlighting}
{Shaded}
\end{verbatim}
\end{center}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Component & Function \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Hidden State} & Maintains memory of previous inputs \\
\textbf{Recurrent Connection} & Feedback from hidden state to itself \\
\textbf{Sequence Processing} & Handles sequential data \\
\end{longtable}
}

\begin{itemize}
\tightlist
\item
  \textbf{Memory}: Retains information from previous time steps
\item
  \textbf{Applications}: Language modeling, speech recognition
\item
  \textbf{Advantage}: Can process variable-length sequences
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``Recurrent Remembers, Loops Back''

\end{mnemonicbox}
\subsection*{Question 4(a) [3 marks]}\label{q4a}

\textbf{Define NLP and write down advantages of it.}

\begin{solutionbox}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.6667}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Term
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{NLP} & Natural Language Processing - enables computers to
understand, interpret, and generate human language \\
\end{longtable}
}

\textbf{Advantages:}

\begin{itemize}
\tightlist
\item
  \textbf{Human-computer interaction}: Natural communication
\item
  \textbf{Automation}: Automated text processing and analysis
\item
  \textbf{Accessibility}: Voice interfaces for disabled users
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``Natural Language, Natural Interaction''

\end{mnemonicbox}
\subsection*{Question 4(b) [4 marks]}\label{q4b}

\textbf{Compare NLU and NLG.}

\begin{solutionbox}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Aspect & NLU (Understanding) & NLG (Generation) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Purpose} & Interpret human language & Generate human language \\
\textbf{Input} & Text/Speech & Structured data \\
\textbf{Output} & Structured data & Text/Speech \\
\textbf{Examples} & Sentiment analysis & Text summarization \\
\end{longtable}
}

\begin{itemize}
\tightlist
\item
  \textbf{NLU}: Converts unstructured text to structured data
\item
  \textbf{NLG}: Converts structured data to natural text
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``NLU Understands, NLG Generates''

\end{mnemonicbox}
\subsection*{Question 4(c) [7 marks]}\label{q4c}

\textbf{Explain word tokenization and frequency distribution of words
with suitable example.}

\begin{solutionbox}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2903}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4194}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2903}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Process
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Tokenization} & Breaking text into individual words/tokens &
``Hello world'' \rightarrow [``Hello'', ``world''] \\
\textbf{Frequency Distribution} & Counting occurrence of each token &
\{``Hello'': 1, ``world'': 1\} \\
\end{longtable}
}

\textbf{Example:}

\begin{verbatim}
Text: "The cat sat on the mat"
Tokens: ["The", "cat", "sat", "on", "the", "mat"]
Frequency: {"The": 1, "cat": 1, "sat": 1, "on": 1, "the": 1, "mat": 1}
\end{verbatim}

\begin{itemize}
\tightlist
\item
  \textbf{Case sensitivity}: ``The'' and ``the'' counted separately
\item
  \textbf{Applications}: Text analysis, search engines
\item
  \textbf{Preprocessing}: Essential step for NLP tasks
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``Tokenize then Count''

\end{mnemonicbox}
\subsection*{Question 4(a) OR [3
marks]}\label{q4a}

\textbf{List disadvantages of NLP.}

\begin{solutionbox}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Disadvantage & Description \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Ambiguity} & Multiple meanings of words/sentences \\
\textbf{Context dependency} & Meaning changes with context \\
\textbf{Language complexity} & Grammar rules and exceptions \\
\end{longtable}
}

\begin{itemize}
\tightlist
\item
  \textbf{Cultural variations}: Different languages, dialects
\item
  \textbf{Computational cost}: Resource-intensive processing
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``Ambiguous, Contextual, Complex''

\end{mnemonicbox}
\subsection*{Question 4(b) OR [4
marks]}\label{q4b}

\textbf{Explain types of ambiguities in NLP.}

\begin{solutionbox}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2143}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4643}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3214}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Lexical} & Word has multiple meanings & ``Bank''
(financial/river) \\
\textbf{Syntactic} & Multiple parse trees possible & ``I saw a man with
a telescope'' \\
\textbf{Semantic} & Multiple interpretations & ``Flying planes can be
dangerous'' \\
\end{longtable}
}

\begin{itemize}
\tightlist
\item
  \textbf{Resolution}: Context analysis, statistical models
\item
  \textbf{Challenge}: Major hurdle in NLP systems
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``Lexical words, Syntactic structure, Semantic
meaning''

\end{mnemonicbox}
\subsection*{Question 4(c) OR [7
marks]}\label{q4c}

\textbf{Explain stemming words and parts of speech(POS) tagging with
suitable example.}

\begin{solutionbox}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2903}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4194}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2903}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Process
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Stemming} & Reducing words to root/stem form & ``running'' \rightarrow
``run'', ``flies'' \rightarrow ``fli'' \\
\textbf{POS Tagging} & Assigning grammatical categories & ``The/DT
cat/NN runs/VB fast/RB'' \\
\end{longtable}
}

\textbf{Stemming Example:}

\begin{verbatim}
Original: ["running", "runs", "runner"]
Stemmed: ["run", "run", "runner"]
\end{verbatim}

\textbf{POS Tagging Example:}

\begin{verbatim}
Sentence: "The quick brown fox jumps"
Tagged: "The/DT quick/JJ brown/JJ fox/NN jumps/VB"
\end{verbatim}

\begin{itemize}
\tightlist
\item
  \textbf{Stemming purpose}: Reduce vocabulary size, group related words
\item
  \textbf{POS purpose}: Understand grammatical structure
\item
  \textbf{Applications}: Information retrieval, grammar checking
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``Stem to Root, Tag by Grammar''

\end{mnemonicbox}
\subsection*{Question 5(a) [3 marks]}\label{q5a}

\textbf{Define the term word embedding and list various word embedding
techniques.}

\begin{solutionbox}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.6667}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Term
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Word Embedding} & Dense vector representations of words that
capture semantic relationships \\
\end{longtable}
}

\textbf{Techniques:}

\begin{itemize}
\tightlist
\item
  \textbf{TF-IDF}: Term Frequency-Inverse Document Frequency
\item
  \textbf{Bag of Words (BoW)}: Simple word occurrence counting
\item
  \textbf{Word2Vec}: Neural network-based embeddings
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``TF-IDF counts, BoW bags, Word2Vec vectorizes''

\end{mnemonicbox}
\subsection*{Question 5(b) [4 marks]}\label{q5b}

\textbf{Explain about Challenges with TF-IDF and BoW.}

\begin{solutionbox}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.4000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.6000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Challenges
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{TF-IDF} & Sparse vectors, no semantic similarity, high
dimensionality \\
\textbf{BoW} & Order ignored, context lost, sparse representation \\
\end{longtable}
}

\textbf{Common Issues:}

\begin{itemize}
\tightlist
\item
  \textbf{Sparsity}: Most vector elements are zero
\item
  \textbf{No semantics}: Similar words have different vectors
\item
  \textbf{High dimensions}: Memory and computation intensive
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``Sparse, No Semantics, High Dimensions''

\end{mnemonicbox}
\subsection*{Question 5(c) [7 marks]}\label{q5c}

\textbf{Explain applications of NLP with suitable examples.}

\begin{solutionbox}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3714}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3714}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2571}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Application
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Machine Translation} & Translate between languages & Google
Translate \\
\textbf{Sentiment Analysis} & Determine emotional tone & Product review
analysis \\
\textbf{Question Answering} & Answer questions from text & Chatbots,
virtual assistants \\
\textbf{Spam Detection} & Identify unwanted emails & Email filters \\
\textbf{Spelling Correction} & Fix spelling errors & Auto-correct in
text editors \\
\end{longtable}
}

\begin{center}
\textbf{Mermaid Diagram (Code)}
\begin{verbatim}
{Shaded}
{Highlighting}[]
graph TD
    A[NLP Applications] {-{-}{} B[Machine Translation]}
    A {-{-}{} C[Sentiment Analysis]}
    A {-{-}{} D[Question Answering]}
    A {-{-}{} E[Spam Detection]}
    A {-{-}{} F[Spelling Correction]}
{Highlighting}
{Shaded}
\end{verbatim}
\end{center}

\begin{itemize}
\tightlist
\item
  \textbf{Real-world impact}: Improves human-computer interaction
\item
  \textbf{Business value}: Automates text processing tasks
\item
  \textbf{Growing field}: New applications emerging constantly
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``Translate, Sentiment, Question, Spam, Spell''

\end{mnemonicbox}
\subsection*{Question 5(a) OR [3
marks]}\label{q5a}

\textbf{Describe the Glove(Global Vector for word representation).}

\begin{solutionbox}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.3810}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.6190}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Purpose} & Create word vectors using global corpus statistics \\
\textbf{Method} & Combines global matrix factorization and local
context \\
\textbf{Advantage} & Captures both global and local statistical
information \\
\end{longtable}
}

\begin{itemize}
\tightlist
\item
  \textbf{Global statistics}: Uses word co-occurrence information
\item
  \textbf{Pre-trained}: Available trained vectors for common use
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``Global Vectors, Local Context''

\end{mnemonicbox}
\subsection*{Question 5(b) OR [4
marks]}\label{q5b}

\textbf{Explain the Inverse Document Frequency (IDF).}

\begin{solutionbox}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Component & Formula & Purpose \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{IDF} & log(N/df) & Measure word importance across documents \\
\textbf{N} & Total documents & Corpus size \\
\textbf{df} & Document frequency & Documents containing the term \\
\end{longtable}
}

\begin{itemize}
\tightlist
\item
  \textbf{High IDF}: Rare words (more informative)
\item
  \textbf{Low IDF}: Common words (less informative)
\item
  \textbf{Application}: Part of TF-IDF weighting scheme
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``Inverse Document, Rare is Important''

\end{mnemonicbox}
\subsection*{Question 5(c) OR [7
marks]}\label{q5c}

\textbf{Explain calculation of TF(Term Frequency) for a document with
suitable example.}

\begin{solutionbox}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2667}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formula
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Raw TF} & f(t,d) & Simple count of term in document \\
\textbf{Normalized TF} & f(t,d)/max(f(w,d)) & Normalized by maximum
frequency \\
\textbf{Log TF} & 1 + log(f(t,d)) & Logarithmic scaling \\
\end{longtable}
}

\textbf{Example Document:} ``The cat sat on the mat. The mat was soft.''

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
Term & Count & Raw TF & Normalized TF & Log TF \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
``the'' & 3 & 3 & 1.0 & 1.48 \\
``cat'' & 1 & 1 & 0.33 & 1.0 \\
``mat'' & 2 & 2 & 0.67 & 1.30 \\
\end{longtable}
}

\textbf{Calculation Steps:}

\begin{enumerate}
\tightlist
\item
  Count each term occurrence
\item
  Apply chosen TF formula
\item
  Use in TF-IDF calculation
\end{enumerate}

\end{solutionbox}
\begin{mnemonicbox}
``Count, Normalize, Log''

\end{mnemonicbox}

\end{document}
