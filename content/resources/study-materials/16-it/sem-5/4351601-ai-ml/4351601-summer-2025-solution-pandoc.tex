\documentclass[10pt,a4paper]{article}
\input{../../../../../../latex-templates/gtu-solutions/preamble.tex}
\input{../../../../../../latex-templates/gtu-solutions/english-boxes.tex}

\begin{document}

\begin{center}
{\Huge\bfseries\color{headcolor} Subject Name Solutions}\\[5pt]
{\LARGE 4351601 -- Summer 2025}\\[3pt]
{\large Semester 1 Study Material}\\[3pt]
{\normalsize\textit{Detailed Solutions and Explanations}}
\end{center}

\vspace{10pt}

\subsection*{Question 1(a) [3 marks]}\label{q1a}

\textbf{What is Word Embedding technique? List out different word
embedding techniques.}

\begin{solutionbox}

\textbf{Word Embedding} is a technique that converts words into
numerical vectors while preserving semantic relationships between words.
It represents words as dense vectors in a high-dimensional space where
similar words are closer together.


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Different Word Embedding Techniques}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2973}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3514}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3514}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Technique
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Key Feature
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{TF-IDF} & Term Frequency-Inverse Document Frequency &
Statistical measure \\
\textbf{Bag of Words (BoW)} & Frequency-based representation & Simple
counting method \\
\textbf{Word2Vec} & Neural network-based embedding & Captures semantic
relationships \\
\textbf{GloVe} & Global Vectors for word representation & Combines
global and local statistics \\
\end{longtable}
}

\begin{itemize}
\tightlist
\item
  \textbf{TF-IDF}: Measures word importance in documents
\item
  \textbf{BoW}: Creates vocabulary-based vectors
\item
  \textbf{Word2Vec}: Uses CBOW and Skip-gram models
\item
  \textbf{GloVe}: Pre-trained embeddings with global context
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``TB-WG'' (TF-IDF, BoW, Word2Vec, GloVe)

\end{mnemonicbox}
\subsection*{Question 1(b) [4 marks]}\label{q1b}

\textbf{Categorize the different types of Artificial Intelligence and
demonstrate it with a diagram.}

\begin{solutionbox}

AI can be categorized based on \textbf{capabilities} and
\textbf{functionality}.

\textbf{Diagram:}

\begin{center}
\textbf{Mermaid Diagram (Code)}
\begin{verbatim}
{Shaded}
{Highlighting}[]
graph TD
    A[Artificial Intelligence] {-{-}{} B[Based on Capabilities]}
    A {-{-}{} C[Based on Functionality]}
    
    B {-{-}{} D[Narrow AI/Weak AI]}
    B {-{-}{} E[General AI/Strong AI]}
    B {-{-}{} F[Super AI]}
    
    C {-{-}{} G[Reactive Machines]}
    C {-{-}{} H[Limited Memory]}
    C {-{-}{} I[Theory of Mind]}
    C {-{-}{} J[Self{-}Awareness]}
{Highlighting}
{Shaded}
\end{verbatim}
\end{center}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{AI Types Comparison}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2632}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1579}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3421}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2368}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Category
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Capabilities} & Narrow AI & Task-specific intelligence & Siri,
Chess programs \\
& General AI & Human-level intelligence & Not yet achieved \\
& Super AI & Beyond human intelligence & Theoretical concept \\
\textbf{Functionality} & Reactive & No memory, responds to stimuli &
Deep Blue \\
& Limited Memory & Uses past data & Self-driving cars \\
\end{longtable}
}

\end{solutionbox}
\begin{mnemonicbox}
``NGS-RLT'' (Narrow-General-Super,
Reactive-Limited-Theory)

\end{mnemonicbox}
\subsection*{Question 1(c) [7 marks]}\label{q1c}

\textbf{Explain NLU and NLG by giving difference.}

\begin{solutionbox}

\textbf{Natural Language Understanding (NLU)} and \textbf{Natural
Language Generation (NLG)} are two key components of Natural Language
Processing.


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{NLU vs NLG Comparison}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4444}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2778}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2778}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NLU
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NLG
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Purpose} & Understands human language & Generates human
language \\
\textbf{Direction} & Input processing & Output generation \\
\textbf{Function} & Interprets meaning & Creates text \\
\textbf{Process} & Analysis and comprehension & Synthesis and
creation \\
\textbf{Examples} & Intent recognition, sentiment analysis & Chatbot
responses, report generation \\
\textbf{Challenges} & Ambiguity resolution & Natural text generation \\
\end{longtable}
}

\textbf{Detailed Explanation:}

\begin{itemize}
\tightlist
\item
  \textbf{NLU (Natural Language Understanding)}:

  \begin{itemize}
  \tightlist
  \item
    Converts unstructured text into structured data
  \item
    Performs semantic analysis and intent extraction
  \item
    Handles ambiguity and context understanding
  \end{itemize}
\item
  \textbf{NLG (Natural Language Generation)}:

  \begin{itemize}
  \tightlist
  \item
    Converts structured data into natural language
  \item
    Creates coherent and contextually appropriate text
  \item
    Ensures grammatical correctness and fluency
  \end{itemize}
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``UI-OG'' (Understanding Input, Output Generation)

\end{mnemonicbox}
\subsection*{Question 1(c) OR [7
marks]}\label{q1c}

\textbf{List out various Industries where Artificial Intelligence is
used and explain any two.}

\begin{solutionbox}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{AI Applications in Industries}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2778}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4444}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2778}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Industry
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Applications
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Benefits
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Healthcare} & Diagnosis, drug discovery & Improved accuracy \\
\textbf{Finance} & Fraud detection, trading & Risk management \\
\textbf{Manufacturing} & Quality control, predictive maintenance &
Efficiency \\
\textbf{Transportation} & Autonomous vehicles, route optimization &
Safety \\
\textbf{Retail} & Recommendation systems, inventory & Personalization \\
\textbf{Education} & Personalized learning, assessment & Adaptive
teaching \\
\end{longtable}
}

\textbf{Detailed Explanation of Two Industries:}

\textbf{1. Healthcare Industry:}

\begin{itemize}
\tightlist
\item
  \textbf{Medical Diagnosis}: AI analyzes medical images and patient
  data
\item
  \textbf{Drug Discovery}: Accelerates identification of potential
  medicines
\item
  \textbf{Personalized Treatment}: Tailors therapy based on patient
  genetics
\item
  \textbf{Benefits}: Faster diagnosis, reduced errors, improved outcomes
\end{itemize}

\textbf{2. Finance Industry:}

\begin{itemize}
\tightlist
\item
  \textbf{Fraud Detection}: Identifies suspicious transactions in
  real-time
\item
  \textbf{Algorithmic Trading}: Automated trading based on market
  patterns
\item
  \textbf{Credit Scoring}: Assesses loan default risk accurately
\item
  \textbf{Benefits}: Enhanced security, faster processing, better risk
  management
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``HF-MR-TE'' (Healthcare-Finance,
Manufacturing-Retail-Transportation-Education)

\end{mnemonicbox}
\subsection*{Question 2(a) [3 marks]}\label{q2a}

\textbf{Define the term Machine Learning. Draw the classification
diagram of Machine Learning.}

\begin{solutionbox}

\textbf{Machine Learning} is a subset of AI that enables computers to
learn and improve from experience without being explicitly programmed.
It uses algorithms to analyze data, identify patterns, and make
predictions.

\textbf{Diagram:}

\begin{center}
\textbf{Mermaid Diagram (Code)}
\begin{verbatim}
{Shaded}
{Highlighting}[]
graph TD
    A[Machine Learning] {-{-}{} B[Supervised Learning]}
    A {-{-}{} C[Unsupervised Learning]}
    A {-{-}{} D[Reinforcement Learning]}
    
    B {-{-}{} E[Classification]}
    B {-{-}{} F[Regression]}
    
    C {-{-}{} G[Clustering]}
    C {-{-}{} H[Association]}
    
    D {-{-}{} I[Model{-}based]}
    D {-{-}{} J[Model{-}free]}
{Highlighting}
{Shaded}
\end{verbatim}
\end{center}

\begin{itemize}
\tightlist
\item
  \textbf{Supervised}: Uses labeled training data
\item
  \textbf{Unsupervised}: Finds patterns in unlabeled data
\item
  \textbf{Reinforcement}: Learns through rewards and penalties
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``SUR'' (Supervised-Unsupervised-Reinforcement)

\end{mnemonicbox}
\subsection*{Question 2(b) [4 marks]}\label{q2b}

\textbf{Differentiate Positive reinforcement and Negative
reinforcement.}

\begin{solutionbox}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Positive vs Negative Reinforcement}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1538}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4231}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4231}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Positive Reinforcement
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Negative Reinforcement
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Definition} & Adding reward for good behavior & Removing
unpleasant stimulus \\
\textbf{Action} & Gives something pleasant & Takes away something
unpleasant \\
\textbf{Purpose} & Increase desired behavior & Increase desired
behavior \\
\textbf{Example} & Bonus for good performance & Removing alarm after
waking up \\
\textbf{Effect} & Motivation through rewards & Motivation through
relief \\
\textbf{Agent Response} & Seeks to repeat action & Avoids negative
consequences \\
\end{longtable}
}

\textbf{Key Points:}

\begin{itemize}
\tightlist
\item
  \textbf{Positive Reinforcement}: Strengthens behavior by adding
  positive stimulus
\item
  \textbf{Negative Reinforcement}: Strengthens behavior by removing
  negative stimulus
\item
  \textbf{Both types}: Aim to increase the likelihood of desired
  behavior
\item
  \textbf{Difference}: Method of encouragement (add vs remove)
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``AR-RN'' (Add Reward, Remove Negative)

\end{mnemonicbox}
\subsection*{Question 2(c) [7 marks]}\label{q2c}

\textbf{Compare Supervised and Unsupervised learning.}

\begin{solutionbox}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Supervised vs Unsupervised Learning}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2157}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3725}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4118}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Parameter
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Supervised Learning
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Unsupervised Learning
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Data Type} & Labeled data (input-output pairs) & Unlabeled data
(only inputs) \\
\textbf{Learning Goal} & Predict outcomes & Find hidden patterns \\
\textbf{Feedback} & Has correct answers & No correct answers \\
\textbf{Algorithms} & SVM, Decision Trees, Neural Networks & K-means,
Hierarchical clustering \\
\textbf{Applications} & Classification, Regression & Clustering,
Association rules \\
\textbf{Accuracy} & Can be measured & Difficult to measure \\
\textbf{Complexity} & Less complex & More complex \\
\textbf{Examples} & Email spam detection, Price prediction & Customer
segmentation, Market basket analysis \\
\end{longtable}
}

\textbf{Detailed Comparison:}

\begin{itemize}
\tightlist
\item
  \textbf{Supervised Learning}:

  \begin{itemize}
  \tightlist
  \item
    Requires training data with known outcomes
  \item
    Performance can be easily evaluated
  \item
    Used for prediction tasks
  \end{itemize}
\item
  \textbf{Unsupervised Learning}:

  \begin{itemize}
  \tightlist
  \item
    Works with data without predefined labels
  \item
    Discovers hidden structures in data
  \item
    Used for exploratory data analysis
  \end{itemize}
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``LP-PF'' (Labeled Prediction, Pattern Finding)

\end{mnemonicbox}
\subsection*{Question 2(a) OR [3
marks]}\label{q2a}

\textbf{Define: Classification, Regression, and clustering.}

\begin{solutionbox}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{ML Task Definitions}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3250}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2250}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Task
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Output Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Classification} & Predicts discrete categories/classes &
Categorical & Email: Spam/Not Spam \\
\textbf{Regression} & Predicts continuous numerical values & Numerical &
House price prediction \\
\textbf{Clustering} & Groups similar data points & Groups/Clusters &
Customer segmentation \\
\end{longtable}
}

\textbf{Detailed Definitions:}

\begin{itemize}
\tightlist
\item
  \textbf{Classification}: Assigns input data to predefined categories
  based on learned patterns
\item
  \textbf{Regression}: Estimates relationships between variables to
  predict continuous values
\item
  \textbf{Clustering}: Discovers natural groupings in data without prior
  knowledge of groups
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``CRC'' (Categories, Real numbers, Clusters)

\end{mnemonicbox}
\subsection*{Question 2(b) OR [4
marks]}\label{q2b}

\textbf{Compare Artificial Neural Network and Biological Neural
Network.}

\begin{solutionbox}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{ANN vs Biological Neural Network}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1311}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4262}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4426}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Artificial Neural Network
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Biological Neural Network
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Processing} & Digital/Binary & Analog \\
\textbf{Speed} & Fast processing & Slower processing \\
\textbf{Learning} & Backpropagation algorithm & Synaptic plasticity \\
\textbf{Memory} & Separate storage & Distributed in connections \\
\textbf{Structure} & Layered architecture & Complex 3D structure \\
\textbf{Fault Tolerance} & Low & High \\
\textbf{Energy} & High power consumption & Low energy consumption \\
\textbf{Parallelism} & Limited parallel processing & Massive parallel
processing \\
\end{longtable}
}

\textbf{Key Differences:}

\begin{itemize}
\tightlist
\item
  \textbf{ANN}: Mathematical model inspired by brain
\item
  \textbf{Biological}: Actual brain neural networks
\item
  \textbf{Purpose}: ANN for computation, Biological for cognition
\item
  \textbf{Adaptability}: Biological networks more flexible
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``DSML-CFEP'' (Digital-Speed-Memory-Layer vs
Complex-Fault-Energy-Parallel)

\end{mnemonicbox}
\subsection*{Question 2(c) OR [7
marks]}\label{q2c}

\textbf{List out various applications of supervised, unsupervised and
reinforcement learning.}

\begin{solutionbox}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Applications of Different Learning Types}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3191}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2766}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4043}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Learning Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Applications
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Real-world Examples
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Supervised} & Email classification, Medical diagnosis, Stock
prediction, Credit scoring & Gmail spam filter, X-ray analysis, Trading
algorithms \\
\textbf{Unsupervised} & Customer segmentation, Anomaly detection, Data
compression & Market research, Fraud detection, Image compression \\
\textbf{Reinforcement} & Game playing, Robotics, Autonomous vehicles,
Resource allocation & AlphaGo, Robot navigation, Self-driving cars \\
\end{longtable}
}

\textbf{Detailed Applications:}

\textbf{Supervised Learning:}

\begin{itemize}
\tightlist
\item
  \textbf{Classification}: Spam detection, sentiment analysis, image
  recognition
\item
  \textbf{Regression}: Price forecasting, weather prediction, sales
  estimation
\end{itemize}

\textbf{Unsupervised Learning:}

\begin{itemize}
\tightlist
\item
  \textbf{Clustering}: Market segmentation, gene sequencing,
  recommendation systems
\item
  \textbf{Association}: Market basket analysis, web usage patterns
\end{itemize}

\textbf{Reinforcement Learning:}

\begin{itemize}
\tightlist
\item
  \textbf{Control Systems}: Robot control, traffic management
\item
  \textbf{Optimization}: Resource scheduling, portfolio management
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``SCR-CRO'' (Supervised-Classification-Regression,
Unsupervised-Clustering-Association, Reinforcement-Control-Optimization)

\end{mnemonicbox}
\subsection*{Question 3(a) [3 marks]}\label{q3a}

\textbf{Explain Single Layer Forward Network with proper diagram.}

\begin{solutionbox}

A \textbf{Single Layer Forward Network} (Perceptron) is the simplest
neural network with one layer of weights between input and output.

\textbf{Diagram:}

\begin{center}
\textbf{Mermaid Diagram (Code)}
\begin{verbatim}
{Shaded}
{Highlighting}[]
graph LR
    X1[Input X1] {-{-}{} |W1| S[Σ]}
    X2[Input X2] {-{-}{} |W2| S}
    X3[Input X3] {-{-}{} |W3| S}
    B[Bias b] {-{-}{} S}
    S {-{-}{} A[Activation Function]}
    A {-{-}{} Y[Output Y]}
{Highlighting}
{Shaded}
\end{verbatim}
\end{center}

\textbf{Components:}

\begin{itemize}
\tightlist
\item
  \textbf{Inputs}: X1, X2, X3 (feature values)
\item
  \textbf{Weights}: W1, W2, W3 (connection strengths)
\item
  \textbf{Bias}: Additional parameter for threshold adjustment
\item
  \textbf{Summation}: Weighted sum of inputs
\item
  \textbf{Activation}: Function to produce output
\end{itemize}

\textbf{Mathematical Formula:} Y = f(Σ(Wi \times Xi) + b)

\end{solutionbox}
\begin{mnemonicbox}
``IWSA'' (Input-Weight-Sum-Activation)

\end{mnemonicbox}
\subsection*{Question 3(b) [4 marks]}\label{q3b}

\textbf{Write a short note on Backpropagation.}

\begin{solutionbox}

\textbf{Backpropagation} is a supervised learning algorithm used to
train neural networks by adjusting weights based on error calculation.


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Backpropagation Process}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4643}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2857}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Phase
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Action
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Forward Pass} & Input propagates through network & Calculate
output \\
\textbf{Error Calculation} & Compare output with target & Find
error/loss \\
\textbf{Backward Pass} & Error propagates backward & Update weights \\
\textbf{Weight Update} & Adjust weights using gradient & Minimize
error \\
\end{longtable}
}

\textbf{Key Features:}

\begin{itemize}
\tightlist
\item
  \textbf{Gradient Descent}: Uses calculus to find optimal weights
\item
  \textbf{Chain Rule}: Calculates error contribution of each weight
\item
  \textbf{Iterative Process}: Repeats until convergence
\item
  \textbf{Learning Rate}: Controls speed of weight updates
\end{itemize}

\textbf{Steps:}

\begin{enumerate}
\tightlist
\item
  Initialize random weights
\item
  Forward propagation to get output
\item
  Calculate error between actual and predicted
\item
  Backward propagation to update weights
\end{enumerate}

\end{solutionbox}
\begin{mnemonicbox}
``FCBU'' (Forward-Calculate-Backward-Update)

\end{mnemonicbox}
\subsection*{Question 3(c) [7 marks]}\label{q3c}

\textbf{Explain the components of architecture of Feed Forward Neuron
Network.}

\begin{solutionbox}

\textbf{Feed Forward Neural Network} consists of multiple layers where
information flows in one direction from input to output.

\textbf{Diagram:}

\begin{center}
\textbf{Mermaid Diagram (Code)}
\begin{verbatim}
{Shaded}
{Highlighting}[]
graph TD
    subgraph Input Layer
    I1[X1]
    I2[X2]
    I3[X3]
    end
    
    subgraph Hidden Layer
    H1[N1]
    H2[N2]
    H3[N3]
    end
    
    subgraph Output Layer
    O1[Y1]
    O2[Y2]
    end
    
    I1 {-{-}{} H1}
    I1 {-{-}{} H2}
    I1 {-{-}{} H3}
    I2 {-{-}{} H1}
    I2 {-{-}{} H2}
    I2 {-{-}{} H3}
    I3 {-{-}{} H1}
    I3 {-{-}{} H2}
    I3 {-{-}{} H3}
    
    H1 {-{-}{} O1}
    H1 {-{-}{} O2}
    H2 {-{-}{} O1}
    H2 {-{-}{} O2}
    H3 {-{-}{} O1}
    H3 {-{-}{} O2}
{Highlighting}
{Shaded}
\end{verbatim}
\end{center}

\textbf{Components:}

\textbf{1. Input Layer:}

\begin{itemize}
\tightlist
\item
  Receives raw data
\item
  No processing, just distribution
\item
  Number of neurons = number of features
\end{itemize}

\textbf{2. Hidden Layer(s):}

\begin{itemize}
\tightlist
\item
  Performs computation and transformation
\item
  Contains activation functions
\item
  Can have multiple hidden layers
\end{itemize}

\textbf{3. Output Layer:}

\begin{itemize}
\tightlist
\item
  Produces final results
\item
  Number of neurons = number of outputs
\item
  Uses appropriate activation for task type
\end{itemize}

\textbf{4. Weights and Biases:}

\begin{itemize}
\tightlist
\item
  \textbf{Weights}: Connection strengths between neurons
\item
  \textbf{Biases}: Threshold adjustment parameters
\end{itemize}

\textbf{5. Activation Functions:}

\begin{itemize}
\tightlist
\item
  Introduce non-linearity
\item
  Common types: ReLU, Sigmoid, Tanh
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``IHO-WA'' (Input-Hidden-Output, Weights-Activation)

\end{mnemonicbox}
\subsection*{Question 3(a) OR [3
marks]}\label{q3a}

\textbf{Explain Multilayer Feed Forward ANN with diagram.}

\begin{solutionbox}

\textbf{Multilayer Feed Forward ANN} contains multiple hidden layers
between input and output layers, enabling complex pattern recognition.

\textbf{Diagram:}

\begin{center}
\textbf{Mermaid Diagram (Code)}
\begin{verbatim}
{Shaded}
{Highlighting}[]
graph LR
    subgraph Input
    I1[X1]
    I2[X2]
    end
    
    subgraph Hidden1
    H11[H1]
    H12[H2]
    end
    
    subgraph Hidden2
    H21[H1]
    H22[H2]
    end
    
    subgraph Output
    O1[Y]
    end
    
    I1 {-{-}{} H11}
    I1 {-{-}{} H12}
    I2 {-{-}{} H11}
    I2 {-{-}{} H12}
    
    H11 {-{-}{} H21}
    H11 {-{-}{} H22}
    H12 {-{-}{} H21}
    H12 {-{-}{} H22}
    
    H21 {-{-}{} O1}
    H22 {-{-}{} O1}
{Highlighting}
{Shaded}
\end{verbatim}
\end{center}

\textbf{Characteristics:}

\begin{itemize}
\tightlist
\item
  \textbf{Deep Architecture}: Multiple hidden layers
\item
  \textbf{Complex Patterns}: Can learn non-linear relationships\\
\item
  \textbf{Universal Approximator}: Can approximate any continuous
  function
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``MDC'' (Multiple layers, Deep learning, Complex
patterns)

\end{mnemonicbox}
\subsection*{Question 3(b) OR [4
marks]}\label{q3b}

\textbf{Explain `ReLU is the most commonly used Activation function.'}

\begin{solutionbox}

\textbf{ReLU (Rectified Linear Unit)} is widely used due to its
simplicity and effectiveness in deep networks.


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Why ReLU is Popular}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3939}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2727}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Advantage
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Benefit
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Computational Efficiency} & Simple max(0,x) operation & Fast
processing \\
\textbf{Gradient Flow} & No vanishing gradient for positive values &
Better learning \\
\textbf{Sparsity} & Outputs zero for negative inputs & Efficient
representation \\
\textbf{Non-linearity} & Introduces non-linear behavior & Complex
pattern learning \\
\end{longtable}
}

\textbf{Mathematical Definition:} f(x) = max(0, x)

\textbf{Comparison with Other Functions:}

\begin{itemize}
\tightlist
\item
  \textbf{vs Sigmoid}: No saturation problem, faster computation
\item
  \textbf{vs Tanh}: Simpler calculation, better gradient flow
\item
  \textbf{Limitations}: Dead neurons problem for negative inputs
\end{itemize}

\textbf{Why Most Common:}

\begin{itemize}
\tightlist
\item
  Solves vanishing gradient problem
\item
  Computationally efficient
\item
  Works well in practice
\item
  Default choice for hidden layers
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``CGSN'' (Computational, Gradient, Sparsity,
Non-linear)

\end{mnemonicbox}
\subsection*{Question 3(c) OR [7
marks]}\label{q3c}

\textbf{Explain step by step learning process of Artificial Neural
Network.}

\begin{solutionbox}

\textbf{ANN Learning Process} involves iterative weight adjustment to
minimize prediction error.


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Step-by-Step Learning Process}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2143}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3214}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4643}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Step
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Process
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{1. Initialization} & Set random weights & Small random values \\
\textbf{2. Forward Propagation} & Calculate output & Input \rightarrow Hidden \rightarrow
Output \\
\textbf{3. Error Calculation} & Compare with target & Loss function
computation \\
\textbf{4. Backward Propagation} & Calculate gradients & Error \rightarrow Hidden
\leftarrow Input \\
\textbf{5. Weight Update} & Adjust parameters & Gradient descent \\
\textbf{6. Iteration} & Repeat process & Until convergence \\
\end{longtable}
}

\textbf{Detailed Steps:}

\textbf{Step 1: Initialize Weights}

\begin{itemize}
\tightlist
\item
  Assign small random values to all weights and biases
\item
  Prevents symmetry breaking problem
\end{itemize}

\textbf{Step 2: Forward Propagation}

\begin{itemize}
\tightlist
\item
  Input data flows through network layers
\item
  Each neuron computes weighted sum + activation
\end{itemize}

\textbf{Step 3: Calculate Error}

\begin{itemize}
\tightlist
\item
  Compare network output with desired output
\item
  Use loss functions like MSE or Cross-entropy
\end{itemize}

\textbf{Step 4: Backward Propagation}

\begin{itemize}
\tightlist
\item
  Calculate error gradient for each weight
\item
  Use chain rule to propagate error backward
\end{itemize}

\textbf{Step 5: Update Weights}

\begin{itemize}
\tightlist
\item
  Adjust weights using gradient descent
\item
  New\_weight = Old\_weight - (learning\_rate \times gradient)
\end{itemize}

\textbf{Step 6: Repeat Process}

\begin{itemize}
\tightlist
\item
  Continue until error converges or maximum epochs reached
\item
  Monitor validation performance to avoid overfitting
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``IFEBWI''
(Initialize-Forward-Error-Backward-Weight-Iterate)

\end{mnemonicbox}
\subsection*{Question 4(a) [3 marks]}\label{q4a}

\textbf{List out various advantages and disadvantages of Natural
Language Processing.}

\begin{solutionbox}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{NLP Advantages and Disadvantages}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.4444}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.5556}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Advantages
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Disadvantages
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Automation} of text processing & \textbf{Ambiguity} in human
language \\
\textbf{24/7 Availability} for customer service & \textbf{Context
Understanding} challenges \\
\textbf{Multilingual Support} capabilities & \textbf{Cultural Nuances}
difficulty \\
\textbf{Scalability} for large datasets & \textbf{High Computational}
requirements \\
\textbf{Consistency} in responses & \textbf{Data Privacy} concerns \\
\textbf{Cost Reduction} in operations & \textbf{Limited Creativity} in
responses \\
\end{longtable}
}

\textbf{Key Points:}

\begin{itemize}
\tightlist
\item
  \textbf{Advantages}: Efficiency, accessibility, consistency
\item
  \textbf{Disadvantages}: Complexity, resource requirements, limitations
\item
  \textbf{Balance}: Benefits outweigh challenges in many applications
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``AMS-ACC'' (Automation-Multilingual-Scalability vs
Ambiguity-Context-Computational)

\end{mnemonicbox}
\subsection*{Question 4(b) [4 marks]}\label{q4b}

\textbf{List out preprocessing techniques in NLP and demonstrate any one
with a python program.}

\begin{solutionbox}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{NLP Preprocessing Techniques}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3793}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3103}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3103}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Technique
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Tokenization} & Split text into words/sentences & ``Hello
world'' \rightarrow [``Hello'', ``world''] \\
\textbf{Stop Words Removal} & Remove common words & Remove ``the'',
``is'', ``and'' \\
\textbf{Stemming} & Reduce words to root form & ``running'' \rightarrow ``run'' \\
\textbf{Lemmatization} & Convert to dictionary form & ``better'' \rightarrow
``good'' \\
\textbf{POS Tagging} & Identify parts of speech & ``run'' \rightarrow verb \\
\textbf{Named Entity Recognition} & Identify entities & ``Apple'' \rightarrow
Organization \\
\end{longtable}
}

\textbf{Python Program - Tokenization:}

\begin{verbatim}
import nltk
from nltk.tokenize import word\_tokenize, sent\_tokenize

\# Sample text
text = "Natural Language Processing is amazing. It helps computers understand human language."

\# Word tokenization
words = word\_tokenize(text)
print("Words:", words)

\# Sentence tokenization  
sentences = sent\_tokenize(text)
print("Sentences:", sentences)
\end{verbatim}

\end{solutionbox}
\begin{mnemonicbox}
``TSSL-PN''
(Tokenization-Stop-Stemming-Lemmatization, POS-NER)

\end{mnemonicbox}
\subsection*{Question 4(c) [7 marks]}\label{q4c}

\textbf{Explain the phases of NLP.}

\begin{solutionbox}

\textbf{NLP Phases} represent the systematic approach to process and
understand natural language.


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{NLP Phases}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1842}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3421}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2368}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2368}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Phase
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Process
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Lexical Analysis} & Tokenization and word identification & Break
text into tokens & ``I am happy'' \rightarrow [``I'', ``am'', ``happy''] \\
\textbf{Syntactic Analysis} & Grammar and sentence structure & Parse
trees, POS tagging & Identify noun, verb, adjective \\
\textbf{Semantic Analysis} & Meaning extraction & Word sense
disambiguation & ``Bank'' \rightarrow financial vs river \\
\textbf{Discourse Integration} & Context across sentences & Resolve
pronouns, references & ``He'' refers to ``John'' \\
\textbf{Pragmatic Analysis} & Intent and context understanding &
Consider situation/culture & Sarcasm, idioms interpretation \\
\end{longtable}
}

\textbf{Detailed Explanation:}

\textbf{1. Lexical Analysis:}

\begin{itemize}
\tightlist
\item
  First phase of NLP pipeline
\item
  Converts character stream into tokens
\item
  Removes punctuation and special characters
\end{itemize}

\textbf{2. Syntactic Analysis:}

\begin{itemize}
\tightlist
\item
  Analyzes grammatical structure
\item
  Creates parse trees
\item
  Identifies sentence components
\end{itemize}

\textbf{3. Semantic Analysis:}

\begin{itemize}
\tightlist
\item
  Extracts meaning from text
\item
  Handles word ambiguity
\item
  Maps words to concepts
\end{itemize}

\textbf{4. Discourse Integration:}

\begin{itemize}
\tightlist
\item
  Analyzes text beyond sentence level
\item
  Maintains context across sentences
\item
  Resolves references and connections
\end{itemize}

\textbf{5. Pragmatic Analysis:}

\begin{itemize}
\tightlist
\item
  Considers real-world context
\item
  Understands speaker's intent
\item
  Handles figurative language
\end{itemize}

\textbf{Mnemaid Diagram:}

\begin{center}
\textbf{Mermaid Diagram (Code)}
\begin{verbatim}
{Shaded}
{Highlighting}[]
graph LR
    A[Raw Text] {-{-}{} B[Lexical Analysis]}
    B {-{-}{} C[Syntactic Analysis]}
    C {-{-}{} D[Semantic Analysis]}
    D {-{-}{} E[Discourse Integration]}
    E {-{-}{} F[Pragmatic Analysis]}
    F {-{-}{} G[Understanding]}
{Highlighting}
{Shaded}
\end{verbatim}
\end{center}

\end{solutionbox}
\begin{mnemonicbox}
``LSSDP''
(Lexical-Syntactic-Semantic-Discourse-Pragmatic)

\end{mnemonicbox}
\subsection*{Question 4(a) OR [3
marks]}\label{q4a}

\textbf{What is Natural Language Processing? List out its applications.}

\begin{solutionbox}

\textbf{Natural Language Processing (NLP)} is a branch of AI that
enables computers to understand, interpret, and generate human language
in a meaningful way.


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{NLP Applications}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3030}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3939}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3030}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Category
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Applications
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Examples
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Communication} & Chatbots, Virtual assistants & Siri, Alexa,
ChatGPT \\
\textbf{Translation} & Language translation & Google Translate \\
\textbf{Analysis} & Sentiment analysis, Text mining & Social media
monitoring \\
\textbf{Search} & Information retrieval & Search engines \\
\textbf{Writing} & Grammar checking, Auto-complete & Grammarly,
predictive text \\
\textbf{Business} & Document processing, Spam detection & Email
filtering \\
\end{longtable}
}

\textbf{Key Applications:}

\begin{itemize}
\tightlist
\item
  \textbf{Machine Translation}: Converting text between languages
\item
  \textbf{Speech Recognition}: Converting speech to text
\item
  \textbf{Text Summarization}: Creating concise summaries
\item
  \textbf{Question Answering}: Providing answers to queries
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``CTAS-WB''
(Communication-Translation-Analysis-Search, Writing-Business)

\end{mnemonicbox}
\subsection*{Question 4(b) OR [4
marks]}\label{q4b}

\textbf{List out the tasks performed with WordNet in NLTK and
demonstrate anyone with a python code.}

\begin{solutionbox}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{WordNet Tasks in NLTK}
\vspace{-10pt}
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Task & Description & Purpose \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Synsets} & Find synonymous words & Word similarity \\
\textbf{Definitions} & Get word meanings & Understanding context \\
\textbf{Examples} & Usage examples & Practical application \\
\textbf{Hyponyms} & Find specific terms & Hierarchical relationships \\
\textbf{Hypernyms} & Find general terms & Category identification \\
\textbf{Antonyms} & Find opposite words & Contrast analysis \\
\end{longtable}
}

\textbf{Python Code - Synsets and Definitions:}

\begin{verbatim}
from nltk.corpus import wordnet

\# Get synsets for word {good}
synsets = wordnet.synsets({good})
print("Synsets:", synsets)

\# Get definition
definition = synsets[0].definition()
print("Definition:", definition)

\# Get examples
examples = synsets[0].examples()
print("Examples:", examples)
\end{verbatim}

\end{solutionbox}
\begin{mnemonicbox}
``SDEHA''
(Synsets-Definitions-Examples-Hyponyms-Antonyms)

\end{mnemonicbox}
\subsection*{Question 4(c) OR [7
marks]}\label{q4c}

\textbf{Explain the types of ambiguities in NLP.}

\begin{solutionbox}

\textbf{NLP Ambiguities} occur when text can be interpreted in multiple
ways, creating challenges for automated understanding.


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Types of Ambiguities}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1538}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2308}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2821}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Resolution
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Lexical} & Multiple meanings of single word & ``Bank''
(financial/river) & Context analysis \\
\textbf{Syntactic} & Multiple grammatical interpretations & ``Flying
planes can be dangerous'' & Parse trees \\
\textbf{Semantic} & Multiple meanings at sentence level & ``Time flies
like an arrow'' & Semantic analysis \\
\textbf{Pragmatic} & Context-dependent interpretation & ``Can you pass
the salt?'' & Situational context \\
\textbf{Referential} & Unclear pronoun references & ``John told Bob he
was wrong'' & Discourse analysis \\
\end{longtable}
}

\textbf{Detailed Explanation:}

\textbf{1. Lexical Ambiguity:}

\begin{itemize}
\tightlist
\item
  Same word, different meanings
\item
  Homonyms and polysemes
\item
  Example: ``Bat'' (animal/sports equipment)
\end{itemize}

\textbf{2. Syntactic Ambiguity:}

\begin{itemize}
\tightlist
\item
  Multiple grammatical structures
\item
  Different parse trees possible
\item
  Example: ``I saw a man with a telescope''
\end{itemize}

\textbf{3. Semantic Ambiguity:}

\begin{itemize}
\tightlist
\item
  Sentence-level meaning confusion
\item
  Multiple interpretations possible
\item
  Example: ``Visiting relatives can be boring''
\end{itemize}

\textbf{4. Pragmatic Ambiguity:}

\begin{itemize}
\tightlist
\item
  Context and intent dependent
\item
  Cultural and situational factors
\item
  Example: Sarcasm and indirect requests
\end{itemize}

\textbf{5. Referential Ambiguity:}

\begin{itemize}
\tightlist
\item
  Unclear references to entities
\item
  Pronoun resolution challenges
\item
  Example: Multiple possible antecedents
\end{itemize}

\textbf{Resolution Strategies:}

\begin{itemize}
\tightlist
\item
  Context analysis and machine learning
\item
  Statistical disambiguation methods
\item
  Knowledge bases and ontologies
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``LSSPR''
(Lexical-Syntactic-Semantic-Pragmatic-Referential)

\end{mnemonicbox}
\subsection*{Question 5(a) [3 marks]}\label{q5a}

\textbf{Explain Bag of Words with example.}

\begin{solutionbox}

\textbf{Bag of Words (BoW)} is a text representation method that
converts text into numerical vectors based on word frequency, ignoring
grammar and word order.


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{BoW Process}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2143}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3214}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4643}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Step
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Process
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{1. Tokenization} & Split text into words & Create vocabulary \\
\textbf{2. Vocabulary Creation} & Unique words collection & Dictionary
of terms \\
\textbf{3. Vector Creation} & Count word frequencies & Numerical
representation \\
\end{longtable}
}

\textbf{Example:}

Documents:

\begin{itemize}
\tightlist
\item
  Doc1: ``I love machine learning''
\item
  Doc2: ``Machine learning is amazing''
\end{itemize}

\textbf{Vocabulary:} [I, love, machine, learning, is, amazing]

\textbf{BoW Vectors:}

\begin{itemize}
\tightlist
\item
  Doc1: [1, 1, 1, 1, 0, 0]
\item
  Doc2: [0, 0, 1, 1, 1, 1]
\end{itemize}

\textbf{Characteristics:}

\begin{itemize}
\tightlist
\item
  \textbf{Order Independent}: Word sequence ignored
\item
  \textbf{Frequency Based}: Counts word occurrences
\item
  \textbf{Sparse Representation}: Many zero values
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``TVC'' (Tokenize-Vocabulary-Count)

\end{mnemonicbox}
\subsection*{Question 5(b) [4 marks]}\label{q5b}

\textbf{What is Word2Vec? Explain its steps.}

\begin{solutionbox}

\textbf{Word2Vec} is a neural network-based technique that creates dense
vector representations of words by learning from their context in large
text corpora.


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Word2Vec Models}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2414}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3448}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4138}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Model
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Approach
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Prediction
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{CBOW} & Continuous Bag of Words & Context \rightarrow Target word \\
\textbf{Skip-gram} & Skip-gram with Negative Sampling & Target word \rightarrow
Context \\
\end{longtable}
}

\textbf{Steps of Word2Vec:}

\textbf{1. Data Preparation:}

\begin{itemize}
\tightlist
\item
  Collect large text corpus
\item
  Clean and preprocess text
\item
  Create training pairs
\end{itemize}

\textbf{2. Model Architecture:}

\begin{itemize}
\tightlist
\item
  Input layer (one-hot encoded words)
\item
  Hidden layer (embedding layer)
\item
  Output layer (softmax for prediction)
\end{itemize}

\textbf{3. Training Process:}

\begin{itemize}
\tightlist
\item
  \textbf{CBOW}: Predict target word from context
\item
  \textbf{Skip-gram}: Predict context from target word
\item
  Use backpropagation to update weights
\end{itemize}

\textbf{4. Vector Extraction:}

\begin{itemize}
\tightlist
\item
  Extract weight matrix from hidden layer
\item
  Each row represents word embedding
\item
  Typically 100-300 dimensions
\end{itemize}

\textbf{Benefits:}

\begin{itemize}
\tightlist
\item
  Captures semantic relationships
\item
  Similar words have similar vectors
\item
  Supports arithmetic operations (King - Man + Woman = Queen)
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``DMAT'' (Data-Model-Architecture-Training)

\end{mnemonicbox}
\subsection*{Question 5(c) [7 marks]}\label{q5c}

\textbf{List out applications of NLP and explain any one in detail.}

\begin{solutionbox}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{NLP Applications}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3250}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3250}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3500}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Application
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Industry Use
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Machine Translation} & Language conversion & Global
communication \\
\textbf{Sentiment Analysis} & Opinion mining & Social media
monitoring \\
\textbf{Chatbots} & Conversational AI & Customer service \\
\textbf{Text Summarization} & Content condensation & News, research \\
\textbf{Speech Recognition} & Voice to text & Virtual assistants \\
\textbf{Information Extraction} & Data mining from text & Business
intelligence \\
\textbf{Question Answering} & Automated responses & Search engines \\
\textbf{Spam Detection} & Email filtering & Cybersecurity \\
\end{longtable}
}

\textbf{Detailed Explanation: Sentiment Analysis}

\textbf{Sentiment Analysis} is the process of determining emotional tone
and opinions expressed in text data.

\textbf{Components:}

\begin{itemize}
\tightlist
\item
  \textbf{Text Preprocessing}: Cleaning and tokenization
\item
  \textbf{Feature Extraction}: TF-IDF, word embeddings
\item
  \textbf{Classification}: Positive, negative, neutral
\item
  \textbf{Confidence Scoring}: Strength of sentiment
\end{itemize}

\textbf{Process Steps:}

\begin{enumerate}
\tightlist
\item
  \textbf{Data Collection}: Gather text from reviews, social media
\item
  \textbf{Preprocessing}: Remove noise, normalize text
\item
  \textbf{Feature Engineering}: Convert text to numerical features
\item
  \textbf{Model Training}: Use ML algorithms for classification
\item
  \textbf{Prediction}: Classify new text sentiment
\item
  \textbf{Evaluation}: Measure accuracy and performance
\end{enumerate}

\textbf{Applications:}

\begin{itemize}
\tightlist
\item
  \textbf{Brand Monitoring}: Track customer opinions
\item
  \textbf{Product Reviews}: Analyze customer feedback
\item
  \textbf{Social Media}: Monitor public sentiment
\item
  \textbf{Market Research}: Understand consumer preferences
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``MSCTSIQ-S''
(Machine-Sentiment-Chatbot-Text-Speech-Information-Question-Spam)

\end{mnemonicbox}
\subsection*{Question 5(a) OR [3
marks]}\label{q5a}

\textbf{Explain TFIDF with example.}

\begin{solutionbox}

\textbf{TF-IDF (Term Frequency-Inverse Document Frequency)} measures
word importance in a document relative to a collection of documents.

\textbf{Formula:} TF-IDF = TF(t,d) \times IDF(t)

Where:

\begin{itemize}
\tightlist
\item
  TF(t,d) = (Number of times term t appears in document d) / (Total
  terms in document d)
\item
  IDF(t) = log(Total documents / Documents containing term t)
\end{itemize}

\textbf{Example:}

Documents:

\begin{itemize}
\tightlist
\item
  Doc1: ``machine learning is good''
\item
  Doc2: ``learning algorithms are good''
\item
  Doc3: ``machine algorithms work well''
\end{itemize}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{TF-IDF Calculation for ``machine''}
\vspace{-10pt}
\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Document & TF & IDF & TF-IDF \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Doc1 & 1/4 = 0.25 & log(3/2) = 0.18 & 0.25 \times 0.18 = 0.045 \\
Doc2 & 0/4 = 0 & log(3/2) = 0.18 & 0 \times 0.18 = 0 \\
Doc3 & 1/4 = 0.25 & log(3/2) = 0.18 & 0.25 \times 0.18 = 0.045 \\
\end{longtable}
}

\textbf{Key Points:}

\begin{itemize}
\tightlist
\item
  \textbf{High TF-IDF}: Important word in specific document
\item
  \textbf{Low TF-IDF}: Common word across documents
\item
  \textbf{Applications}: Information retrieval, text mining
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``TI-FD'' (Term frequency, Inverse Document
frequency)

\end{mnemonicbox}
\subsection*{Question 5(b) OR [4
marks]}\label{q5b}

\textbf{Explain about challenges with TFIDF and BOW.}

\begin{solutionbox}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Challenges with TF-IDF and BOW}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3438}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1562}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Challenge
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
TF-IDF
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
BOW
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Impact
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Semantic Understanding} & Cannot capture meaning & Ignores word
relationships & Poor context understanding \\
\textbf{Word Order} & Position ignored & Sequence lost & Grammar meaning
lost \\
\textbf{Sparsity} & High-dimensional vectors & Many zero values & Memory
inefficient \\
\textbf{Vocabulary Size} & Large feature space & Grows with corpus &
Computational complexity \\
\textbf{Out-of-Vocabulary} & Unknown words ignored & New words not
handled & Limited generalization \\
\textbf{Polysemy} & Multiple meanings & Same treatment for different
senses & Ambiguity issues \\
\end{longtable}
}

\textbf{Detailed Challenges:}

\textbf{1. Lack of Semantic Understanding:}

\begin{itemize}
\tightlist
\item
  Words treated as independent features
\item
  Cannot understand synonyms or related concepts
\item
  ``Good'' and ``excellent'' treated differently
\end{itemize}

\textbf{2. Loss of Word Order:}

\begin{itemize}
\tightlist
\item
  ``Dog bites man'' vs ``Man bites dog'' same representation
\item
  Context and grammar information lost
\item
  Sentence structure ignored
\end{itemize}

\textbf{3. High Dimensionality:}

\begin{itemize}
\tightlist
\item
  Vector size equals vocabulary size
\item
  Sparse matrices with mostly zeros
\item
  Storage and computation problems
\end{itemize}

\textbf{4. Context Insensitivity:}

\begin{itemize}
\tightlist
\item
  Same word different contexts treated equally
\item
  ``Apple'' company vs fruit same representation
\item
  Polysemy and homonymy issues
\end{itemize}

\textbf{Solutions:}

\begin{itemize}
\tightlist
\item
  \textbf{Word Embeddings}: Word2Vec, GloVe
\item
  \textbf{Contextual Models}: BERT, GPT
\item
  \textbf{N-grams}: Capture some word order
\item
  \textbf{Dimensionality Reduction}: PCA, SVD
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``SSVO-CP'' (Semantic-Sequence-Vocabulary-OOV,
Context-Polysemy)

\end{mnemonicbox}
\subsection*{Question 5(c) OR [7
marks]}\label{q5c}

\textbf{Explain the working of GloVe.}

\begin{solutionbox}

\textbf{GloVe (Global Vectors for Word Representation)} combines global
statistical information with local context windows to create word
embeddings.


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{GloVe vs Other Methods}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1818}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1591}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2273}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4318}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
GloVe
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Word2Vec
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Traditional Methods
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Approach} & Global + Local statistics & Local context windows &
Frequency-based \\
\textbf{Training} & Matrix factorization & Neural networks & Counting
methods \\
\textbf{Efficiency} & Fast training & Slower training & Very fast \\
\textbf{Performance} & High accuracy & Good accuracy & Limited
performance \\
\end{longtable}
}

\textbf{Working Process:}

\textbf{1. Co-occurrence Matrix Construction:}

\begin{itemize}
\tightlist
\item
  Count word co-occurrences in context windows
\item
  Create global statistics matrix
\item
  Xij = number of times word j appears in context of word i
\end{itemize}

\textbf{2. Ratio Calculation:}

\begin{itemize}
\tightlist
\item
  Calculate probability ratios
\item
  P(k\textbar i) = Xik / Xi (probability of word k given word i)
\item
  Focus on meaningful ratios between probabilities
\end{itemize}

\textbf{3. Objective Function:}

\begin{itemize}
\tightlist
\item
  Minimize weighted least squares objective
\item
  J = Σ f(Xij)(wi\^{}T wj + bi + bj - log Xij)^{2}
\item
  Where f(x) is weighting function
\end{itemize}

\textbf{4. Vector Learning:}

\begin{itemize}
\tightlist
\item
  Use gradient descent to optimize objective
\item
  Learn word vectors wi and context vectors wj
\item
  Final representation combines both vectors
\end{itemize}

\textbf{Key Features:}

\textbf{Global Statistics:}

\begin{itemize}
\tightlist
\item
  Uses entire corpus information
\item
  Captures global word relationships
\item
  More stable than local methods
\end{itemize}

\textbf{Efficiency:}

\begin{itemize}
\tightlist
\item
  Trains on co-occurrence statistics
\item
  Faster than neural network methods
\item
  Scalable to large corpora
\end{itemize}

\textbf{Performance:}

\begin{itemize}
\tightlist
\item
  Performs well on analogy tasks
\item
  Captures both semantic and syntactic relationships
\item
  Good performance on similarity tasks
\end{itemize}

\textbf{Mathematical Foundation:}

\begin{verbatim}
J = Σ(i,j=1 to V) f(Xij)(wi^T wj + bi + bj - log Xij)^{2}
\end{verbatim}

Where:

\begin{itemize}
\tightlist
\item
  V = vocabulary size
\item
  Xij = co-occurrence count
\item
  wi, wj = word vectors
\item
  bi, bj = bias terms
\item
  f(x) = weighting function
\end{itemize}

\textbf{Advantages:}

\begin{itemize}
\tightlist
\item
  \textbf{Combines Benefits}: Global statistics + local context
\item
  \textbf{Interpretable}: Clear mathematical foundation
\item
  \textbf{Efficient}: Faster training than Word2Vec
\item
  \textbf{Effective}: Good performance on various tasks
\end{itemize}

\textbf{Applications:}

\begin{itemize}
\tightlist
\item
  \textbf{Word Similarity}: Find related words
\item
  \textbf{Analogy Tasks}: King - Man + Woman = Queen
\item
  \textbf{Text Classification}: Feature representation
\item
  \textbf{Machine Translation}: Cross-lingual mappings
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``CROF-PGAE''
(Co-occurrence-Ratio-Objective-Function,
Performance-Global-Advantage-Efficiency)

\end{mnemonicbox}

\end{document}
