\documentclass{article}
\input{/Users/milav/Code/milav-next/latex-templates/gtu-solutions/preamble.tex}
\input{/Users/milav/Code/milav-next/latex-templates/gtu-solutions/gujarati-boxes.tex}
\input{/Users/milav/Code/milav-next/latex-templates/gtu-solutions/commands.tex}

\title{Foundation of AI and ML (4351601) - Summer 2025 Solution}
\date{May 12, 2025}

\begin{document}
\maketitle

\questionmarks{1}{a}{3}
\textbf{વર્ડ એમ્બેડિંગ ટેકનિક શું છે? વિવિધ વર્ડ એમ્બેડિંગ તકનીકોની સૂચિ બનાવો.}

\textbf{જવાબ:}

\textbf{વર્ડ એમ્બેડિંગ} એ એવી તકનીક છે જે શબ્દોને આંકડાકીય vectors માં રૂપાંતરિત કરે છે અને શબ્દો વચ્ચેના semantic સંબંધોને જાળવી રાખે છે. આ શબ્દોને high-dimensional space માં dense vectors તરીકે દર્શાવે છે.

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{તકનીક} & \textbf{વર્ણન} & \textbf{મુખ્ય લક્ષણ} \\
\hline
\textbf{TF-IDF} & Term Frequency-Inverse Document Frequency & આંકડાકીય માપદંડ \\
\hline
\textbf{Bag of Words (BoW)} & આવર્તન-આધારિત રજૂઆત & સરળ ગણતરી પદ્ધતિ \\
\hline
\textbf{Word2Vec} & Neural network-આધારિત embedding & Semantic સંબંધો કેપ્ચર કરે \\
\hline
\textbf{GloVe} & Global Vectors for word representation & Global અને local આંકડા સંયોજન \\
\hline
\end{tabulary}

\textbf{મુખ્ય પોઈન્ટ્સ:}
\begin{itemize}
    \item \textbf{TF-IDF}: દસ્તાવેજોમાં શબ્દની મહત્ત્વતા માપે છે
    \item \textbf{BoW}: Vocabulary-આધારિત vectors બનાવે છે
    \item \textbf{Word2Vec}: CBOW અને Skip-gram models વાપરે છે
    \item \textbf{GloVe}: Global context સાથે pre-trained embeddings
\end{itemize}

\begin{mnemonicbox}
"TB-WG" (TF-IDF, BoW, Word2Vec, GloVe)
\end{mnemonicbox}

\questionmarks{1}{b}{4}
\textbf{આર્ટિફિશિયલ ઇન્ટેલિજન્સના વિવિધ પ્રકારોનું વર્ગીકરણ કરો અને તેને ડાયાગ્રામ વડે દર્શાવો.}

\textbf{જવાબ:}

AI ને \textbf{ક્ષમતાઓ} અને \textbf{કાર્યક્ષમતા} આધારે વર્ગીકૃત કરી શકાય છે.

\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=1.5cm,
    level 1/.style={sibling distance=7cm},
    level 2/.style={sibling distance=2.5cm}, 
    edge from parent/.style={draw, -latex}]
    
    \node [gtu block] {Artificial Intelligence}
        child {node [gtu block] {Based on Capabilities\\(ક્ષમતાઓ આધારે)}
            child {node [gtu block] {Narrow AI/\\Weak AI}}
            child {node [gtu block] {General AI/\\Strong AI}}
            child {node [gtu block] {Super AI}}
        }
        child {node [gtu block] {Based on Functionality\\(કાર્યક્ષમતા આધારે)}
            child {node [gtu block] {Reactive\\Machines}}
            child {node [gtu block] {Limited\\Memory}}
            child {node [gtu block] {Theory of\\Mind}}
            child {node [gtu block] {Self-\\Awareness}}
        };
\end{tikzpicture}
\caption{Classification of AI}
\end{figure}

\begin{tabulary}{\linewidth}{|L|L|L|L|}
\hline
\textbf{વર્ગ} & \textbf{પ્રકાર} & \textbf{વર્ણન} & \textbf{ઉદાહરણ} \\
\hline
\textbf{ક્ષમતાઓ} & Narrow AI & કાર્ય-વિશિષ્ટ બુદ્ધિ & Siri, Chess programs \\
\cline{2-4}
 & General AI & માનવ-સ્તરની બુદ્ધિ & હજુ પ્રાપ્ત નથી \\
\cline{2-4}
 & Super AI & માનવ બુદ્ધિથી વધુ & સૈદ્ધાંતિક ખ્યાલ \\
\hline
\textbf{કાર્યક્ષમતા} & Reactive & કોઈ યાદદાશ્ત નથી & Deep Blue \\
\cline{2-4}
 & Limited Memory & ભૂતકાળના ડેટાનો ઉપયોગ & Self-driving cars \\
\hline
\end{tabulary}

\begin{mnemonicbox}
"NGS-RLT" (Narrow-General-Super, Reactive-Limited-Theory)
\end{mnemonicbox}

\questionmarks{1}{c}{7}
\textbf{તફાવત આપીને NLU અને NLG સમજાવો.}

\textbf{જવાબ:}

\textbf{Natural Language Understanding (NLU)} અને \textbf{Natural Language Generation (NLG)} Natural Language Processing ના બે મુખ્ય ઘટકો છે.

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{પાસું} & \textbf{NLU} & \textbf{NLG} \\
\hline
\textbf{હેતુ} & માનવી ભાષાને સમજવું & માનવી ભાષા જનરેટ કરવું \\
\hline
\textbf{દિશા} & Input processing & Output generation \\
\hline
\textbf{કાર્ય} & અર્થનું અર્થઘટન & ટેક્સ્ટ રચના \\
\hline
\textbf{પ્રક્રિયા} & વિશ્લેષણ અને સમજ & સંશ્લેષણ અને સર્જન \\
\hline
\textbf{ઉદાહરણો} & Intent recognition, sentiment analysis & Chatbot responses, report generation \\
\hline
\textbf{પડકારો} & અસ્પષ્ટતા નિવારણ & Natural text generation \\
\hline
\end{tabulary}

\textbf{વિગતવાર સમજાવટ:}

\textbf{NLU (Natural Language Understanding):}
\begin{itemize}
    \item Unstructured text ને structured data માં કન્વર્ટ કરે છે
    \item Semantic analysis અને intent extraction કરે છે
    \item અસ્પષ્ટતા અને context ની સમજ હેન્ડલ કરે છે
\end{itemize}

\textbf{NLG (Natural Language Generation):}
\begin{itemize}
    \item Structured data ને natural language માં કન્વર્ટ કરે છે
    \item સુસંગત અને contextually યોગ્ય ટેક્સ્ટ બનાવે છે
    \item વ્યાકરણની ચુસ્તતા અને પ્રવાહિતા સુનિશ્ચિત કરે છે
\end{itemize}

\begin{mnemonicbox}
"UI-OG" (Understanding Input, Output Generation)
\end{mnemonicbox}

\questionmarks{1}{c}{7}
\textbf{આર્ટિફિશિયલ ઈન્ટેલિજન્સનો ઉપયોગ થાય છે તેવા વિવિધ ઉદ્યોગોની યાદી બનાવો અને કોઈપણ બેને સમજાવો.}

\textbf{જવાબ:}

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{ઉદ્યોગ} & \textbf{AI એપ્લિકેશન} & \textbf{લાભો} \\
\hline
\textbf{આરોગ્ય} & નિદાન, દવા શોધ & ચુસ્તતામાં સુધારો \\
\hline
\textbf{ફાઇનાન્સ} & છેતરપિંડી શોધ, ટ્રેડિંગ & જોખમ વ્યવસ્થાપન \\
\hline
\textbf{ઉત્પાદન} & ગુણવત્તા નિયંત્રણ & કાર્યક્ષમતા \\
\hline
\textbf{પરિવહન} & સ્વાયત્ત વાહનો & સુરક્ષા \\
\hline
\textbf{રિટેલ} & સુલેખન સિસ્ટમ & વ્યક્તિગતકરણ \\
\hline
\textbf{શિક્ષણ} & વ્યક્તિગત શિક્ષણ & અનુકૂલન શિક્ષણ \\
\hline
\end{tabulary}

\textbf{બે ઉદ્યોગોની વિગતવાર સમજાવટ:}

\textbf{1. આરોગ્ય ઉદ્યોગ:}
\begin{itemize}
    \item \textbf{તબીબી નિદાન}: AI તબીબી છબીઓ અને દર્દીના ડેટાનું વિશ્લેષણ કરે છે
    \item \textbf{દવા શોધ}: સંભવિત દવાઓની ઝડપી ઓળખ
    \item \textbf{વ્યક્તિગત સારવાર}: દર્દીના genetics આધારે ઉપચાર
    \item \textbf{લાભો}: ઝડપી નિદાન, ભૂલો ઘટાડવી, પરિણામોમાં સુધારો
\end{itemize}

\textbf{2. ફાઇનાન્સ ઉદ્યોગ:}
\begin{itemize}
    \item \textbf{છેતરપિંડી શોધ}: Real-time માં શંકાસ્પદ વ્યવહારો ઓળખવા
    \item \textbf{Algorithmic Trading}: બજારના patterns આધારે automated trading
    \item \textbf{Credit Scoring}: લોન ડિફોલ્ટ જોખમનું ચોક્કસ મૂલ્યાંકન
    \item \textbf{લાભો}: વર્ધેલી સુરક્ષા, ઝડપી પ્રક્રિયા, વધુ સારું જોખમ વ્યવસ્થાપન
\end{itemize}

\begin{mnemonicbox}
"HF-MR-TE" (Healthcare-Finance, Manufacturing-Retail-Transportation-Education)
\end{mnemonicbox}

\questionmarks{2}{a}{3}
\textbf{મશીન લર્નિંગ શબ્દને વ્યાખ્યાયિત કરો. મશીન લર્નિંગનું વર્ગીકરણ રેખાકૃતિ દોરો.}

\textbf{જવાબ:}

\textbf{મશીન લર્નિંગ} AI નો ઉપવિભાગ છે જે કોમ્પ્યુટરોને સ્પષ્ટ રીતે પ્રોગ્રામ કર્યા વિના અનુભવથી શીખવા અને સુધારવા સક્ષમ બનાવે છે. આ ડેટાનું વિશ્લેષણ કરવા, patterns ઓળખવા અને predictions કરવા algorithms નો ઉપયોગ કરે છે.

\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=1.5cm,
    level 1/.style={sibling distance=4.5cm},
    level 2/.style={sibling distance=2.2cm}, 
    edge from parent/.style={draw, -latex}]
    
    \node [gtu block] {Machine Learning}
        child {node [gtu block] {Supervised\\Learning}
            child {node [gtu block] {Classification}}
            child {node [gtu block] {Regression}}
        }
        child {node [gtu block] {Unsupervised\\Learning}
            child {node [gtu block] {Clustering}}
            child {node [gtu block] {Association}}
        }
        child {node [gtu block] {Reinforcement\\Learning}
            child {node [gtu block] {Model-based}}
            child {node [gtu block] {Model-free}}
        };
\end{tikzpicture}
\caption{Classification of Machine Learning}
\end{figure}

\textbf{મુખ્ય પોઈન્ટ્સ:}
\begin{itemize}
    \item \textbf{Supervised}: Labeled training data વાપરે છે
    \item \textbf{Unsupervised}: Unlabeled data માં patterns શોધે છે
    \item \textbf{Reinforcement}: Rewards અને penalties દ્વારા શીખે છે
\end{itemize}

\begin{mnemonicbox}
"SUR" (Supervised-Unsupervised-Reinforcement)
\end{mnemonicbox}

\questionmarks{2}{b}{4}
\textbf{Positive reinforcement અને Negative reinforcement નો તફાવત દર્શાવો.}

\textbf{જવાબ:}

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{પાસું} & \textbf{Positive Reinforcement} & \textbf{Negative Reinforcement} \\
\hline
\textbf{વ્યાખ્યા} & સારા વર્તન માટે રિવોર્ડ ઉમેરવું & અપ્રિય stimulus દૂર કરવું \\
\hline
\textbf{ક્રિયા} & કંઈક આનંદદાયક આપવું & કંઈક અપ્રિય દૂર કરવું \\
\hline
\textbf{હેતુ} & ઇચ્છિત વર્તન વધારવું & ઇચ્છિત વર્તન વધારવું \\
\hline
\textbf{ઉદાહરણ} & સારા પ્રદર્શન માટે બોનસ & જાગ્યા પછી alarm બંધ કરવું \\
\hline
\textbf{અસર} & Rewards દ્વારા પ્રેરણા & રાહત દ્વારા પ્રેરણા \\
\hline
\textbf{Agent પ્રતિસાદ} & ક્રિયા પુનરાવર્તન કરવી & નકારાત્મક પરિણામો ટાળવા \\
\hline
\end{tabulary}

\textbf{મુખ્ય પોઈન્ટ્સ:}
\begin{itemize}
    \item \textbf{Positive Reinforcement}: Positive stimulus ઉમેરીને વર્તન મજબૂત બનાવે છે
    \item \textbf{Negative Reinforcement}: Negative stimulus દૂર કરીને વર્તન મજબૂત બનાવે છે
    \item \textbf{બંને પ્રકાર}: ઇચ્છિત વર્તનની સંભાવના વધારવાનું લક્ષ્ય છે
    \item \textbf{તફાવત}: પ્રોત્સાહનની પદ્ધતિ (ઉમેરવું vs દૂર કરવું)
\end{itemize}

\begin{mnemonicbox}
"AR-RN" (Add Reward, Remove Negative)
\end{mnemonicbox}

\questionmarks{2}{c}{7}
\textbf{Supervised અને Unsupervised learning ની તુલના કરો.}

\textbf{જવાબ:}

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{પેરામીટર} & \textbf{Supervised Learning} & \textbf{Unsupervised Learning} \\
\hline
\textbf{ડેટા પ્રકાર} & Labeled data (input-output pairs) & Unlabeled data (માત્ર inputs) \\
\hline
\textbf{શીખવાનું લક્ષ્ય} & પરિણામોની આગાહ & છુપા patterns શોધવા \\
\hline
\textbf{Feedback} & સાચા જવાબો છે & સાચા જવાબો નથી \\
\hline
\textbf{Algorithms} & SVM, Decision Trees, Neural Networks & K-means, Hierarchical clustering \\
\hline
\textbf{એપ્લિકેશન} & Classification, Regression & Clustering, Association rules \\
\hline
\textbf{ચોકસાઈ} & માપી શકાય છે & માપવી મુશ્કેલ \\
\hline
\textbf{જટિલતા} & ઓછી જટિલ & વધુ જટિલ \\
\hline
\textbf{ઉદાહરણો} & Email spam detection, કિંમત આગાહ & Customer segmentation, Market basket analysis \\
\hline
\end{tabulary}

\textbf{વિગતવાર તુલના:}

\textbf{Supervised Learning:}
\begin{itemize}
    \item જાણીતા પરિણામો સાથે training data ની જરૂર
    \item પ્રદર્શનનું મૂલ્યાંકન સરળતાથી કરી શકાય છે
    \item આગાહીના કાર્યો માટે વપરાય છે
\end{itemize}

\textbf{Unsupervised Learning:}
\begin{itemize}
    \item પૂર્વ-નિર્ધારિત labels વિના ડેટા સાથે કામ કરે છે
    \item ડેટામાં છુપાયેલા structures શોધે છે
    \item અન્વેષણાત્મક ડેટા વિશ્લેષણ માટે વપરાય છે
\end{itemize}

\begin{mnemonicbox}
"LP-PF" (Labeled Prediction, Pattern Finding)
\end{mnemonicbox}

\questionmarks{2}{a}{3}
\textbf{વ્યાખ્યાયિત કરો: Classification, Regression અને clustering.}

\textbf{જવાબ:}

\begin{tabulary}{\linewidth}{|L|L|L|L|}
\hline
\textbf{કાર્ય} & \textbf{વ્યાખ્યા} & \textbf{આઉટપુટ પ્રકાર} & \textbf{ઉદાહરણ} \\
\hline
\textbf{Classification} & Discrete categories/classes ની આગાહ & Categorical & Email: Spam/Not Spam \\
\hline
\textbf{Regression} & સતત આંકડાકીય મૂલ્યોની આગાહ & આંકડાકીય & ઘરની કિંમત આગાહ \\
\hline
\textbf{Clustering} & સમાન ડેટા points ને જૂથ બનાવવા & જૂથો/Clusters & Customer segmentation \\
\hline
\end{tabulary}

\textbf{વિગતવાર વ્યાખ્યાઓ:}
\begin{itemize}
    \item \textbf{Classification}: શીખેલા patterns આધારે input data ને પૂર્વ-નિર્ધારિત વર્ગોમાં સોંપે છે
    \item \textbf{Regression}: સતત મૂલ્યોની આગાહ કરવા variables વચ્ચેના સંબંધોનો અંદાજ કાઢે છે
    \item \textbf{Clustering}: જૂથોની પૂર્વ જાણકારી વિના ડેટામાં કુદરતી જૂથો શોધે છે
\end{itemize}

\begin{mnemonicbox}
"CRC" (Categories, Real numbers, Clusters)
\end{mnemonicbox}

\questionmarks{2}{b}{4}
\textbf{Artificial Neural Network અને Biological Neural Network ની તુલના કરો.}

\textbf{જવાબ:}

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{પાસું} & \textbf{Artificial Neural Network} & \textbf{Biological Neural Network} \\
\hline
\textbf{પ્રોસેસિંગ} & Digital/Binary & Analog \\
\hline
\textbf{ઝડપ} & ઝડપી પ્રોસેસિંગ & ધીમી પ્રોસેસિંગ \\
\hline
\textbf{શીખવું} & Backpropagation algorithm & Synaptic plasticity \\
\hline
\textbf{મેમરી} & અલગ સ્ટોરેજ & કનેક્શનમાં વિતરિત \\
\hline
\textbf{સ્ટ્રક્ચર} & સ્તરવાર આર્કિટેક્ચર & જટિલ 3D structure \\
\hline
\textbf{ખોટ સહન} & ઓછું & વધુ \\
\hline
\textbf{ઊર્જા} & વધુ પાવર consumption & ઓછો ઊર્જા વપરાશ \\
\hline
\textbf{સમાંતર પ્રક્રિયા} & મર્યાદિત parallel processing & વિશાળ parallel processing \\
\hline
\end{tabulary}

\textbf{મુખ્ય તફાવતો:}
\begin{itemize}
    \item \textbf{ANN}: મગજથી પ્રેરિત ગાણિતિક મોડલ
    \item \textbf{Biological}: વાસ્તવિક મગજના neural networks
    \item \textbf{હેતુ}: ANN computation માટે, Biological cognition માટે
    \item \textbf{અનુકૂલનક્ષમતા}: Biological networks વધુ flexible
\end{itemize}

\begin{mnemonicbox}
"DSML-CFEP" (Digital-Speed-Memory-Layer vs Complex-Fault-Energy-Parallel)
\end{mnemonicbox}

\questionmarks{2}{c}{7}
\textbf{Supervised, unsupervised અને reinforcement learning ની વિવિધ applications ની સૂચિ બનાવો.}

\textbf{જવાબ:}

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{Learning પ્રકાર} & \textbf{Applications} & \textbf{વાસ્તવિક જગતના ઉદાહરણો} \\
\hline
\textbf{Supervised} & Email classification, તબીબી નિદાન, Stock prediction, Credit scoring & Gmail spam filter, X-ray analysis, Trading algorithms \\
\hline
\textbf{Unsupervised} & Customer segmentation, Anomaly detection, Data compression & Market research, Fraud detection, Image compression \\
\hline
\textbf{Reinforcement} & Game playing, Robotics, Autonomous vehicles, Resource allocation & AlphaGo, Robot navigation, Self-driving cars \\
\hline
\end{tabulary}

\textbf{વિગતવાર Applications:}

\textbf{Supervised Learning:}
\begin{itemize}
    \item \textbf{Classification}: Spam detection, sentiment analysis, image recognition
    \item \textbf{Regression}: કિંમત આગાહ, હવામાન આગાહ, વેચાણ અંદાજ
\end{itemize}

\textbf{Unsupervised Learning:}
\begin{itemize}
    \item \textbf{Clustering}: Market segmentation, gene sequencing, recommendation systems
    \item \textbf{Association}: Market basket analysis, web usage patterns
\end{itemize}

\textbf{Reinforcement Learning:}
\begin{itemize}
    \item \textbf{Control Systems}: Robot control, traffic management
    \item \textbf{Optimization}: Resource scheduling, portfolio management
\end{itemize}

\begin{mnemonicbox}
"SCR-CRO" (Supervised-Classification-Regression, Unsupervised-Clustering-Association, Reinforcement-Control-Optimization)
\end{mnemonicbox}

\questionmarks{3}{a}{3}
\textbf{સિંગલ લેયર ફોરવર્ડ નેટવર્કને યોગ્ય ડાયાગ્રામ સાથે સમજાવો.}

\textbf{જવાબ:}

\textbf{સિંગલ લેયર ફોરવર્ડ નેટવર્ક} (Perceptron) એ સૌથી સરળ neural network છે જેમાં input અને output વચ્ચે weights નો એક સ્તર હોય છે.

\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=2cm, auto, >=latex]
    % Input nodes
    \node [gtu state] (x1) {$X_1$};
    \node [gtu state, below of=x1] (x2) {$X_2$};
    \node [gtu state, below of=x2] (x3) {$X_3$};
    \node [gtu state, below of=x3] (b) {$b$};
    
    % Summation node
    \node [gtu state, right=3cm of x2] (sum) {$\Sigma$};
    
    % Activation node
    \node [gtu state, right=2cm of sum] (act) {$\phi$};
    
    % Output node
    \node [right=2cm of act] (y) {$Y$};
    
    % Edges
    \path [gtu arrow] (x1) -- node [above] {$W_1$} (sum);
    \path [gtu arrow] (x2) -- node [above] {$W_2$} (sum);
    \path [gtu arrow] (x3) -- node [above] {$W_3$} (sum);
    \path [gtu arrow] (b) -- (sum);
    
    \path [gtu arrow] (sum) -- (act);
    \path [gtu arrow] (act) -- (y);
    
    % Labels
    \node [left of=x2, node distance=1.5cm, rotate=90] {Input Layer};
    \node [below of=sum, node distance=1cm] {Summation};
    \node [below of=act, node distance=1cm] {Activation};
\end{tikzpicture}
\caption{Single Layer Forward Network}
\end{figure}

\textbf{ઘટકો:}
\begin{itemize}
    \item \textbf{Inputs}: X1, X2, X3 (feature values)
    \item \textbf{Weights}: W1, W2, W3 (connection strengths)
    \item \textbf{Bias}: Threshold adjustment માટે વધારાનું parameter
    \item \textbf{Summation}: Inputs નો weighted sum
    \item \textbf{Activation}: Output બનાવવા માટેનું function
\end{itemize}

\textbf{ગાણિતિક સૂત્ર:}
$$Y = f(\Sigma(W_i \times X_i) + b)$$

\begin{mnemonicbox}
"IWSA" (Input-Weight-Sum-Activation)
\end{mnemonicbox}

\questionmarks{3}{b}{4}
\textbf{Backpropagation પર ટૂંકી નોંધ લખો.}

\textbf{જવાબ:}

\textbf{Backpropagation} એ supervised learning algorithm છે જે error calculation આધારે weights adjust કરીને neural networks ને train કરવા માટે વપરાય છે.

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{તબક્કો} & \textbf{વર્ણન} & \textbf{ક્રિયા} \\
\hline
\textbf{Forward Pass} & Input network દ્વારા આગળ વધે છે & Output ની ગણતરી \\
\hline
\textbf{Error Calculation} & Output ને target સાથે compare કરવું & Error/loss શોધવો \\
\hline
\textbf{Backward Pass} & Error પાછળની દિશામાં વધે છે & Weights update કરવા \\
\hline
\textbf{Weight Update} & Gradient વાપરીને weights adjust કરવા & Error ઘટાડવો \\
\hline
\end{tabulary}

\textbf{મુખ્ય લક્ષણો:}
\begin{itemize}
    \item \textbf{Gradient Descent}: Optimal weights શોધવા માટે calculus વાપરે છે
    \item \textbf{Chain Rule}: દરેક weight ના error contribution ની ગણતરી
    \item \textbf{Iterative Process}: Convergence સુધી પુનરાવર્તન
    \item \textbf{Learning Rate}: Weight updates ની ઝડપ નિયંત્રિત કરે છે
\end{itemize}

\textbf{પગલાં:}
\begin{enumerate}
    \item Random weights initialize કરવા
    \item Output મેળવવા forward propagation
    \item Actual અને predicted વચ્ચે error ની ગણતરી
    \item Weights update કરવા backward propagation
\end{enumerate}

\begin{mnemonicbox}
"FCBU" (Forward-Calculate-Backward-Update)
\end{mnemonicbox}

\questionmarks{3}{c}{7}
\textbf{ફીડ ફોરવર્ડ ન્યુરોન નેટવર્કના આર્કિટેક્ચરના components સમજાવો.}

\textbf{જવાબ:}

\textbf{ફીડ ફોરવર્ડ ન્યુરલ નેટવર્ક} અનેક સ્તરો ધરાવે છે જ્યાં માહિતી input થી output સુધી એક દિશામાં વહે છે.

\begin{figure}[H]
\centering
\begin{tikzpicture}[x=1.5cm, y=1.2cm, >=latex]
    % Input Layer
    \foreach \i in {1,2,3}
        \node[gtu state] (I\i) at (0,-\i) {$X_\i$};

    % Hidden Layer
    \foreach \i in {1,2,3}
        \node[gtu state] (H\i) at (2,-\i) {$N_\i$};

    % Output Layer
    \foreach \i in {1,2}
        \node[gtu state] (O\i) at (4,-{\i-0.5}) {$Y_\i$};

    % Connections Input -> Hidden
    \foreach \i in {1,2,3}
        \foreach \j in {1,2,3}
            \draw[gtu arrow] (I\i) -- (H\j);

    % Connections Hidden -> Output
    \foreach \i in {1,2,3}
        \foreach \j in {1,2}
            \draw[gtu arrow] (H\i) -- (O\j);

    % Labels
    \node at (0,0) {\textbf{Input Layer}};
    \node at (2,0) {\textbf{Hidden Layer}};
    \node at (4,0) {\textbf{Output Layer}};
\end{tikzpicture}
\caption{Feed Forward Neural Network Architecture}
\end{figure}

\textbf{ઘટકો:}

\textbf{1. Input Layer:}
\begin{itemize}
    \item Raw data મેળવે છે
    \item કોઈ processing નથી, માત્ર વિતરણ
    \item Neurons ની સંખ્યા = features ની સંખ્યા
\end{itemize}

\textbf{2. Hidden Layer(s):}
\begin{itemize}
    \item Computation અને transformation કરે છે
    \item Activation functions ધરાવે છે
    \item અનેક hidden layers હોઈ શકે છે
\end{itemize}

\textbf{3. Output Layer:}
\begin{itemize}
    \item અંતિમ પરિણામો ઉત્પન્ન કરે છે
    \item Neurons ની સંખ્યા = outputs ની સંખ્યા
    \item Task પ્રકાર માટે યોગ્ય activation વાપરે છે
\end{itemize}

\textbf{4. Weights અને Biases:}
\begin{itemize}
    \item \textbf{Weights}: Neurons વચ્ચેની connection strengths
    \item \textbf{Biases}: Threshold adjustment parameters
\end{itemize}

\textbf{5. Activation Functions:}
\begin{itemize}
    \item Non-linearity દાખલ કરે છે
    \item સામાન્ય પ્રકારો: ReLU, Sigmoid, Tanh
\end{itemize}

\begin{mnemonicbox}
"IHO-WA" (Input-Hidden-Output, Weights-Activation)
\end{mnemonicbox}

\questionmarks{3}{a}{3}
\textbf{મલ્ટિલેયર ફીડ ફોરવર્ડ ANN ને ડાયાગ્રામ સાથે સમજાવો.}

\textbf{જવાબ:}

\textbf{મલ્ટિલેયર ફીડ ફોરવર્ડ ANN} માં input અને output layers વચ્ચે અનેક hidden layers હોય છે, જે જટિલ pattern recognition સક્ષમ બનાવે છે.

\begin{figure}[H]
\centering
\begin{tikzpicture}[x=1.5cm, y=1.2cm, >=latex]
    % Input Layer
    \foreach \i in {1,2}
        \node[gtu state] (I\i) at (0,-\i) {$X_\i$};

    % Hidden Layer 1
    \foreach \i in {1,2}
        \node[gtu state] (H1\i) at (2,-\i) {$H1_\i$};

    % Hidden Layer 2
    \foreach \i in {1,2}
        \node[gtu state] (H2\i) at (4,-\i) {$H2_\i$};
        
    % Output
    \node[gtu state] (O1) at (6,-1.5) {$Y$};

    % Connections
    \foreach \i in {1,2}
        \foreach \j in {1,2}
            \draw[gtu arrow] (I\i) -- (H1\j);
            
    \foreach \i in {1,2}
        \foreach \j in {1,2}
            \draw[gtu arrow] (H1\i) -- (H2\j);
            
    \foreach \i in {1,2}
        \draw[gtu arrow] (H2\i) -- (O1);

    % Labels
    \node at (0,0) {Input};
    \node at (2,0) {Hidden 1};
    \node at (4,0) {Hidden 2};
    \node at (6,0) {Output};
\end{tikzpicture}
\caption{Multilayer Feed Forward ANN}
\end{figure}

\textbf{લક્ષણો:}
\begin{itemize}
    \item \textbf{Deep Architecture}: અનેક hidden layers
    \item \textbf{જટિલ Patterns}: Non-linear relationships શીખી શકે છે
    \item \textbf{Universal Approximator}: કોઈપણ સતત function નો અંદાજ લગાવી શકે છે
\end{itemize}

\begin{mnemonicbox}
"MDC" (Multiple layers, Deep learning, Complex patterns)
\end{mnemonicbox}

\questionmarks{3}{b}{4}
\textbf{સમજાવો 'ReLU એ સૌથી વધુ ઉપયોગમાં લેવાતું Activation function છે.'}

\textbf{જવાબ:}

\textbf{ReLU (Rectified Linear Unit)} તેની સરળતા અને deep networks માં અસરકારકતાને કારણે વ્યાપક રીતે વપરાય છે.

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{ફાયદો} & \textbf{વર્ણન} & \textbf{લાભ} \\
\hline
\textbf{Computational Efficiency} & સરળ max(0,x) operation & ઝડપી processing \\
\hline
\textbf{Gradient Flow} & Positive values માટે vanishing gradient નથી & વધુ સારું learning \\
\hline
\textbf{Sparsity} & Negative inputs માટે zero output & કાર્યક્ષમ representation \\
\hline
\textbf{Non-linearity} & Non-linear વર્તન દાખલ કરે છે & જટિલ pattern learning \\
\hline
\end{tabulary}

\textbf{ગાણિતિક વ્યાખ્યા:}
$$f(x) = \max(0, x)$$

\textbf{અન્ય Functions સાથે તુલના:}
\begin{itemize}
    \item \textbf{vs Sigmoid}: Saturation સમસ્યા નથી, ઝડપી computation
    \item \textbf{vs Tanh}: સરળ ગણતરી, વધુ સારો gradient flow
    \item \textbf{મર્યાદાઓ}: Negative inputs માટે dead neurons સમસ્યા
\end{itemize}

\textbf{સૌથી સામાન્ય કેમ:}
\begin{itemize}
    \item Vanishing gradient સમસ્યા હલ કરે છે
    \item Computationally કાર્યક્ષમ
    \item વ્યવહારમાં સારું કામ કરે છે
    \item Hidden layers માટે default પસંદગી
\end{itemize}

\begin{mnemonicbox}
"CGSN" (Computational, Gradient, Sparsity, Non-linear)
\end{mnemonicbox}

\questionmarks{3}{c}{7}
\textbf{Artificial Neural Network ની સ્ટેપ બાય સ્ટેપ લર્નિંગ પ્રક્રિયા સમજાવો.}

\textbf{જવાબ:}

\textbf{ANN Learning Process} માં prediction error ઘટાડવા માટે iterative weight adjustment સામેલ છે.

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{સ્ટેપ} & \textbf{પ્રક્રિયા} & \textbf{વર્ણન} \\
\hline
\textbf{1. Initialization} & Random weights સેટ કરવા & નાના random values \\
\hline
\textbf{2. Forward Propagation} & Output ની ગણતરી & Input $\rightarrow$ Hidden $\rightarrow$ Output \\
\hline
\textbf{3. Error Calculation} & Target સાથે સરખામણી & Loss function computation \\
\hline
\textbf{4. Backward Propagation} & Gradients ની ગણતરી & Error $\rightarrow$ Hidden $\leftarrow$ Input \\
\hline
\textbf{5. Weight Update} & Parameters adjust કરવા & Gradient descent \\
\hline
\textbf{6. Iteration} & પ્રક્રિયા પુનરાવર્તન & Convergence સુધી \\
\hline
\end{tabulary}

\textbf{વિગતવાર સ્ટેપ્સ:}

\textbf{સ્ટેપ 1: Weights Initialize કરવા}
\begin{itemize}
    \item બધા weights અને biases ને નાના random values સોંપવા
    \item Symmetry breaking સમસ્યા અટકાવે છે
\end{itemize}

\textbf{સ્ટેપ 2: Forward Propagation}
\begin{itemize}
    \item Input data network layers દ્વારા આગળ વહે છે
    \item દરેક neuron weighted sum + activation ની ગણતરી કરે છે
\end{itemize}

\textbf{સ્ટેપ 3: Error ની ગણતરી}
\begin{itemize}
    \item Network output ને desired output સાથે compare કરવું
    \item MSE અથવા Cross-entropy જેવા loss functions વાપરવા
\end{itemize}

\textbf{સ્ટેપ 4: Backward Propagation}
\begin{itemize}
    \item દરેક weight માટે error gradient ની ગણતરી
    \item Error પાછળની દિશામાં propagate કરવા chain rule વાપરવું
\end{itemize}

\textbf{સ્ટેપ 5: Weights Update કરવા}
\begin{itemize}
    \item Gradient descent વાપરીને weights adjust કરવા
    \item New\_weight = Old\_weight - (learning\_rate $\times$ gradient)
\end{itemize}

\textbf{સ્ટેપ 6: પ્રક્રિયા પુનરાવર્તન}
\begin{itemize}
    \item Error converge થાય અથવા maximum epochs સુધી ચાલુ રાખવું
    \item Overfitting ટાળવા validation performance monitor કરવું
\end{itemize}

\begin{mnemonicbox}
"IFEBWI" (Initialize-Forward-Error-Backward-Weight-Iterate)
\end{mnemonicbox}

\questionmarks{4}{a}{3}
\textbf{નેચરલ લેંગ્વેજ પ્રોસેસિંગના વિવિધ ફાયદા અને ગેરફાયદાની યાદી બનાવો.}

\textbf{જવાબ:}

\begin{tabulary}{\linewidth}{|L|L|}
\hline
\textbf{ફાયદા} & \textbf{ગેરફાયદા} \\
\hline
\textbf{Automation} of text processing & \textbf{અસ્પષ્ટતા} in human language \\
\hline
\textbf{24/7 ઉપલબ્ધતા} customer service માટે & \textbf{Context Understanding} પડકારો \\
\hline
\textbf{બહુભાષીય સપોર્ટ} ક્ષમતાઓ & \textbf{સાંસ્કૃતિક સૂક્ષ્મતાઓ} મુશ્કેલી \\
\hline
\textbf{સ્કેલેબિલિટી} મોટા datasets માટે & \textbf{ઉચ્ચ Computational} જરૂરિયાતો \\
\hline
\textbf{સુસંગતતા} responses માં & \textbf{ડેટા ગોપનીયતા} ચિંતાઓ \\
\hline
\textbf{લાગત ઘટાડવી} operations માં & \textbf{મર્યાદિત સર્જનાત્મકતા} responses માં \\
\hline
\end{tabulary}

\textbf{મુખ્ય પોઈન્ટ્સ:}
\begin{itemize}
    \item \textbf{ફાયદા}: કાર્યક્ષમતા, સુલભતા, સુસંગતતા
    \item \textbf{ગેરફાયદા}: જટિલતા, resource જરૂરિયાતો, મર્યાદાઓ
    \item \textbf{સંતુલન}: ઘણી applications માં ફાયદા પડકારો કરતાં વધુ છે
\end{itemize}

\begin{mnemonicbox}
"AMS-ACC" (Automation-Multilingual-Scalability vs Ambiguity-Context-Computational)
\end{mnemonicbox}

\questionmarks{4}{b}{4}
\textbf{NLP માં પ્રી-પ્રોસેસિંગ તકનીકોની સૂચિ બનાવો અને પાયથોન પ્રોગ્રામ વડે કોઈપણ એકને demonstrate કરો.}

\textbf{જવાબ:}

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{તકનીક} & \textbf{હેતુ} & \textbf{ઉદાહરણ} \\
\hline
\textbf{Tokenization} & ટેક્સ્ટને words/sentences માં વિભાજન & "Hello world" $\rightarrow$ ["Hello", "world"] \\
\hline
\textbf{Stop Words Removal} & સામાન્ય શબ્દો દૂર કરવા & "the", "is", "and" દૂર કરવા \\
\hline
\textbf{Stemming} & શબ્દોને root form માં ઘટાડવા & "running" $\rightarrow$ "run" \\
\hline
\textbf{Lemmatization} & Dictionary form માં કન્વર્ટ કરવું & "better" $\rightarrow$ "good" \\
\hline
\textbf{POS Tagging} & Parts of speech ઓળખવા & "run" $\rightarrow$ verb \\
\hline
\textbf{Named Entity Recognition} & Entities ઓળખવા & "Apple" $\rightarrow$ Organization \\
\hline
\end{tabulary}

\textbf{Python પ્રોગ્રામ - Tokenization:}

\begin{lstlisting}[language=Python]
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize

# Sample text
text = "Natural Language Processing અદ્ભુત છે. તે કોમ્પ્યુટરોને માનવી ભાષા સમજવામાં મદદ કરે છે."

# Word tokenization
words = word_tokenize(text)
print("Words:", words)

# Sentence tokenization  
sentences = sent_tokenize(text)
print("Sentences:", sentences)
\end{lstlisting}

\begin{mnemonicbox}
"TSSL-PN" (Tokenization-Stop-Stemming-Lemmatization, POS-NER)
\end{mnemonicbox}

\questionmarks{4}{c}{7}
\textbf{NLP ના phases સમજાવો.}

\textbf{જવાબ:}

\textbf{NLP Phases} natural language ને process અને સમજવા માટેના વ્યવસ્થિત અભિગમને દર્શાવે છે.

\begin{tabulary}{\linewidth}{|L|L|L|L|}
\hline
\textbf{તબક્કો} & \textbf{વર્ણન} & \textbf{પ્રક્રિયા} & \textbf{ઉદાહરણ} \\
\hline
\textbf{Lexical Analysis} & Tokenization અને word identification & ટેક્સ્ટને tokens માં વિભાજન & "હું ખુશ છું" $\rightarrow$ ["હું", "ખુશ", "છું"] \\
\hline
\textbf{Syntactic Analysis} & વ્યાકરણ અને વાક્ય structure & Parse trees, POS tagging & Noun, verb, adjective ઓળખવા \\
\hline
\textbf{Semantic Analysis} & અર્થ extraction & Word sense disambiguation & "બેંક" $\rightarrow$ financial vs નદીનો કિનારો \\
\hline
\textbf{Discourse Integration} & વાક્યો પારના context & Pronouns, references resolve કરવા & "તે" refers to "જોન" \\
\hline
\textbf{Pragmatic Analysis} & Intent અને context understanding & Situation/culture consider કરવું & વ્યંગ, idioms interpretation \\
\hline
\end{tabulary}

\textbf{વિગતવાર સમજાવટ:}

\textbf{1. Lexical Analysis:}
\begin{itemize}
    \item NLP pipeline નો પ્રથમ તબક્કો
    \item Character stream ને tokens માં કન્વર્ટ કરે છે
    \item Punctuation અને special characters દૂર કરે છે
\end{itemize}

\textbf{2. Syntactic Analysis:}
\begin{itemize}
    \item વ્યાકરણનું structure વિશ્લેષણ કરે છે
    \item Parse trees બનાવે છે
    \item વાક્યના ઘટકો ઓળખે છે
\end{itemize}

\textbf{3. Semantic Analysis:}
\begin{itemize}
    \item ટેક્સ્ટમાંથી અર્થ extract કરે છે
    \item શબ્દની અસ્પષ્ટતા handle કરે છે
    \item શબ્દોને concepts સાથે map કરે છે
\end{itemize}

\textbf{4. Discourse Integration:}
\begin{itemize}
    \item વાક્ય સ્તર પાર ટેક્સ્ટનું વિશ્લેષણ કરે છે
    \item વાક્યો પાર context જાળવે છે
    \item References અને connections resolve કરે છે
\end{itemize}

\textbf{5. Pragmatic Analysis:}
\begin{itemize}
    \item વાસ્તવિક જગતનો context consider કરે છે
    \item Speaker નો intent સમજે છે
    \item રૂપક ભાષા handle કરે છે
\end{itemize}

\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=1.8cm, auto, >=latex]
    \node [gtu block] (raw) {Raw Text};
    \node [gtu block, right of=raw, node distance=2.5cm] (lex) {Lexical\\Analysis};
    \node [gtu block, right of=lex, node distance=2.5cm] (syn) {Syntactic\\Analysis};
    \node [gtu block, right of=syn, node distance=2.5cm] (sem) {Semantic\\Analysis};
    
    \node [gtu block, below of=syn] (dis) {Discourse\\Integration};
    \node [gtu block, left of=dis, node distance=2.5cm] (pra) {Pragmatic\\Analysis};
    \node [gtu block, left of=pra, node distance=2.5cm] (und) {Understanding};
    
    \path [gtu arrow] (raw) -- (lex);
    \path [gtu arrow] (lex) -- (syn);
    \path [gtu arrow] (syn) -- (sem);
    \path [gtu arrow] (sem) |- (dis);
    \path [gtu arrow] (dis) -- (pra);
    \path [gtu arrow] (pra) -- (und);
\end{tikzpicture}
\caption{Phases of NLP}
\end{figure}

\begin{mnemonicbox}
"LSSDP" (Lexical-Syntactic-Semantic-Discourse-Pragmatic)
\end{mnemonicbox}

\questionmarks{4}{a}{3}
\textbf{નેચરલ લેંગ્વેજ પ્રોસેસિંગ શું છે? તેની applications ની યાદી બનાવો.}

\textbf{જવાબ:}

\textbf{નેચરલ લેંગ્વેજ પ્રોસેસિંગ (NLP)} AI ની એક શાખા છે જે કોમ્પ્યુટરોને માનવી ભાષાને અર્થપૂર્ણ રીતે સમજવા, અર્થઘટન કરવા અને generate કરવા સક્ષમ બનાવે છે.

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{વર્ગ} & \textbf{Applications} & \textbf{ઉદાહરણો} \\
\hline
\textbf{કોમ્યુનિકેશન} & Chatbots, Virtual assistants & Siri, Alexa, ChatGPT \\
\hline
\textbf{અનુવાદ} & ભાષા અનુવાદ & Google Translate \\
\hline
\textbf{વિશ્લેષણ} & Sentiment analysis, Text mining & Social media monitoring \\
\hline
\textbf{શોધ} & માહિતી પુનઃપ્રાપ્તિ & Search engines \\
\hline
\textbf{લેખન} & વ્યાકરણ તપાસ, Auto-complete & Grammarly, predictive text \\
\hline
\textbf{બિઝનેસ} & દસ્તાવેજ processing, Spam detection & Email filtering \\
\hline
\end{tabulary}

\textbf{મુખ્ય Applications:}
\begin{itemize}
    \item \textbf{મશીન અનુવાદ}: ભાષાઓ વચ્ચે ટેક્સ્ટ કન્વર્ટ કરવું
    \item \textbf{સ્પીચ રેકગ્નિશન}: વાણીને ટેક્સ્ટમાં કન્વર્ટ કરવું
    \item \textbf{ટેક્સ્ટ સમરાઈઝેશન}: સંક્ષિપ્ત સારાંશ બનાવવા
    \item \textbf{પ્રશ્ન જવાબ}: પ્રશ્નોના જવાબો આપવા
\end{itemize}

\begin{mnemonicbox}
"CTAS-WB" (Communication-Translation-Analysis-Search, Writing-Business)
\end{mnemonicbox}

\questionmarks{4}{b}{4}
\textbf{NLTK માં WordNet સાથે કરવામાં આવતા કાર્યોની સૂચિ બનાવો અને python code વડે કોઈપણ એકને demonstrate કરો.}

\textbf{જવાબ:}

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{કાર્ય} & \textbf{વર્ણન} & \textbf{હેતુ} \\
\hline
\textbf{Synsets} & સમાનાર્થી શબ્દો શોધવા & શબ્દ સમાનતા \\
\hline
\textbf{Definitions} & શબ્દના અર્થો મેળવવા & Context સમજવા \\
\hline
\textbf{Examples} & ઉપયોગના ઉદાહરણો & વ્યવહારિક application \\
\hline
\textbf{Hyponyms} & Specific terms શોધવા & વંશવેલો સંબંધો \\
\hline
\textbf{Hypernyms} & સામાન્ય terms શોધવા & Category identification \\
\hline
\textbf{Antonyms} & વિરોધી શબ્દો શોધવા & Contrast analysis \\
\hline
\end{tabulary}

\textbf{Python કોડ - Synsets અને Definitions:}

\begin{lstlisting}[language=Python]
from nltk.corpus import wordnet

# 'સારું' શબ્દ માટે synsets મેળવવા
synsets = wordnet.synsets('good')
print("Synsets:", synsets)

# વ્યાખ્યા મેળવવી
definition = synsets[0].definition()
print("Definition:", definition)

# ઉદાહરણો મેળવવા
examples = synsets[0].examples()
print("Examples:", examples)
\end{lstlisting}

\begin{mnemonicbox}
"SDEHA" (Synsets-Definitions-Examples-Hyponyms-Antonyms)
\end{mnemonicbox}

\questionmarks{4}{c}{7}
\textbf{NLP માં ambiguities ના પ્રકારો સમજાવો.}

\textbf{જવાબ:}

\textbf{NLP Ambiguities} ત્યારે થાય છે જ્યારે ટેક્સ્ટનું અનેક રીતે અર્થઘટન થઈ શકે છે, જે automated understanding માટે પડકારો બનાવે છે.

\begin{tabulary}{\linewidth}{|L|L|L|L|}
\hline
\textbf{પ્રકાર} & \textbf{વર્ણન} & \textbf{ઉદાહરણ} & \textbf{નિવારણ} \\
\hline
\textbf{Lexical} & એક શબ્દના અનેક અર્થ & "બેંક" (financial/નદીનો કિનારો) & Context analysis \\
\hline
\textbf{Syntactic} & અનેક વ્યાકરણ અર્થઘટન & "ઉડતા વિમાનો ખતરનાક હોઈ શકે છે" & Parse trees \\
\hline
\textbf{Semantic} & વાક્ય સ્તરે અનેક અર્થ & "સમય તીરની જેમ ઉડે છે" & Semantic analysis \\
\hline
\textbf{Pragmatic} & Context-આધારિત અર્થઘટન & "શું તમે મીઠું આપી શકશો?" & Situational context \\
\hline
\textbf{Referential} & અસ્પષ્ટ pronoun references & "જોને બોબને કહ્યું તે ખોટો હતો" & Discourse analysis \\
\hline
\end{tabulary}

\textbf{વિગતવાર સમજાવટ:}

\textbf{1. Lexical Ambiguity:}
\begin{itemize}
    \item એક જ શબ્દ, વિવિધ અર્થો
    \item Homonyms અને polysemes
    \item ઉદાહરણ: "બેટ" (પ્રાણી/રમત સાધન)
\end{itemize}

\textbf{2. Syntactic Ambiguity:}
\begin{itemize}
    \item અનેક વ્યાકરણ structures
    \item વિવિધ parse trees શક્ય
    \item ઉદાહરણ: "મેં દૂરબીન સાથે એક માણસને જોયો"
\end{itemize}

\textbf{3. Semantic Ambiguity:}
\begin{itemize}
    \item વાક્ય-સ્તરે અર્થની ગૂંચવણ
    \item અનેક અર્થઘટન શક્ય
    \item ઉદાહરણ: "સંબંધીઓની મુલાકાત કંટાળાજનક હોઈ શકે છે"
\end{itemize}

\textbf{4. Pragmatic Ambiguity:}
\begin{itemize}
    \item Context અને intent આધારિત
    \item સાંસ્કૃતિક અને પરિસ્થિતિગત પરિબળો
    \item ઉદાહરણ: વ્યંગ અને indirect requests
\end{itemize}

\textbf{5. Referential Ambiguity:}
\begin{itemize}
    \item Entities ના અસ્પષ્ટ references
    \item Pronoun resolution પડકારો
    \item ઉદાહરણ: અનેક શક્ય antecedents
\end{itemize}

\textbf{નિવારણ વ્યૂહરચનાઓ:}
\begin{itemize}
    \item Context analysis અને machine learning
    \item આંકડાકીય disambiguation પદ્ધતિઓ
    \item Knowledge bases અને ontologies
\end{itemize}

\begin{mnemonicbox}
"LSSPR" (Lexical-Syntactic-Semantic-Pragmatic-Referential)
\end{mnemonicbox}

\questionmarks{5}{a}{3}
\textbf{બેગ ઓફ વર્ડ્સને ઉદાહરણ સાથે સમજાવો.}

\textbf{જવાબ:}

\textbf{બેગ ઓફ વર્ડ્સ (BoW)} એ ટેક્સ્ટ પ્રતિનિધિત્વ પદ્ધતિ છે જે શબ્દની આવર્તન આધારે ટેક્સ્ટને આંકડાકીય vectors માં કન્વર્ટ કરે છે, વ્યાકરણ અને શબ્દ ક્રમને અવગણીને.

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{સ્ટેપ} & \textbf{પ્રક્રિયા} & \textbf{વર્ણન} \\
\hline
\textbf{1. Tokenization} & ટેક્સ્ટને શબ્દોમાં વિભાજન & Vocabulary બનાવવી \\
\hline
\textbf{2. Vocabulary Creation} & અનન્ય શબ્દોનો સંગ્રહ & Terms નો શબ્દકોશ \\
\hline
\textbf{3. Vector Creation} & શબ્દ આવર્તન ગણવી & આંકડાકીય પ્રતિનિધિત્વ \\
\hline
\end{tabulary}

\textbf{ઉદાહરણ:}

દસ્તાવેજો:
\begin{itemize}
    \item Doc1: "હું મશીન લર્નિંગ પસંદ કરું છું"
    \item Doc2: "મશીન લર્નિંગ અદ્ભુત છે"
\end{itemize}

\textbf{Vocabulary:} [હું, પસંદ, મશીન, લર્નિંગ, અદ્ભુત, છે, કરું, છું]

\textbf{BoW Vectors:}
\begin{itemize}
    \item Doc1: [1, 1, 1, 1, 0, 0, 1, 1]
    \item Doc2: [0, 0, 1, 1, 1, 1, 0, 0]
\end{itemize}

\textbf{લક્ષણો:}
\begin{itemize}
    \item \textbf{ક્રમ સ્વતંત્ર}: શબ્દ ક્રમ અવગણવામાં આવે છે
    \item \textbf{આવર્તન આધારિત}: શબ્દ occurrences ગણે છે
    \item \textbf{Sparse Representation}: ઘણા શૂન્ય મૂલ્યો
\end{itemize}

\begin{mnemonicbox}
"TVC" (Tokenize-Vocabulary-Count)
\end{mnemonicbox}

\questionmarks{5}{b}{4}
\textbf{Word2Vec શું છે? તેના steps સમજાવો.}

\textbf{જવાબ:}

\textbf{Word2Vec} એ neural network-આધારિત તકનીક છે જે મોટા text corpora માં તેમના context થી શીખીને શબ્દોના dense vector representations બનાવે છે.

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{મોડલ} & \textbf{અભિગમ} & \textbf{આગાહી} \\
\hline
\textbf{CBOW} & Continuous Bag of Words & સંદર્ભ $\rightarrow$ લક્ષ્ય શબ્દ \\
\hline
\textbf{Skip-gram} & Skip-gram with Negative Sampling & લક્ષ્ય શબ્દ $\rightarrow$ સંદર્ભ \\
\hline
\end{tabulary}

\textbf{Word2Vec ના સ્ટેપ્સ:}

\textbf{1. ડેટા તૈયારી:}
\begin{itemize}
    \item મોટો text corpus એકત્ર કરવો
    \item ટેક્સ્ટ સાફ કરવું અને preprocess કરવું
    \item Training pairs બનાવવા
\end{itemize}

\textbf{2. મોડલ આર્કિટેક્ચર:}
\begin{itemize}
    \item Input layer (one-hot encoded words)
    \item Hidden layer (embedding layer)
    \item Output layer (prediction માટે softmax)
\end{itemize}

\textbf{3. Training પ્રક્રિયા:}
\begin{itemize}
    \item \textbf{CBOW}: context થી target word ની આગાહ
    \item \textbf{Skip-gram}: target word થી context ની આગાહ
    \item Weights update કરવા backpropagation વાપરવું
\end{itemize}

\textbf{4. Vector Extraction:}
\begin{itemize}
    \item Hidden layer થી weight matrix extract કરવું
    \item દરેક row word embedding દર્શાવે છે
    \item સામાન્યતઃ 100-300 dimensions
\end{itemize}

\textbf{લાભો:}
\begin{itemize}
    \item Semantic relationships કેપ્ચર કરે છે
    \item સમાન શબ્દોના સમાન vectors હોય છે
    \item Arithmetic operations સપોર્ટ કરે છે (રાજા - પુરુષ + સ્ત્રી = રાણી)
\end{itemize}

\begin{mnemonicbox}
"DMAT" (Data-Model-Architecture-Training)
\end{mnemonicbox}

\questionmarks{5}{c}{7}
\textbf{NLP ની applications ની યાદી બનાવો અને કોઈપણ એકને વિગતવાર સમજાવો.}

\textbf{જવાબ:}

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textbf{Application} & \textbf{વર્ણન} & \textbf{ઉદ્યોગ ઉપયોગ} \\
\hline
\textbf{મશીન અનુવાદ} & ભાષા રૂપાંતરણ & વૈશ્વિક કોમ્યુનિકેશન \\
\hline
\textbf{Sentiment Analysis} & મત ખનન & Social media monitoring \\
\hline
\textbf{Chatbots} & વાતચીત AI & Customer service \\
\hline
\textbf{ટેક્સ્ટ સારાંશ} & સામગ્રી સંકુચન & સમાચાર, સંશોધન \\
\hline
\textbf{સ્પીચ રેકગ્નિશન} & અવાજ થી ટેક્સ્ટ & Virtual assistants \\
\hline
\textbf{માહિતી નિષ્કર્ષણ} & ટેક્સ્ટમાંથી ડેટા માઇનિંગ & Business intelligence \\
\hline
\textbf{પ્રશ્ન જવાબ} & સ્વચાલિત પ્રતિસાદો & Search engines \\
\hline
\textbf{સ્પામ શોધ} & Email filtering & Cybersecurity \\
\hline
\end{tabulary}

\textbf{વિગતવાર સમજાવટ: Sentiment Analysis}

\textbf{Sentiment Analysis} એ ટેક્સ્ટ ડેટામાં વ્યક્ત કરાયેલ ભાવનાત્મક tone અને મંતવ્યો નક્કી કરવાની પ્રક્રિયા છે.

\textbf{ઘટકો:}
\begin{itemize}
    \item \textbf{ટેક્સ્ટ પ્રીપ્રોસેસિંગ}: સફાઈ અને tokenization
    \item \textbf{Feature Extraction}: TF-IDF, word embeddings
    \item \textbf{વર્ગીકરણ}: સકારાત્મક, નકારાત્મક, તટસ્થ
    \item \textbf{વિશ્વાસ સ્કોરિંગ}: Sentiment ની મજબૂતાઈ
\end{itemize}

\textbf{પ્રક્રિયા સ્ટેપ્સ:}
\begin{enumerate}
    \item \textbf{ડેટા સંગ્રહ}: Reviews, social media થી ટેક્સ્ટ એકત્ર કરવું
    \item \textbf{પ્રીપ્રોસેસિંગ}: Noise દૂર કરવો, ટેક્સ્ટ normalize કરવું
    \item \textbf{Feature Engineering}: ટેક્સ્ટને આંકડાકીય features માં કન્વર્ટ કરવું
    \item \textbf{મોડલ ટ્રેનિંગ}: વર્ગીકરણ માટે ML algorithms વાપરવા
    \item \textbf{આગાહી}: નવા ટેક્સ્ટના sentiment ની વર્ગીકરણ
    \item \textbf{મૂલ્યાંકન}: ચોકસાઈ અને કામગીરી માપવી
\end{enumerate}

\textbf{Applications:}
\begin{itemize}
    \item \textbf{બ્રાન્ડ મોનિટરિંગ}: ગ્રાહક મંતવ્યોને ટ્રેક કરવા
    \item \textbf{પ્રોડક્ટ રિવ્યૂ}: ગ્રાહક પ્રતિસાદનું વિશ્લેષણ
    \item \textbf{સોશિયલ મીડિયા}: જાહેર sentiment ને monitor કરવું
    \item \textbf{બજાર સંશોધન}: ગ્રાહક પસંદગીઓ સમજવી
\end{itemize}

\begin{mnemonicbox}
"MSCTSIQ-S" (Machine-Sentiment-Chatbot-Text-Speech-Information-Question-Spam)
\end{mnemonicbox}

\questionmarks{5}{a}{3}
\textbf{ઉદાહરણ સાથે TFIDF સમજાવો.}

\textbf{જવાબ:}

\textbf{TF-IDF (Term Frequency-Inverse Document Frequency)} દસ્તાવેજોના સંગ્રહ સાપેક્ષે દસ્તાવેજમાં શબ્દની મહત્ત્વતા માપે છે.

\textbf{સૂત્ર:}
$$TF-IDF = TF(t,d) \times IDF(t)$$

જ્યાં:
\begin{itemize}
    \item TF(t,d) = (દસ્તાવેજ d માં term t કેટલી વખત આવે છે) / (દસ્તાવેજ d માં કુલ terms)
    \item IDF(t) = log(કુલ દસ્તાવેજો / term t ધરાવતા દસ્તાવેજો)
\end{itemize}

\textbf{ઉદાહરણ:}

દસ્તાવેજો:
\begin{itemize}
    \item Doc1: "મશીન લર્નિંગ સારું છે"
    \item Doc2: "લર્નિંગ algorithms સારા છે"
    \item Doc3: "મશીન algorithms સારું કામ કરે છે"
\end{itemize}

\textbf{ટેબલ: "મશીન" માટે TF-IDF ગણતરી}

\begin{tabulary}{\linewidth}{|L|L|L|L|}
\hline
\textbf{દસ્તાવેજ} & \textbf{TF} & \textbf{IDF} & \textbf{TF-IDF} \\
\hline
Doc1 & 1/4 = 0.25 & log(3/2) = 0.18 & 0.25 $\times$ 0.18 = 0.045 \\
\hline
Doc2 & 0/4 = 0 & log(3/2) = 0.18 & 0 $\times$ 0.18 = 0 \\
\hline
Doc3 & 1/4 = 0.25 & log(3/2) = 0.18 & 0.25 $\times$ 0.18 = 0.045 \\
\hline
\end{tabulary}

\textbf{મુખ્ય પોઈન્ટ્સ:}
\begin{itemize}
    \item \textbf{ઉચ્ચ TF-IDF}: વિશિષ્ટ દસ્તાવેજમાં મહત્વપૂર્ણ શબ્દ
    \item \textbf{નીચો TF-IDF}: દસ્તાવેજો પાર સામાન્ય શબ્દ
    \item \textbf{Applications}: માહિતી પુનઃપ્રાપ્તિ, text mining
\end{itemize}

\begin{mnemonicbox}
"TI-FD" (Term frequency, Inverse Document frequency)
\end{mnemonicbox}

\questionmarks{5}{b}{4}
\textbf{TFIDF અને BOW સાથેના challenges વિશે સમજાવો.}

\textbf{જવાબ:}

\begin{tabulary}{\linewidth}{|L|L|L|L|}
\hline
\textbf{પડકાર} & \textbf{TF-IDF} & \textbf{BOW} & \textbf{અસર} \\
\hline
\textbf{Semantic Understanding} & અર્થ કેપ્ચર કરી શકતું નથી & શબ્દ સંબંધો અવગણે છે & Context ની ખરાબ સમજ \\
\hline
\textbf{શબ્દ ક્રમ} & સ્થિતિ અવગણે છે & ક્રમ ખોવાઈ જાય છે & વ્યાકરણ અર્થ ખોવાઈ જાય છે \\
\hline
\textbf{Sparsity} & ઉચ્ચ-dimensional vectors & ઘણા શૂન્ય મૂલ્યો & મેમરી અકાર્યક્ષમ \\
\hline
\textbf{Vocabulary માપ} & મોટો feature space & Corpus સાથે વધે છે & Computational જટિલતા \\
\hline
\textbf{Out-of-Vocabulary} & અજાણ્યા શબ્દો અવગણે છે & નવા શબ્દો handle કરતું નથી & મર્યાદિત સામાન્યીકરણ \\
\hline
\textbf{Polysemy} & અનેક અર્થો & વિવિધ senses માટે સમાન વર્તન & અસ્પષ્ટતા સમસ્યાઓ \\
\hline
\end{tabulary}

\textbf{વિગતવાર પડકારો:}

\textbf{1. Semantic Understanding નો અભાવ:}
\begin{itemize}
    \item શબ્દો સ્વતંત્ર features તરીકે ગણાય છે
    \item Synonyms અથવા સંબંધિત concepts સમજી શકતા નથી
    \item "સારું" અને "ઉત્તમ" અલગ રીતે ગણાય છે
\end{itemize}

\textbf{2. શબ્દ ક્રમ ખોવાઈ જવો:}
\begin{itemize}
    \item "કુતરો માણસને કરડે છે" vs "માણસ કુતરાને કરડે છે" સમાન representation
    \item Context અને વ્યાકરણની માહિતી ખોવાઈ જાય છે
    \item વાક્ય structure અવગણવામાં આવે છે
\end{itemize}

\textbf{3. ઉચ્ચ Dimensionality:}
\begin{itemize}
    \item Vector size vocabulary size ની બરાબર
    \item મોટાભાગે શૂન્યો સાથે sparse matrices
    \item Storage અને computation સમસ્યાઓ
\end{itemize}

\textbf{4. Context Insensitivity:}
\begin{itemize}
    \item એક જ શબ્દ વિવિધ contexts માં સમાન ગણાય છે
    \item "Apple" કંપની vs ફળ સમાન representation
    \item Polysemy અને homonymy સમસ્યાઓ
\end{itemize}

\textbf{ઉકેલો:}
\begin{itemize}
    \item \textbf{Word Embeddings}: Word2Vec, GloVe
    \item \textbf{Contextual Models}: BERT, GPT
    \item \textbf{N-grams}: કેટલાક શબ્દ ક્રમ કેપ્ચર કરવા
    \item \textbf{Dimensionality Reduction}: PCA, SVD
\end{itemize}

\begin{mnemonicbox}
"SSVO-CP" (Semantic-Sequence-Vocabulary-OOV, Context-Polysemy)
\end{mnemonicbox}

\questionmarks{5}{c}{7}
\textbf{GloVe ની કામગીરી સમજાવો.}

\textbf{જવાબ:}

\textbf{GloVe (Global Vectors for Word Representation)} word embeddings બનાવવા માટે global આંકડાકીય માહિતીને local context windows સાથે જોડે છે.

\begin{tabulary}{\linewidth}{|L|L|L|L|}
\hline
\textbf{પાસું} & \textbf{GloVe} & \textbf{Word2Vec} & \textbf{પરંપરાગત પદ્ધતિઓ} \\
\hline
\textbf{અભિગમ} & Global + Local statistics & Local context windows & આવર્તન-આધારિત \\
\hline
\textbf{Training} & Matrix factorization & Neural networks & ગણતરી પદ્ધતિઓ \\
\hline
\textbf{કાર્યક્ષમતા} & ઝડપી training & ધીમી training & ખૂબ ઝડપી \\
\hline
\textbf{પ્રદર્શન} & ઉચ્ચ ચોકસાઈ & સારી ચોકસાઈ & મર્યાદિત પ્રદર્શન \\
\hline
\end{tabulary}

\textbf{કામગીરી પ્રક્રિયા:}

\textbf{1. Co-occurrence Matrix Construction:}
\begin{itemize}
    \item Context windows માં શબ્દ co-occurrences ગણવા
    \item Global statistics matrix બનાવવો
    \item $X_{ij}$ = word j, word i ના context માં કેટલી વખત આવે છે
\end{itemize}

\textbf{2. Ratio ગણતરી:}
\begin{itemize}
    \item સંભાવના ratios ની ગણતરી
    \item $P(k|i) = X_{ik} / X_i$ (word i આપવામાં આવે તો word k ની સંભાવના)
    \item સંભાવનાઓ વચ્ચેના અર્થપૂર્ણ ratios પર ધ્યાન
\end{itemize}

\textbf{3. Objective Function:}
\begin{itemize}
    \item Weighted least squares objective minimize કરવું
    \item $J = \sum f(X_{ij})(w_i^T w_j + b_i + b_j - \log X_{ij})^2$
    \item જ્યાં $f(x)$ weighting function છે
\end{itemize}

\textbf{4. Vector Learning:}
\begin{itemize}
    \item Objective optimize કરવા gradient descent વાપરવું
    \item Word vectors $w_i$ અને context vectors $w_j$ શીખવા
    \item અંતિમ representation બંને vectors combine કરે છે
\end{itemize}

\textbf{મુખ્ય લક્ષણો:}

\textbf{Global Statistics:}
\begin{itemize}
    \item સમગ્ર corpus માહિતી વાપરે છે
    \item Global word relationships કેપ્ચર કરે છે
    \item Local પદ્ધતિઓ કરતાં વધુ સ્થિર
\end{itemize}

\textbf{કાર્યક્ષમતા:}
\begin{itemize}
    \item Co-occurrence statistics પર train કરે છે
    \item Neural network પદ્ધતિઓ કરતાં ઝડપી
    \item મોટા corpora માટે scalable
\end{itemize}

\textbf{પ્રદર્શન:}
\begin{itemize}
    \item Analogy tasks પર સારું પ્રદર્શન
    \item Semantic અને syntactic બંને સંબંધો કેપ્ચર કરે છે
    \item Similarity tasks પર સારી કામગીરી
\end{itemize}

\textbf{ગાણિતિક આધાર:}
$$J = \sum_{i,j=1}^{V} f(X_{ij})(w_i^T w_j + b_i + b_j - \log X_{ij})^2$$

જ્યાં:
\begin{itemize}
    \item V = vocabulary size
    \item $X_{ij}$ = co-occurrence count
    \item $w_i, w_j$ = word vectors
    \item $b_i, b_j$ = bias terms
    \item $f(x)$ = weighting function
\end{itemize}

\textbf{ફાયદા:}
\begin{itemize}
    \item \textbf{લાભો સંયોજન}: Global statistics + local context
    \item \textbf{સમજી શકાય તેવું}: સ્પષ્ટ ગાણિતિક આધાર
    \item \textbf{કાર્યક્ષમ}: Word2Vec કરતાં ઝડપી training
    \item \textbf{અસરકારક}: વિવિધ tasks પર સારી કામગીરી
\end{itemize}

\textbf{Applications:}
\begin{itemize}
    \item \textbf{શબ્દ સમાનતા}: સંબંધિત શબ્દો શોધવા
    \item \textbf{Analogy Tasks}: રાજા - પુરુષ + સ્ત્રી = રાણી
    \item \textbf{ટેક્સ્ટ વર્ગીકરણ}: Feature representation
    \item \textbf{મશીન અનુવાદ}: Cross-lingual mappings
\end{itemize}

\begin{mnemonicbox}
"CROF-PGAE" (Co-occurrence-Ratio-Objective-Function, Performance-Global-Advantage-Efficiency)
\end{mnemonicbox}

\end{document}
