\documentclass[10pt,a4paper]{article}
\input{../../../../../../latex-templates/gtu-solutions/preamble.tex}
\input{../../../../../../latex-templates/gtu-solutions/gujarati-boxes.tex}

\begin{document}

\begin{center}
{\Huge\bfseries\color{headcolor} Subject Name (Gujarati)}\\[5pt]
{\LARGE 4351601 -- Summer 2025}\\[3pt]
{\large Semester 1 Study Material}\\[3pt]
{\normalsize\textit{Detailed Solutions and Explanations}}
\end{center}

\vspace{10pt}

\subsection*{પ્રશ્ન 1(અ) [3
ગુણ]}\label{uxaaauxab0uxab6uxaa8-1uxa85-3-uxa97uxaa3}

\textbf{વર્ડ એમ્બેડિંગ ટેકનિક શું છે? વિવિધ વર્ડ એમ્બેડિંગ તકનીકોની સૂચિ બનાવો.}

\begin{solutionbox}

\textbf{વર્ડ એમ્બેડિંગ} એ એવી તકનીક છે જે શબ્દોને આંકડાકીય vectors માં રૂપાંતરિત કરે
છે અને શબ્દો વચ્ચેના semantic સંબંધોને જાળવી રાખે છે. આ શબ્દોને high-dimensional
space માં dense vectors તરીકે દર્શાવે છે.

\textbf{ટેબલ: વિવિધ વર્ડ એમ્બેડિંગ તકનીકો}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2759}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2759}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4483}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
તકનીક
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
વર્ણન
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
મુખ્ય લક્ષણ
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{TF-IDF} & Term Frequency-Inverse Document Frequency & આંકડાકીય
માપદંડ \\
\textbf{Bag of Words (BoW)} & આવર્તન-આધારિત રજૂઆત & સરળ ગણતરી પદ્ધતિ \\
\textbf{Word2Vec} & Neural network-આધારિત embedding & Semantic સંબંધો કેપ્ચર
કરે \\
\textbf{GloVe} & Global Vectors for word representation & Global અને
local આંકડા સંયોજન \\
\end{longtable}
}

\textbf{મુખ્ય પોઈન્ટ્સ:}

\begin{itemize}
\tightlist
\item
  \textbf{TF-IDF}: દસ્તાવેજોમાં શબ્દની મહત્ત્વતા માપે છે
\item
  \textbf{BoW}: Vocabulary-આધારિત vectors બનાવે છે
\item
  \textbf{Word2Vec}: CBOW અને Skip-gram models વાપરે છે
\item
  \textbf{GloVe}: Global context સાથે pre-trained embeddings
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``TB-WG'' (TF-IDF, BoW, Word2Vec, GloVe)

\end{mnemonicbox}
\subsection*{પ્રશ્ન 1(બ) [4
ગુણ]}\label{uxaaauxab0uxab6uxaa8-1uxaac-4-uxa97uxaa3}

\textbf{આર્ટિફિશિયલ ઇન્ટેલિજન્સના વિવિધ પ્રકારોનું વર્ગીકરણ કરો અને તેને ડાયાગ્રામ
વડે દર્શાવો.}

\begin{solutionbox}

AI ને \textbf{ક્ષમતાઓ} અને \textbf{કાર્યક્ષમતા} આધારે વર્ગીકૃત કરી શકાય છે.

\textbf{ડાયાગ્રામ:}

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{mermaid-d51e5e5a.pdf}

\textbf{ટેબલ: AI પ્રકારોની તુલના}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
વર્ગ & પ્રકાર & વર્ણન & ઉદાહરણ \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{ક્ષમતાઓ} & Narrow AI & કાર્ય-વિશિષ્ટ બુદ્ધિ & Siri, Chess programs \\
& General AI & માનવ-સ્તરની બુદ્ધિ & હજુ પ્રાપ્ત નથી \\
& Super AI & માનવ બુદ્ધિથી વધુ & સૈદ્ધાંતિક ખ્યાલ \\
\textbf{કાર્યક્ષમતા} & Reactive & કોઈ યાદદાશ્ત નથી & Deep Blue \\
& Limited Memory & ભૂતકાળના ડેટાનો ઉપયોગ & Self-driving cars \\
\end{longtable}
}

\end{solutionbox}
\begin{mnemonicbox}
``NGS-RLT'' (Narrow-General-Super,
Reactive-Limited-Theory)

\end{mnemonicbox}
\subsection*{પ્રશ્ન 1(ક) [7
ગુણ]}\label{uxaaauxab0uxab6uxaa8-1uxa95-7-uxa97uxaa3}

\textbf{તફાવત આપીને NLU અને NLG સમજાવો.}

\begin{solutionbox}

\textbf{Natural Language Understanding (NLU)} અને \textbf{Natural
Language Generation (NLG)} Natural Language Processing ના બે મુખ્ય ઘટકો છે.

\textbf{ટેબલ: NLU vs NLG તુલના}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3750}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3125}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3125}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
પાસું
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NLU
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
NLG
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{હેતુ} & માનવી ભાષાને સમજવું & માનવી ભાષા જનરેટ કરવું \\
\textbf{દિશા} & Input processing & Output generation \\
\textbf{કાર્ય} & અર્થનું અર્થઘટન & ટેક્સ્ટ રચના \\
\textbf{પ્રક્રિયા} & વિશ્લેષણ અને સમજ & સંશ્લેષણ અને સર્જન \\
\textbf{ઉદાહરણો} & Intent recognition, sentiment analysis & Chatbot
responses, report generation \\
\textbf{પડકારો} & અસ્પષ્ટતા નિવારણ & Natural text generation \\
\end{longtable}
}

\textbf{વિગતવાર સમજાવટ:}

\textbf{NLU (Natural Language Understanding):}

\begin{itemize}
\tightlist
\item
  Unstructured text ને structured data માં કન્વર્ટ કરે છે
\item
  Semantic analysis અને intent extraction કરે છે
\item
  અસ્પષ્ટતા અને context ની સમજ હેન્ડલ કરે છે
\end{itemize}

\textbf{NLG (Natural Language Generation):}

\begin{itemize}
\tightlist
\item
  Structured data ને natural language માં કન્વર્ટ કરે છે
\item
  સુસંગત અને contextually યોગ્ય ટેક્સ્ટ બનાવે છે
\item
  વ્યાકરણની ચુસ્તતા અને પ્રવાહિતા સુનિશ્ચિત કરે છે
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``UI-OG'' (Understanding Input, Output Generation)

\end{mnemonicbox}
\subsection*{પ્રશ્ન 1(ક) OR [7
ગુણ]}\label{uxaaauxab0uxab6uxaa8-1uxa95-or-7-uxa97uxaa3}

\textbf{આર્ટિફિશિયલ ઈન્ટેલિજન્સનો ઉપયોગ થાય છે તેવા વિવિધ ઉદ્યોગોની યાદી બનાવો
અને કોઈપણ બેને સમજાવો.}

\begin{solutionbox}

\textbf{ટેબલ: ઉદ્યોગોમાં AI એપ્લિકેશન}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
ઉદ્યોગ & AI એપ્લિકેશન & લાભો \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{આરોગ્ય} & નિદાન, દવા શોધ & ચુસ્તતામાં સુધારો \\
\textbf{ફાઇનાન્સ} & છેતરપિંડી શોધ, ટ્રેડિંગ & જોખમ વ્યવસ્થાપન \\
\textbf{ઉત્પાદન} & ગુણવત્તા નિયંત્રણ & કાર્યક્ષમતા \\
\textbf{પરિવહન} & સ્વાયત્ત વાહનો & સુરક્ષા \\
\textbf{રિટેલ} & સુલેખન સિસ્ટમ & વ્યક્તિગતકરણ \\
\textbf{શિક્ષણ} & વ્યક્તિગત શિક્ષણ & અનુકૂલન શિક્ષણ \\
\end{longtable}
}

\textbf{બે ઉદ્યોગોની વિગતવાર સમજાવટ:}

\textbf{1. આરોગ્ય ઉદ્યોગ:}

\begin{itemize}
\tightlist
\item
  \textbf{તબીબી નિદાન}: AI તબીબી છબીઓ અને દર્દીના ડેટાનું વિશ્લેષણ કરે છે
\item
  \textbf{દવા શોધ}: સંભવિત દવાઓની ઝડપી ઓળખ
\item
  \textbf{વ્યક્તિગત સારવાર}: દર્દીના genetics આધારે ઉપચાર
\item
  \textbf{લાભો}: ઝડપી નિદાન, ભૂલો ઘટાડવી, પરિણામોમાં સુધારો
\end{itemize}

\textbf{2. ફાઇનાન્સ ઉદ્યોગ:}

\begin{itemize}
\tightlist
\item
  \textbf{છેતરપિંડી શોધ}: Real-time માં શંકાસ્પદ વ્યવહારો ઓળખવા
\item
  \textbf{Algorithmic Trading}: બજારના patterns આધારે automated trading
\item
  \textbf{Credit Scoring}: લોન ડિફોલ્ટ જોખમનું ચોક્કસ મૂલ્યાંકન
\item
  \textbf{લાભો}: વર્ધેલી સુરક્ષા, ઝડપી પ્રક્રિયા, વધુ સારું જોખમ વ્યવસ્થાપન
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``HF-MR-TE'' (Healthcare-Finance,
Manufacturing-Retail-Transportation-Education)

\end{mnemonicbox}
\subsection*{પ્રશ્ન 2(અ) [3
ગુણ]}\label{uxaaauxab0uxab6uxaa8-2uxa85-3-uxa97uxaa3}

\textbf{મશીન લર્નિંગ શબ્દને વ્યાખ્યાયિત કરો. મશીન લર્નિંગનું વર્ગીકરણ રેખાકૃતિ દોરો.}

\begin{solutionbox}

\textbf{મશીન લર્નિંગ} AI નો ઉપવિભાગ છે જે કોમ્પ્યુટરોને સ્પષ્ટ રીતે પ્રોગ્રામ કર્યા
વિના અનુભવથી શીખવા અને સુધારવા સક્ષમ બનાવે છે. આ ડેટાનું વિશ્લેષણ કરવા, patterns
ઓળખવા અને predictions કરવા algorithms નો ઉપયોગ કરે છે.

\textbf{ડાયાગ્રામ:}

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{mermaid-6488ef74.pdf}

\textbf{મુખ્ય પોઈન્ટ્સ:}

\begin{itemize}
\tightlist
\item
  \textbf{Supervised}: Labeled training data વાપરે છે
\item
  \textbf{Unsupervised}: Unlabeled data માં patterns શોધે છે
\item
  \textbf{Reinforcement}: Rewards અને penalties દ્વારા શીખે છે
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``SUR'' (Supervised-Unsupervised-Reinforcement)

\end{mnemonicbox}
\subsection*{પ્રશ્ન 2(બ) [4
ગુણ]}\label{uxaaauxab0uxab6uxaa8-2uxaac-4-uxa97uxaa3}

\textbf{Positive reinforcement અને Negative reinforcement નો તફાવત
દર્શાવો.}

\begin{solutionbox}

\textbf{ટેબલ: Positive vs Negative Reinforcement}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
પાસું & Positive Reinforcement & Negative Reinforcement \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{વ્યાખ્યા} & સારા વર્તન માટે રિવોર્ડ ઉમેરવું & અપ્રિય stimulus દૂર કરવું \\
\textbf{ક્રિયા} & કંઈક આનંદદાયક આપવું & કંઈક અપ્રિય દૂર કરવું \\
\textbf{હેતુ} & ઇચ્છિત વર્તન વધારવું & ઇચ્છિત વર્તન વધારવું \\
\textbf{ઉદાહરણ} & સારા પ્રદર્શન માટે બોનસ & જાગ્યા પછી alarm બંધ કરવું \\
\textbf{અસર} & Rewards દ્વારા પ્રેરણા & રાહત દ્વારા પ્રેરણા \\
\textbf{Agent પ્રતિસાદ} & ક્રિયા પુનરાવર્તન કરવી & નકારાત્મક પરિણામો ટાળવા \\
\end{longtable}
}

\textbf{મુખ્ય પોઈન્ટ્સ:}

\begin{itemize}
\tightlist
\item
  \textbf{Positive Reinforcement}: Positive stimulus ઉમેરીને વર્તન મજબૂત
  બનાવે છે
\item
  \textbf{Negative Reinforcement}: Negative stimulus દૂર કરીને વર્તન મજબૂત
  બનાવે છે
\item
  \textbf{બંને પ્રકાર}: ઇચ્છિત વર્તનની સંભાવના વધારવાનું લક્ષ્ય છે
\item
  \textbf{તફાવત}: પ્રોત્સાહનની પદ્ધતિ (ઉમેરવું vs દૂર કરવું)
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``AR-RN'' (Add Reward, Remove Negative)

\end{mnemonicbox}
\subsection*{પ્રશ્ન 2(ક) [7
ગુણ]}\label{uxaaauxab0uxab6uxaa8-2uxa95-7-uxa97uxaa3}

\textbf{Supervised અને Unsupervised learning ની તુલના કરો.}

\begin{solutionbox}

\textbf{ટેબલ: Supervised vs Unsupervised Learning}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3800}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4200}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
પેરામીટર
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Supervised Learning
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Unsupervised Learning
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{ડેટા પ્રકાર} & Labeled data (input-output pairs) & Unlabeled data
(માત્ર inputs) \\
\textbf{શીખવાનું લક્ષ્ય} & પરિણામોની આગાહ & છુપા patterns શોધવા \\
\textbf{Feedback} & સાચા જવાબો છે & સાચા જવાબો નથી \\
\textbf{Algorithms} & SVM, Decision Trees, Neural Networks & K-means,
Hierarchical clustering \\
\textbf{એપ્લિકેશન} & Classification, Regression & Clustering, Association
rules \\
\textbf{ચોકસાઈ} & માપી શકાય છે & માપવી મુશ્કેલ \\
\textbf{જટિલતા} & ઓછી જટિલ & વધુ જટિલ \\
\textbf{ઉદાહરણો} & Email spam detection, કિંમત આગાહ & Customer
segmentation, Market basket analysis \\
\end{longtable}
}

\textbf{વિગતવાર તુલના:}

\textbf{Supervised Learning:}

\begin{itemize}
\tightlist
\item
  જાણીતા પરિણામો સાથે training data ની જરૂર
\item
  પ્રદર્શનનું મૂલ્યાંકન સરળતાથી કરી શકાય છે
\item
  આગાહીના કાર્યો માટે વપરાય છે
\end{itemize}

\textbf{Unsupervised Learning:}

\begin{itemize}
\tightlist
\item
  પૂર્વ-નિર્ધારિત labels વિના ડેટા સાથે કામ કરે છે
\item
  ડેટામાં છુપાયેલા structures શોધે છે
\item
  અન્વેષણાત્મક ડેટા વિશ્લેષણ માટે વપરાય છે
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``LP-PF'' (Labeled Prediction, Pattern Finding)

\end{mnemonicbox}
\subsection*{પ્રશ્ન 2(અ) OR [3
ગુણ]}\label{uxaaauxab0uxab6uxaa8-2uxa85-or-3-uxa97uxaa3}

\textbf{વ્યાખ્યાયિત કરો: Classification, Regression અને clustering.}

\begin{solutionbox}

\textbf{ટેબલ: ML કાર્યોની વ્યાખ્યાઓ}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3750}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2250}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
કાર્ય
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
વ્યાખ્યા
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
આઉટપુટ પ્રકાર
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ઉદાહરણ
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Classification} & Discrete categories/classes ની આગાહ &
Categorical & Email: Spam/Not Spam \\
\textbf{Regression} & સતત આંકડાકીય મૂલ્યોની આગાહ & આંકડાકીય & ઘરની કિંમત
આગાહ \\
\textbf{Clustering} & સમાન ડેટા points ને જૂથ બનાવવા & જૂથો/Clusters &
Customer segmentation \\
\end{longtable}
}

\textbf{વિગતવાર વ્યાખ્યાઓ:}

\begin{itemize}
\tightlist
\item
  \textbf{Classification}: શીખેલા patterns આધારે input data ને પૂર્વ-નિર્ધારિત
  વર્ગોમાં સોંપે છે
\item
  \textbf{Regression}: સતત મૂલ્યોની આગાહ કરવા variables વચ્ચેના સંબંધોનો અંદાજ
  કાઢે છે
\item
  \textbf{Clustering}: જૂથોની પૂર્વ જાણકારી વિના ડેટામાં કુદરતી જૂથો શોધે છે
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``CRC'' (Categories, Real numbers, Clusters)

\end{mnemonicbox}
\subsection*{પ્રશ્ન 2(બ) OR [4
ગુણ]}\label{uxaaauxab0uxab6uxaa8-2uxaac-or-4-uxa97uxaa3}

\textbf{Artificial Neural Network અને Biological Neural Network ની તુલના
કરો.}

\begin{solutionbox}

\textbf{ટેબલ: ANN vs Biological Neural Network}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1017}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4407}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4576}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
પાસું
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Artificial Neural Network
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Biological Neural Network
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{પ્રોસેસિંગ} & Digital/Binary & Analog \\
\textbf{ઝડપ} & ઝડપી પ્રોસેસિંગ & ધીમી પ્રોસેસિંગ \\
\textbf{શીખવું} & Backpropagation algorithm & Synaptic plasticity \\
\textbf{મેમરી} & અલગ સ્ટોરેજ & કનેક્શનમાં વિતરિત \\
\textbf{સ્ટ્રક્ચર} & સ્તરવાર આર્કિટેક્ચર & જટિલ 3D structure \\
\textbf{ખોટ સહન} & ઓછું & વધુ \\
\textbf{ઊર્જા} & વધુ પાવર consumption & ઓછો ઊર્જા વપરાશ \\
\textbf{સમાંતર પ્રક્રિયા} & મર્યાદિત parallel processing & વિશાળ parallel
processing \\
\end{longtable}
}

\textbf{મુખ્ય તફાવતો:}

\begin{itemize}
\tightlist
\item
  \textbf{ANN}: મગજથી પ્રેરિત ગાણિતિક મોડલ
\item
  \textbf{Biological}: વાસ્તવિક મગજના neural networks
\item
  \textbf{હેતુ}: ANN computation માટે, Biological cognition માટે
\item
  \textbf{અનુકૂલનક્ષમતા}: Biological networks વધુ flexible
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``DSML-CFEP'' (Digital-Speed-Memory-Layer vs
Complex-Fault-Energy-Parallel)

\end{mnemonicbox}
\subsection*{પ્રશ્ન 2(ક) OR [7
ગુણ]}\label{uxaaauxab0uxab6uxaa8-2uxa95-or-7-uxa97uxaa3}

\textbf{Supervised, unsupervised અને reinforcement learning ની વિવિધ
applications ની સૂચિ બનાવો.}

\begin{solutionbox}

\textbf{ટેબલ: વિવિધ Learning પ્રકારોની Applications}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3148}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2407}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4444}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Learning પ્રકાર
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Applications
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
વાસ્તવિક જગતના ઉદાહરણો
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Supervised} & Email classification, તબીબી નિદાન, Stock
prediction, Credit scoring & Gmail spam filter, X-ray analysis, Trading
algorithms \\
\textbf{Unsupervised} & Customer segmentation, Anomaly detection, Data
compression & Market research, Fraud detection, Image compression \\
\textbf{Reinforcement} & Game playing, Robotics, Autonomous vehicles,
Resource allocation & AlphaGo, Robot navigation, Self-driving cars \\
\end{longtable}
}

\textbf{વિગતવાર Applications:}

\textbf{Supervised Learning:}

\begin{itemize}
\tightlist
\item
  \textbf{Classification}: Spam detection, sentiment analysis, image
  recognition
\item
  \textbf{Regression}: કિંમત આગાહ, હવામાન આગાહ, વેચાણ અંદાજ
\end{itemize}

\textbf{Unsupervised Learning:}

\begin{itemize}
\tightlist
\item
  \textbf{Clustering}: Market segmentation, gene sequencing,
  recommendation systems
\item
  \textbf{Association}: Market basket analysis, web usage patterns
\end{itemize}

\textbf{Reinforcement Learning:}

\begin{itemize}
\tightlist
\item
  \textbf{Control Systems}: Robot control, traffic management
\item
  \textbf{Optimization}: Resource scheduling, portfolio management
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``SCR-CRO'' (Supervised-Classification-Regression,
Unsupervised-Clustering-Association, Reinforcement-Control-Optimization)

\end{mnemonicbox}
\subsection*{પ્રશ્ન 3(અ) [3
ગુણ]}\label{uxaaauxab0uxab6uxaa8-3uxa85-3-uxa97uxaa3}

\textbf{સિંગલ લેયર ફોરવર્ડ નેટવર્કને યોગ્ય ડાયાગ્રામ સાથે સમજાવો.}

\begin{solutionbox}

\textbf{સિંગલ લેયર ફોરવર્ડ નેટવર્ક} (Perceptron) એ સૌથી સરળ neural network છે
જેમાં input અને output વચ્ચે weights નો એક સ્તર હોય છે.

\textbf{ડાયાગ્રામ:}

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{mermaid-609021d9.pdf}

\textbf{ઘટકો:}

\begin{itemize}
\tightlist
\item
  \textbf{Inputs}: X1, X2, X3 (feature values)
\item
  \textbf{Weights}: W1, W2, W3 (connection strengths)
\item
  \textbf{Bias}: Threshold adjustment માટે વધારાનું parameter
\item
  \textbf{Summation}: Inputs નો weighted sum
\item
  \textbf{Activation}: Output બનાવવા માટેનું function
\end{itemize}

\textbf{ગાણિતિક સૂત્ર:} Y = f(Σ(Wi \times Xi) + b)

\end{solutionbox}
\begin{mnemonicbox}
``IWSA'' (Input-Weight-Sum-Activation)

\end{mnemonicbox}
\subsection*{પ્રશ્ન 3(બ) [4
ગુણ]}\label{uxaaauxab0uxab6uxaa8-3uxaac-4-uxa97uxaa3}

\textbf{Backpropagation પર ટૂંકી નોંધ લખો.}

\begin{solutionbox}

\textbf{Backpropagation} એ supervised learning algorithm છે જે error
calculation આધારે weights adjust કરીને neural networks ને train કરવા માટે
વપરાય છે.

\textbf{ટેબલ: Backpropagation પ્રક્રિયા}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3478}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3043}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3478}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
તબક્કો
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
વર્ણન
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ક્રિયા
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Forward Pass} & Input network દ્વારા આગળ વધે છે & Output ની
ગણતરી \\
\textbf{Error Calculation} & Output ને target સાથે compare કરવું &
Error/loss શોધવો \\
\textbf{Backward Pass} & Error પાછળની દિશામાં વધે છે & Weights update
કરવા \\
\textbf{Weight Update} & Gradient વાપરીને weights adjust કરવા & Error
ઘટાડવો \\
\end{longtable}
}

\textbf{મુખ્ય લક્ષણો:}

\begin{itemize}
\tightlist
\item
  \textbf{Gradient Descent}: Optimal weights શોધવા માટે calculus વાપરે છે
\item
  \textbf{Chain Rule}: દરેક weight ના error contribution ની ગણતરી
\item
  \textbf{Iterative Process}: Convergence સુધી પુનરાવર્તન
\item
  \textbf{Learning Rate}: Weight updates ની ઝડપ નિયંત્રિત કરે છે
\end{itemize}

\textbf{પગલાં:}

\begin{enumerate}
\tightlist
\item
  Random weights initialize કરવા
\item
  Output મેળવવા forward propagation
\item
  Actual અને predicted વચ્ચે error ની ગણતરી
\item
  Weights update કરવા backward propagation
\end{enumerate}

\end{solutionbox}
\begin{mnemonicbox}
``FCBU'' (Forward-Calculate-Backward-Update)

\end{mnemonicbox}
\subsection*{પ્રશ્ન 3(ક) [7
ગુણ]}\label{uxaaauxab0uxab6uxaa8-3uxa95-7-uxa97uxaa3}

\textbf{ફીડ ફોરવર્ડ ન્યુરોન નેટવર્કના આર્કિટેક્ચરના components સમજાવો.}

\begin{solutionbox}

\textbf{ફીડ ફોરવર્ડ ન્યુરલ નેટવર્ક} અનેક સ્તરો ધરાવે છે જ્યાં માહિતી input થી
output સુધી એક દિશામાં વહે છે.

\textbf{ડાયાગ્રામ:}

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{mermaid-09dee1ce.pdf}

\textbf{ઘટકો:}

\textbf{1. Input Layer:}

\begin{itemize}
\tightlist
\item
  Raw data મેળવે છે
\item
  કોઈ processing નથી, માત્ર વિતરણ
\item
  Neurons ની સંખ્યા = features ની સંખ્યા
\end{itemize}

\textbf{2. Hidden Layer(s):}

\begin{itemize}
\tightlist
\item
  Computation અને transformation કરે છે
\item
  Activation functions ધરાવે છે
\item
  અનેક hidden layers હોઈ શકે છે
\end{itemize}

\textbf{3. Output Layer:}

\begin{itemize}
\tightlist
\item
  અંતિમ પરિણામો ઉત્પન્ન કરે છે
\item
  Neurons ની સંખ્યા = outputs ની સંખ્યા
\item
  Task પ્રકાર માટે યોગ્ય activation વાપરે છે
\end{itemize}

\textbf{4. Weights અને Biases:}

\begin{itemize}
\tightlist
\item
  \textbf{Weights}: Neurons વચ્ચેની connection strengths
\item
  \textbf{Biases}: Threshold adjustment parameters
\end{itemize}

\textbf{5. Activation Functions:}

\begin{itemize}
\tightlist
\item
  Non-linearity દાખલ કરે છે
\item
  સામાન્ય પ્રકારો: ReLU, Sigmoid, Tanh
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``IHO-WA'' (Input-Hidden-Output, Weights-Activation)

\end{mnemonicbox}
\subsection*{પ્રશ્ન 3(અ) OR [3
ગુણ]}\label{uxaaauxab0uxab6uxaa8-3uxa85-or-3-uxa97uxaa3}

\textbf{મલ્ટિલેયર ફીડ ફોરવર્ડ ANN ને ડાયાગ્રામ સાથે સમજાવો.}

\begin{solutionbox}

\textbf{મલ્ટિલેયર ફીડ ફોરવર્ડ ANN} માં input અને output layers વચ્ચે અનેક hidden
layers હોય છે, જે જટિલ pattern recognition સક્ષમ બનાવે છે.

\textbf{ડાયાગ્રામ:}

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{mermaid-1be09fce.pdf}

\textbf{લક્ષણો:}

\begin{itemize}
\tightlist
\item
  \textbf{Deep Architecture}: અનેક hidden layers
\item
  \textbf{જટિલ Patterns}: Non-linear relationships શીખી શકે છે
\item
  \textbf{Universal Approximator}: કોઈપણ સતત function નો અંદાજ લગાવી શકે છે
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``MDC'' (Multiple layers, Deep learning, Complex
patterns)

\end{mnemonicbox}
\subsection*{પ્રશ્ન 3(બ) OR [4
ગુણ]}\label{uxaaauxab0uxab6uxaa8-3uxaac-or-4-uxa97uxaa3}

\textbf{સમજાવો `ReLU એ સૌથી વધુ ઉપયોગમાં લેવાતું Activation function છે.'}

\begin{solutionbox}

\textbf{ReLU (Rectified Linear Unit)} તેની સરળતા અને deep networks માં
અસરકારકતાને કારણે વ્યાપક રીતે વપરાય છે.

\textbf{ટેબલ: ReLU કેમ લોકપ્રિય છે}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2500}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
ફાયદો
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
વર્ણન
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
લાભ
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Computational Efficiency} & સરળ max(0,x) operation & ઝડપી
processing \\
\textbf{Gradient Flow} & Positive values માટે vanishing gradient નથી & વધુ
સારું learning \\
\textbf{Sparsity} & Negative inputs માટે zero output & કાર્યક્ષમ
representation \\
\textbf{Non-linearity} & Non-linear વર્તન દાખલ કરે છે & જટિલ pattern
learning \\
\end{longtable}
}

\textbf{ગાણિતિક વ્યાખ્યા:} f(x) = max(0, x)

\textbf{અન્ય Functions સાથે તુલના:}

\begin{itemize}
\tightlist
\item
  \textbf{vs Sigmoid}: Saturation સમસ્યા નથી, ઝડપી computation
\item
  \textbf{vs Tanh}: સરળ ગણતરી, વધુ સારો gradient flow
\item
  \textbf{મર્યાદાઓ}: Negative inputs માટે dead neurons સમસ્યા
\end{itemize}

\textbf{સૌથી સામાન્ય કેમ:}

\begin{itemize}
\tightlist
\item
  Vanishing gradient સમસ્યા હલ કરે છે
\item
  Computationally કાર્યક્ષમ
\item
  વ્યવહારમાં સારું કામ કરે છે
\item
  Hidden layers માટે default પસંદગી
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``CGSN'' (Computational, Gradient, Sparsity,
Non-linear)

\end{mnemonicbox}
\subsection*{પ્રશ્ન 3(ક) OR [7
ગુણ]}\label{uxaaauxab0uxab6uxaa8-3uxa95-or-7-uxa97uxaa3}

\textbf{Artificial Neural Network ની સ્ટેપ બાય સ્ટેપ લર્નિંગ પ્રક્રિયા સમજાવો.}

\begin{solutionbox}

\textbf{ANN Learning Process} માં prediction error ઘટાડવા માટે iterative
weight adjustment સામેલ છે.

\textbf{ટેબલ: સ્ટેપ-બાય-સ્ટેપ લર્નિંગ પ્રક્રિયા}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2727}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4091}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3182}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
સ્ટેપ
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
પ્રક્રિયા
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
વર્ણન
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{1. Initialization} & Random weights સેટ કરવા & નાના random
values \\
\textbf{2. Forward Propagation} & Output ની ગણતરી & Input \rightarrow Hidden \rightarrow
Output \\
\textbf{3. Error Calculation} & Target સાથે સરખામણી & Loss function
computation \\
\textbf{4. Backward Propagation} & Gradients ની ગણતરી & Error \rightarrow Hidden \leftarrow
Input \\
\textbf{5. Weight Update} & Parameters adjust કરવા & Gradient descent \\
\textbf{6. Iteration} & પ્રક્રિયા પુનરાવર્તન & Convergence સુધી \\
\end{longtable}
}

\textbf{વિગતવાર સ્ટેપ્સ:}

\textbf{સ્ટેપ 1: Weights Initialize કરવા}

\begin{itemize}
\tightlist
\item
  બધા weights અને biases ને નાના random values સોંપવા
\item
  Symmetry breaking સમસ્યા અટકાવે છે
\end{itemize}

\textbf{સ્ટેપ 2: Forward Propagation}

\begin{itemize}
\tightlist
\item
  Input data network layers દ્વારા આગળ વહે છે
\item
  દરેક neuron weighted sum + activation ની ગણતરી કરે છે
\end{itemize}

\textbf{સ્ટેપ 3: Error ની ગણતરી}

\begin{itemize}
\tightlist
\item
  Network output ને desired output સાથે compare કરવું
\item
  MSE અથવા Cross-entropy જેવા loss functions વાપરવા
\end{itemize}

\textbf{સ્ટેપ 4: Backward Propagation}

\begin{itemize}
\tightlist
\item
  દરેક weight માટે error gradient ની ગણતરી
\item
  Error પાછળની દિશામાં propagate કરવા chain rule વાપરવું
\end{itemize}

\textbf{સ્ટેપ 5: Weights Update કરવા}

\begin{itemize}
\tightlist
\item
  Gradient descent વાપરીને weights adjust કરવા
\item
  New\_weight = Old\_weight - (learning\_rate \times gradient)
\end{itemize}

\textbf{સ્ટેપ 6: પ્રક્રિયા પુનરાવર્તન}

\begin{itemize}
\tightlist
\item
  Error converge થાય અથવા maximum epochs સુધી ચાલુ રાખવું
\item
  Overfitting ટાળવા validation performance monitor કરવું
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``IFEBWI''
(Initialize-Forward-Error-Backward-Weight-Iterate)

\end{mnemonicbox}
\subsection*{પ્રશ્ન 4(અ) [3
ગુણ]}\label{uxaaauxab0uxab6uxaa8-4uxa85-3-uxa97uxaa3}

\textbf{નેચરલ લેંગ્વેજ પ્રોસેસિંગના વિવિધ ફાયદા અને ગેરફાયદાની યાદી બનાવો.}

\begin{solutionbox}

\textbf{ટેબલ: NLP ફાયદા અને ગેરફાયદા}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.4444}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.5556}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
ફાયદા
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ગેરફાયદા
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Automation} of text processing & \textbf{અસ્પષ્ટતા} in human
language \\
\textbf{24/7 ઉપલબ્ધતા} customer service માટે & \textbf{Context
Understanding} પડકારો \\
\textbf{બહુભાષીય સપોર્ટ} ક્ષમતાઓ & \textbf{સાંસ્કૃતિક સૂક્ષ્મતાઓ} મુશ્કેલી \\
\textbf{સ્કેલેબિલિટી} મોટા datasets માટે & \textbf{ઉચ્ચ Computational}
જરૂરિયાતો \\
\textbf{સુસંગતતા} responses માં & \textbf{ડેટા ગોપનીયતા} ચિંતાઓ \\
\textbf{લાગત ઘટાડવી} operations માં & \textbf{મર્યાદિત સર્જનાત્મકતા}
responses માં \\
\end{longtable}
}

\textbf{મુખ્ય પોઈન્ટ્સ:}

\begin{itemize}
\tightlist
\item
  \textbf{ફાયદા}: કાર્યક્ષમતા, સુલભતા, સુસંગતતા
\item
  \textbf{ગેરફાયદા}: જટિલતા, resource જરૂરિયાતો, મર્યાદાઓ
\item
  \textbf{સંતુલન}: ઘણી applications માં ફાયદા પડકારો કરતાં વધુ છે
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``AMS-ACC'' (Automation-Multilingual-Scalability vs
Ambiguity-Context-Computational)

\end{mnemonicbox}
\subsection*{પ્રશ્ન 4(બ) [4
ગુણ]}\label{uxaaauxab0uxab6uxaa8-4uxaac-4-uxa97uxaa3}

\textbf{NLP માં પ્રી-પ્રોસેસિંગ તકનીકોની સૂચિ બનાવો અને પાયથોન પ્રોગ્રામ વડે કોઈપણ
એકને demonstrate કરો.}

\begin{solutionbox}

\textbf{ટેબલ: NLP પ્રીપ્રોસેસિંગ તકનીકો}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3636}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2273}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4091}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
તકનીક
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
હેતુ
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ઉદાહરણ
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Tokenization} & ટેક્સ્ટને words/sentences માં વિભાજન & ``Hello
world'' \rightarrow [``Hello'', ``world''] \\
\textbf{Stop Words Removal} & સામાન્ય શબ્દો દૂર કરવા & ``the'', ``is'',
``and'' દૂર કરવા \\
\textbf{Stemming} & શબ્દોને root form માં ઘટાડવા & ``running'' \rightarrow ``run'' \\
\textbf{Lemmatization} & Dictionary form માં કન્વર્ટ કરવું & ``better'' \rightarrow
``good'' \\
\textbf{POS Tagging} & Parts of speech ઓળખવા & ``run'' \rightarrow verb \\
\textbf{Named Entity Recognition} & Entities ઓળખવા & ``Apple'' \rightarrow
Organization \\
\end{longtable}
}

\textbf{Python પ્રોગ્રામ - Tokenization:}

\begin{lstlisting}[language=Python]
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize

# Sample text
text = "Natural Language Processing અદ્ભુત છે. તે કોમ્પ્યુટરોને માનવી ભાષા સમજવામાં મદદ કરે છે."

# Word tokenization
words = word_tokenize(text)
print("Words:", words)

# Sentence tokenization  
sentences = sent_tokenize(text)
print("Sentences:", sentences)
\end{lstlisting}

\end{solutionbox}
\begin{mnemonicbox}
``TSSL-PN''
(Tokenization-Stop-Stemming-Lemmatization, POS-NER)

\end{mnemonicbox}
\subsection*{પ્રશ્ન 4(ક) [7
ગુણ]}\label{uxaaauxab0uxab6uxaa8-4uxa95-7-uxa97uxaa3}

\textbf{NLP ના phases સમજાવો.}

\begin{solutionbox}

\textbf{NLP Phases} natural language ને process અને સમજવા માટેના વ્યવસ્થિત
અભિગમને દર્શાવે છે.

\textbf{ટેબલ: NLP Phases}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2424}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2121}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2727}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2727}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
તબક્કો
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
વર્ણન
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
પ્રક્રિયા
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ઉદાહરણ
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Lexical Analysis} & Tokenization અને word identification & ટેક્સ્ટને
tokens માં વિભાજન & ``હું ખુશ છું'' \rightarrow [``હું'', ``ખુશ'', ``છું''] \\
\textbf{Syntactic Analysis} & વ્યાકરણ અને વાક્ય structure & Parse trees,
POS tagging & Noun, verb, adjective ઓળખવા \\
\textbf{Semantic Analysis} & અર્થ extraction & Word sense disambiguation
& ``બેંક'' \rightarrow financial vs નદીનો કિનારો \\
\textbf{Discourse Integration} & વાક્યો પારના context & Pronouns,
references resolve કરવા & ``તે'' refers to ``જોન'' \\
\textbf{Pragmatic Analysis} & Intent અને context understanding &
Situation/culture consider કરવું & વ્યંગ, idioms interpretation \\
\end{longtable}
}

\textbf{વિગતવાર સમજાવટ:}

\textbf{1. Lexical Analysis:}

\begin{itemize}
\tightlist
\item
  NLP pipeline નો પ્રથમ તબક્કો
\item
  Character stream ને tokens માં કન્વર્ટ કરે છે
\item
  Punctuation અને special characters દૂર કરે છે
\end{itemize}

\textbf{2. Syntactic Analysis:}

\begin{itemize}
\tightlist
\item
  વ્યાકરણનું structure વિશ્લેષણ કરે છે
\item
  Parse trees બનાવે છે
\item
  વાક્યના ઘટકો ઓળખે છે
\end{itemize}

\textbf{3. Semantic Analysis:}

\begin{itemize}
\tightlist
\item
  ટેક્સ્ટમાંથી અર્થ extract કરે છે
\item
  શબ્દની અસ્પષ્ટતા handle કરે છે
\item
  શબ્દોને concepts સાથે map કરે છે
\end{itemize}

\textbf{4. Discourse Integration:}

\begin{itemize}
\tightlist
\item
  વાક્ય સ્તર પાર ટેક્સ્ટનું વિશ્લેષણ કરે છે
\item
  વાક્યો પાર context જાળવે છે
\item
  References અને connections resolve કરે છે
\end{itemize}

\textbf{5. Pragmatic Analysis:}

\begin{itemize}
\tightlist
\item
  વાસ્તવિક જગતનો context consider કરે છે
\item
  Speaker નો intent સમજે છે
\item
  રૂપક ભાષા handle કરે છે
\end{itemize}

\textbf{મેર્મેઈડ ડાયાગ્રામ:}

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{mermaid-8d31fcb8.pdf}

\end{solutionbox}
\begin{mnemonicbox}
``LSSDP''
(Lexical-Syntactic-Semantic-Discourse-Pragmatic)

\end{mnemonicbox}
\subsection*{પ્રશ્ન 4(અ) OR [3
ગુણ]}\label{uxaaauxab0uxab6uxaa8-4uxa85-or-3-uxa97uxaa3}

\textbf{નેચરલ લેંગ્વેજ પ્રોસેસિંગ શું છે? તેની applications ની યાદી બનાવો.}

\begin{solutionbox}

\textbf{નેચરલ લેંગ્વેજ પ્રોસેસિંગ (NLP)} AI ની એક શાખા છે જે કોમ્પ્યુટરોને માનવી ભાષાને
અર્થપૂર્ણ રીતે સમજવા, અર્થઘટન કરવા અને generate કરવા સક્ષમ બનાવે છે.

\textbf{ટેબલ: NLP Applications}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2069}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4483}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3448}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
વર્ગ
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Applications
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ઉદાહરણો
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{કોમ્યુનિકેશન} & Chatbots, Virtual assistants & Siri, Alexa,
ChatGPT \\
\textbf{અનુવાદ} & ભાષા અનુવાદ & Google Translate \\
\textbf{વિશ્લેષણ} & Sentiment analysis, Text mining & Social media
monitoring \\
\textbf{શોધ} & માહિતી પુનઃપ્રાપ્તિ & Search engines \\
\textbf{લેખન} & વ્યાકરણ તપાસ, Auto-complete & Grammarly, predictive
text \\
\textbf{બિઝનેસ} & દસ્તાવેજ processing, Spam detection & Email filtering \\
\end{longtable}
}

\textbf{મુખ્ય Applications:}

\begin{itemize}
\tightlist
\item
  \textbf{મશીન અનુવાદ}: ભાષાઓ વચ્ચે ટેક્સ્ટ કન્વર્ટ કરવું
\item
  \textbf{સ્પીચ રેકગ્નિશન}: વાણીને ટેક્સ્ટમાં કન્વર્ટ કરવું
\item
  \textbf{ટેક્સ્ટ સમરાઈઝેશન}: સંક્ષિપ્ત સારાંશ બનાવવા
\item
  \textbf{પ્રશ્ન જવાબ}: પ્રશ્નોના જવાબો આપવા
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``CTAS-WB''
(Communication-Translation-Analysis-Search, Writing-Business)

\end{mnemonicbox}
\subsection*{પ્રશ્ન 4(બ) OR [4
ગુણ]}\label{uxaaauxab0uxab6uxaa8-4uxaac-or-4-uxa97uxaa3}

\textbf{NLTK માં WordNet સાથે કરવામાં આવતા કાર્યોની સૂચિ બનાવો અને python code
વડે કોઈપણ એકને demonstrate કરો.}

\begin{solutionbox}

\textbf{ટેબલ: NLTK માં WordNet કાર્યો}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
કાર્ય & વર્ણન & હેતુ \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Synsets} & સમાનાર્થી શબ્દો શોધવા & શબ્દ સમાનતા \\
\textbf{Definitions} & શબ્દના અર્થો મેળવવા & Context સમજવા \\
\textbf{Examples} & ઉપયોગના ઉદાહરણો & વ્યવહારિક application \\
\textbf{Hyponyms} & Specific terms શોધવા & વંશવેલો સંબંધો \\
\textbf{Hypernyms} & સામાન્ય terms શોધવા & Category identification \\
\textbf{Antonyms} & વિરોધી શબ્દો શોધવા & Contrast analysis \\
\end{longtable}
}

\textbf{Python કોડ - Synsets અને Definitions:}

\begin{lstlisting}[language=Python]
from nltk.corpus import wordnet

# 'સારું' શબ્દ માટે synsets મેળવવા
synsets = wordnet.synsets('good')
print("Synsets:", synsets)

# વ્યાખ્યા મેળવવી
definition = synsets[0].definition()
print("Definition:", definition)

# ઉદાહરણો મેળવવા
examples = synsets[0].examples()
print("Examples:", examples)
\end{lstlisting}

\end{solutionbox}
\begin{mnemonicbox}
``SDEHA''
(Synsets-Definitions-Examples-Hyponyms-Antonyms)

\end{mnemonicbox}
\subsection*{પ્રશ્ન 4(ક) OR [7
ગુણ]}\label{uxaaauxab0uxab6uxaa8-4uxa95-or-7-uxa97uxaa3}

\textbf{NLP માં ambiguities ના પ્રકારો સમજાવો.}

\begin{solutionbox}

\textbf{NLP Ambiguities} ત્યારે થાય છે જ્યારે ટેક્સ્ટનું અનેક રીતે અર્થઘટન થઈ શકે છે, જે
automated understanding માટે પડકારો બનાવે છે.

\textbf{ટેબલ: Ambiguities ના પ્રકારો}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1935}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2258}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2903}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2903}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
પ્રકાર
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
વર્ણન
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ઉદાહરણ
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
નિવારણ
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Lexical} & એક શબ્દના અનેક અર્થ & ``બેંક'' (financial/નદીનો કિનારો) &
Context analysis \\
\textbf{Syntactic} & અનેક વ્યાકરણ અર્થઘટન & ``ઉડતા વિમાનો ખતરનાક હોઈ શકે છે''
& Parse trees \\
\textbf{Semantic} & વાક્ય સ્તરે અનેક અર્થ & ``સમય તીરની જેમ ઉડે છે'' & Semantic
analysis \\
\textbf{Pragmatic} & Context-આધારિત અર્થઘટન & ``શું તમે મીઠું આપી શકશો?'' &
Situational context \\
\textbf{Referential} & અસ્પષ્ટ pronoun references & ``જોને બોબને કહ્યું તે ખોટો
હતો'' & Discourse analysis \\
\end{longtable}
}

\textbf{વિગતવાર સમજાવટ:}

\textbf{1. Lexical Ambiguity:}

\begin{itemize}
\tightlist
\item
  એક જ શબ્દ, વિવિધ અર્થો
\item
  Homonyms અને polysemes
\item
  ઉદાહરણ: ``બેટ'' (પ્રાણી/રમત સાધન)
\end{itemize}

\textbf{2. Syntactic Ambiguity:}

\begin{itemize}
\tightlist
\item
  અનેક વ્યાકરણ structures
\item
  વિવિધ parse trees શક્ય
\item
  ઉદાહરણ: ``મેં દૂરબીન સાથે એક માણસને જોયો''
\end{itemize}

\textbf{3. Semantic Ambiguity:}

\begin{itemize}
\tightlist
\item
  વાક્ય-સ્તરે અર્થની ગૂંચવણ
\item
  અનેક અર્થઘટન શક્ય
\item
  ઉદાહરણ: ``સંબંધીઓની મુલાકાત કંટાળાજનક હોઈ શકે છે''
\end{itemize}

\textbf{4. Pragmatic Ambiguity:}

\begin{itemize}
\tightlist
\item
  Context અને intent આધારિત
\item
  સાંસ્કૃતિક અને પરિસ્થિતિગત પરિબળો
\item
  ઉદાહરણ: વ્યંગ અને indirect requests
\end{itemize}

\textbf{5. Referential Ambiguity:}

\begin{itemize}
\tightlist
\item
  Entities ના અસ્પષ્ટ references
\item
  Pronoun resolution પડકારો
\item
  ઉદાહરણ: અનેક શક્ય antecedents
\end{itemize}

\textbf{નિવારણ વ્યૂહરચનાઓ:}

\begin{itemize}
\tightlist
\item
  Context analysis અને machine learning
\item
  આંકડાકીય disambiguation પદ્ધતિઓ
\item
  Knowledge bases અને ontologies
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``LSSPR''
(Lexical-Syntactic-Semantic-Pragmatic-Referential)

\end{mnemonicbox}
\subsection*{પ્રશ્ન 5(અ) [3
ગુણ]}\label{uxaaauxab0uxab6uxaa8-5uxa85-3-uxa97uxaa3}

\textbf{બેગ ઓફ વર્ડ્સને ઉદાહરણ સાથે સમજાવો.}

\begin{solutionbox}

\textbf{બેગ ઓફ વર્ડ્સ (BoW)} એ ટેક્સ્ટ પ્રતિનિધિત્વ પદ્ધતિ છે જે શબ્દની આવર્તન આધારે
ટેક્સ્ટને આંકડાકીય vectors માં કન્વર્ટ કરે છે, વ્યાકરણ અને શબ્દ ક્રમને અવગણીને.

\textbf{ટેબલ: BoW પ્રક્રિયા}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
સ્ટેપ & પ્રક્રિયા & વર્ણન \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{1. Tokenization} & ટેક્સ્ટને શબ્દોમાં વિભાજન & Vocabulary બનાવવી \\
\textbf{2. Vocabulary Creation} & અનન્ય શબ્દોનો સંગ્રહ & Terms નો શબ્દકોશ \\
\textbf{3. Vector Creation} & શબ્દ આવર્તન ગણવી & આંકડાકીય પ્રતિનિધિત્વ \\
\end{longtable}
}

\textbf{ઉદાહરણ:}

દસ્તાવેજો:

\begin{itemize}
\tightlist
\item
  Doc1: ``હું મશીન લર્નિંગ પસંદ કરું છું''
\item
  Doc2: ``મશીન લર્નિંગ અદ્ભુત છે''
\end{itemize}

\textbf{Vocabulary:} [હું, પસંદ, મશીન, લર્નિંગ, અદ્ભુત, છે, કરું, છું]

\textbf{BoW Vectors:}

\begin{itemize}
\tightlist
\item
  Doc1: [1, 1, 1, 1, 0, 0, 1, 1]
\item
  Doc2: [0, 0, 1, 1, 1, 1, 0, 0]
\end{itemize}

\textbf{લક્ષણો:}

\begin{itemize}
\tightlist
\item
  \textbf{ક્રમ સ્વતંત્ર}: શબ્દ ક્રમ અવગણવામાં આવે છે
\item
  \textbf{આવર્તન આધારિત}: શબ્દ occurrences ગણે છે
\item
  \textbf{Sparse Representation}: ઘણા શૂન્ય મૂલ્યો
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``TVC'' (Tokenize-Vocabulary-Count)

\end{mnemonicbox}
\subsection*{પ્રશ્ન 5(બ) [4
ગુણ]}\label{uxaaauxab0uxab6uxaa8-5uxaac-4-uxa97uxaa3}

\textbf{Word2Vec શું છે? તેના steps સમજાવો.}

\begin{solutionbox}

\textbf{Word2Vec} એ neural network-આધારિત તકનીક છે જે મોટા text corpora માં
તેમના context થી શીખીને શબ્દોના dense vector representations બનાવે છે.

\textbf{ટેબલ: Word2Vec Models}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
મોડલ & અભિગમ & આગાહી \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{CBOW} & Continuous Bag of Words & સંદર્ભ \rightarrow લક્ષ્ય શબ્દ \\
\textbf{Skip-gram} & Skip-gram with Negative Sampling & લક્ષ્ય શબ્દ \rightarrow
સંદર્ભ \\
\end{longtable}
}

\textbf{Word2Vec ના સ્ટેપ્સ:}

\textbf{1. ડેટા તૈયારી:}

\begin{itemize}
\tightlist
\item
  મોટો text corpus એકત્ર કરવો
\item
  ટેક્સ્ટ સાફ કરવું અને preprocess કરવું
\item
  Training pairs બનાવવા
\end{itemize}

\textbf{2. મોડલ આર્કિટેક્ચર:}

\begin{itemize}
\tightlist
\item
  Input layer (one-hot encoded words)
\item
  Hidden layer (embedding layer)
\item
  Output layer (prediction માટે softmax)
\end{itemize}

\textbf{3. Training પ્રક્રિયા:}

\begin{itemize}
\tightlist
\item
  \textbf{CBOW}: context થી target word ની આગાહ
\item
  \textbf{Skip-gram}: target word થી context ની આગાહ
\item
  Weights update કરવા backpropagation વાપરવું
\end{itemize}

\textbf{4. Vector Extraction:}

\begin{itemize}
\tightlist
\item
  Hidden layer થી weight matrix extract કરવું
\item
  દરેક row word embedding દર્શાવે છે
\item
  સામાન્યતઃ 100-300 dimensions
\end{itemize}

\textbf{લાભો:}

\begin{itemize}
\tightlist
\item
  Semantic relationships કેપ્ચર કરે છે
\item
  સમાન શબ્દોના સમાન vectors હોય છે
\item
  Arithmetic operations સપોર્ટ કરે છે (રાજા - પુરુષ + સ્ત્રી = રાણી)
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``DMAT'' (Data-Model-Architecture-Training)

\end{mnemonicbox}
\subsection*{પ્રશ્ન 5(ક) [7
ગુણ]}\label{uxaaauxab0uxab6uxaa8-5uxa95-7-uxa97uxaa3}

\textbf{NLP ની applications ની યાદી બનાવો અને કોઈપણ એકને વિગતવાર સમજાવો.}

\begin{solutionbox}

\textbf{ટેબલ: NLP Applications}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Application & વર્ણન & ઉદ્યોગ ઉપયોગ \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{મશીન અનુવાદ} & ભાષા રૂપાંતરણ & વૈશ્વિક કોમ્યુનિકેશન \\
\textbf{Sentiment Analysis} & મત ખનન & Social media monitoring \\
\textbf{Chatbots} & વાતચીત AI & Customer service \\
\textbf{ટેક્સ્ટ સારાંશ} & સામગ્રી સંકુચન & સમાચાર, સંશોધન \\
\textbf{સ્પીચ રેકગ્નિશન} & અવાજ થી ટેક્સ્ટ & Virtual assistants \\
\textbf{માહિતી નિષ્કર્ષણ} & ટેક્સ્ટમાંથી ડેટા માઇનિંગ & Business intelligence \\
\textbf{પ્રશ્ન જવાબ} & સ્વચાલિત પ્રતિસાદો & Search engines \\
\textbf{સ્પામ શોધ} & Email filtering & Cybersecurity \\
\end{longtable}
}

\textbf{વિગતવાર સમજાવટ: Sentiment Analysis}

\textbf{Sentiment Analysis} એ ટેક્સ્ટ ડેટામાં વ્યક્ત કરાયેલ ભાવનાત્મક tone અને
મંતવ્યો નક્કી કરવાની પ્રક્રિયા છે.

\textbf{ઘટકો:}

\begin{itemize}
\tightlist
\item
  \textbf{ટેક્સ્ટ પ્રીપ્રોસેસિંગ}: સફાઈ અને tokenization
\item
  \textbf{Feature Extraction}: TF-IDF, word embeddings
\item
  \textbf{વર્ગીકરણ}: સકારાત્મક, નકારાત્મક, તટસ્થ
\item
  \textbf{વિશ્વાસ સ્કોરિંગ}: Sentiment ની મજબૂતાઈ
\end{itemize}

\textbf{પ્રક્રિયા સ્ટેપ્સ:}

\begin{enumerate}
\tightlist
\item
  \textbf{ડેટા સંગ્રહ}: Reviews, social media થી ટેક્સ્ટ એકત્ર કરવું
\item
  \textbf{પ્રીપ્રોસેસિંગ}: Noise દૂર કરવો, ટેક્સ્ટ normalize કરવું
\item
  \textbf{Feature Engineering}: ટેક્સ્ટને આંકડાકીય features માં કન્વર્ટ કરવું
\item
  \textbf{મોડલ ટ્રેનિંગ}: વર્ગીકરણ માટે ML algorithms વાપરવા
\item
  \textbf{આગાહી}: નવા ટેક્સ્ટના sentiment ની વર્ગીકરણ
\item
  \textbf{મૂલ્યાંકન}: ચોકસાઈ અને કામગીરી માપવી
\end{enumerate}

\textbf{Applications:}

\begin{itemize}
\tightlist
\item
  \textbf{બ્રાન્ડ મોનિટરિંગ}: ગ્રાહક મંતવ્યોને ટ્રેક કરવા
\item
  \textbf{પ્રોડક્ટ રિવ્યૂ}: ગ્રાહક પ્રતિસાદનું વિશ્લેષણ
\item
  \textbf{સોશિયલ મીડિયા}: જાહેર sentiment ને monitor કરવું
\item
  \textbf{બજાર સંશોધન}: ગ્રાહક પસંદગીઓ સમજવી
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``MSCTSIQ-S''
(Machine-Sentiment-Chatbot-Text-Speech-Information-Question-Spam)

\end{mnemonicbox}
\subsection*{પ્રશ્ન 5(અ) OR [3
ગુણ]}\label{uxaaauxab0uxab6uxaa8-5uxa85-or-3-uxa97uxaa3}

\textbf{ઉદાહરણ સાથે TFIDF સમજાવો.}

\begin{solutionbox}

\textbf{TF-IDF (Term Frequency-Inverse Document Frequency)} દસ્તાવેજોના
સંગ્રહ સાપેક્ષે દસ્તાવેજમાં શબ્દની મહત્ત્વતા માપે છે.

\textbf{સૂત્ર:} TF-IDF = TF(t,d) \times IDF(t)

જ્યાં:

\begin{itemize}
\tightlist
\item
  TF(t,d) = (દસ્તાવેજ d માં term t કેટલી વખત આવે છે) / (દસ્તાવેજ d માં કુલ terms)
\item
  IDF(t) = log(કુલ દસ્તાવેજો / term t ધરાવતા દસ્તાવેજો)
\end{itemize}

\textbf{ઉદાહરણ:}

દસ્તાવેજો:

\begin{itemize}
\tightlist
\item
  Doc1: ``મશીન લર્નિંગ સારું છે''
\item
  Doc2: ``લર્નિંગ algorithms સારા છે''
\item
  Doc3: ``મશીન algorithms સારું કામ કરે છે''
\end{itemize}

\textbf{ટેબલ: ``મશીન'' માટે TF-IDF ગણતરી}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
દસ્તાવેજ & TF & IDF & TF-IDF \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Doc1 & 1/4 = 0.25 & log(3/2) = 0.18 & 0.25 \times 0.18 = 0.045 \\
Doc2 & 0/4 = 0 & log(3/2) = 0.18 & 0 \times 0.18 = 0 \\
Doc3 & 1/4 = 0.25 & log(3/2) = 0.18 & 0.25 \times 0.18 = 0.045 \\
\end{longtable}
}

\textbf{મુખ્ય પોઈન્ટ્સ:}

\begin{itemize}
\tightlist
\item
  \textbf{ઉચ્ચ TF-IDF}: વિશિષ્ટ દસ્તાવેજમાં મહત્વપૂર્ણ શબ્દ
\item
  \textbf{નીચો TF-IDF}: દસ્તાવેજો પાર સામાન્ય શબ્દ
\item
  \textbf{Applications}: માહિતી પુનઃપ્રાપ્તિ, text mining
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``TI-FD'' (Term frequency, Inverse Document
frequency)

\end{mnemonicbox}
\subsection*{પ્રશ્ન 5(બ) OR [4
ગુણ]}\label{uxaaauxab0uxab6uxaa8-5uxaac-or-4-uxa97uxaa3}

\textbf{TFIDF અને BOW સાથેના challenges વિશે સમજાવો.}

\begin{solutionbox}

\textbf{ટેબલ: TF-IDF અને BOW સાથેના પડકારો}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2800}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3200}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
પડકાર
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
TF-IDF
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
BOW
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
અસર
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Semantic Understanding} & અર્થ કેપ્ચર કરી શકતું નથી & શબ્દ સંબંધો અવગણે છે
& Context ની ખરાબ સમજ \\
\textbf{શબ્દ ક્રમ} & સ્થિતિ અવગણે છે & ક્રમ ખોવાઈ જાય છે & વ્યાકરણ અર્થ ખોવાઈ જાય
છે \\
\textbf{Sparsity} & ઉચ્ચ-dimensional vectors & ઘણા શૂન્ય મૂલ્યો & મેમરી
અકાર્યક્ષમ \\
\textbf{Vocabulary માપ} & મોટો feature space & Corpus સાથે વધે છે &
Computational જટિલતા \\
\textbf{Out-of-Vocabulary} & અજાણ્યા શબ્દો અવગણે છે & નવા શબ્દો handle કરતું
નથી & મર્યાદિત સામાન્યીકરણ \\
\textbf{Polysemy} & અનેક અર્થો & વિવિધ senses માટે સમાન વર્તન & અસ્પષ્ટતા
સમસ્યાઓ \\
\end{longtable}
}

\textbf{વિગતવાર પડકારો:}

\textbf{1. Semantic Understanding નો અભાવ:}

\begin{itemize}
\tightlist
\item
  શબ્દો સ્વતંત્ર features તરીકે ગણાય છે
\item
  Synonyms અથવા સંબંધિત concepts સમજી શકતા નથી
\item
  ``સારું'' અને ``ઉત્તમ'' અલગ રીતે ગણાય છે
\end{itemize}

\textbf{2. શબ્દ ક્રમ ખોવાઈ જવો:}

\begin{itemize}
\tightlist
\item
  ``કુતરો માણસને કરડે છે'' vs ``માણસ કુતરાને કરડે છે'' સમાન representation
\item
  Context અને વ્યાકરણની માહિતી ખોવાઈ જાય છે
\item
  વાક્ય structure અવગણવામાં આવે છે
\end{itemize}

\textbf{3. ઉચ્ચ Dimensionality:}

\begin{itemize}
\tightlist
\item
  Vector size vocabulary size ની બરાબર
\item
  મોટાભાગે શૂન્યો સાથે sparse matrices
\item
  Storage અને computation સમસ્યાઓ
\end{itemize}

\textbf{4. Context Insensitivity:}

\begin{itemize}
\tightlist
\item
  એક જ શબ્દ વિવિધ contexts માં સમાન ગણાય છે
\item
  ``Apple'' કંપની vs ફળ સમાન representation
\item
  Polysemy અને homonymy સમસ્યાઓ
\end{itemize}

\textbf{ઉકેલો:}

\begin{itemize}
\tightlist
\item
  \textbf{Word Embeddings}: Word2Vec, GloVe
\item
  \textbf{Contextual Models}: BERT, GPT
\item
  \textbf{N-grams}: કેટલાક શબ્દ ક્રમ કેપ્ચર કરવા
\item
  \textbf{Dimensionality Reduction}: PCA, SVD
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``SSVO-CP'' (Semantic-Sequence-Vocabulary-OOV,
Context-Polysemy)

\end{mnemonicbox}
\subsection*{પ્રશ્ન 5(ક) OR [7
ગુણ]}\label{uxaaauxab0uxab6uxaa8-5uxa95-or-7-uxa97uxaa3}

\textbf{GloVe ની કામગીરી સમજાવો.}

\begin{solutionbox}

\textbf{GloVe (Global Vectors for Word Representation)} word embeddings
બનાવવા માટે global આંકડાકીય માહિતીને local context windows સાથે જોડે છે.

\textbf{ટેબલ: GloVe vs અન્ય પદ્ધતિઓ}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2381}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4524}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
પાસું
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
GloVe
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Word2Vec
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
પરંપરાગત પદ્ધતિઓ
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{અભિગમ} & Global + Local statistics & Local context windows &
આવર્તન-આધારિત \\
\textbf{Training} & Matrix factorization & Neural networks & ગણતરી
પદ્ધતિઓ \\
\textbf{કાર્યક્ષમતા} & ઝડપી training & ધીમી training & ખૂબ ઝડપી \\
\textbf{પ્રદર્શન} & ઉચ્ચ ચોકસાઈ & સારી ચોકસાઈ & મર્યાદિત પ્રદર્શન \\
\end{longtable}
}

\textbf{કામગીરી પ્રક્રિયા:}

\textbf{1. Co-occurrence Matrix Construction:}

\begin{itemize}
\tightlist
\item
  Context windows માં શબ્દ co-occurrences ગણવા
\item
  Global statistics matrix બનાવવો
\item
  Xij = word j, word i ના context માં કેટલી વખત આવે છે
\end{itemize}

\textbf{2. Ratio ગણતરી:}

\begin{itemize}
\tightlist
\item
  સંભાવના ratios ની ગણતરી
\item
  P(k\textbar i) = Xik / Xi (word i આપવામાં આવે તો word k ની સંભાવના)
\item
  સંભાવનાઓ વચ્ચેના અર્થપૂર્ણ ratios પર ધ્યાન
\end{itemize}

\textbf{3. Objective Function:}

\begin{itemize}
\tightlist
\item
  Weighted least squares objective minimize કરવું
\item
  J = Σ f(Xij)(wi\^{}T wj + bi + bj - log Xij)^{2}
\item
  જ્યાં f(x) weighting function છે
\end{itemize}

\textbf{4. Vector Learning:}

\begin{itemize}
\tightlist
\item
  Objective optimize કરવા gradient descent વાપરવું
\item
  Word vectors wi અને context vectors wj શીખવા
\item
  અંતિમ representation બંને vectors combine કરે છે
\end{itemize}

\textbf{મુખ્ય લક્ષણો:}

\textbf{Global Statistics:}

\begin{itemize}
\tightlist
\item
  સમગ્ર corpus માહિતી વાપરે છે
\item
  Global word relationships કેપ્ચર કરે છે
\item
  Local પદ્ધતિઓ કરતાં વધુ સ્થિર
\end{itemize}

\textbf{કાર્યક્ષમતા:}

\begin{itemize}
\tightlist
\item
  Co-occurrence statistics પર train કરે છે
\item
  Neural network પદ્ધતિઓ કરતાં ઝડપી
\item
  મોટા corpora માટે scalable
\end{itemize}

\textbf{પ્રદર્શન:}

\begin{itemize}
\tightlist
\item
  Analogy tasks પર સારું પ્રદર્શન
\item
  Semantic અને syntactic બંને સંબંધો કેપ્ચર કરે છે
\item
  Similarity tasks પર સારી કામગીરી
\end{itemize}

\textbf{ગાણિતિક આધાર:}

\begin{lstlisting}
J = Σ(i,j=1 to V) f(Xij)(wi^T wj + bi + bj - log Xij)^{2}
\end{lstlisting}

જ્યાં:

\begin{itemize}
\tightlist
\item
  V = vocabulary size
\item
  Xij = co-occurrence count
\item
  wi, wj = word vectors
\item
  bi, bj = bias terms
\item
  f(x) = weighting function
\end{itemize}

\textbf{ફાયદા:}

\begin{itemize}
\tightlist
\item
  \textbf{લાભો સંયોજન}: Global statistics + local context
\item
  \textbf{સમજી શકાય તેવું}: સ્પષ્ટ ગાણિતિક આધાર
\item
  \textbf{કાર્યક્ષમ}: Word2Vec કરતાં ઝડપી training
\item
  \textbf{અસરકારક}: વિવિધ tasks પર સારી કામગીરી
\end{itemize}

\textbf{Applications:}

\begin{itemize}
\tightlist
\item
  \textbf{શબ્દ સમાનતા}: સંબંધિત શબ્દો શોધવા
\item
  \textbf{Analogy Tasks}: રાજા - પુરુષ + સ્ત્રી = રાણી
\item
  \textbf{ટેક્સ્ટ વર્ગીકરણ}: Feature representation
\item
  \textbf{મશીન અનુવાદ}: Cross-lingual mappings
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``CROF-PGAE''
(Co-occurrence-Ratio-Objective-Function,
Performance-Global-Advantage-Efficiency)

\end{mnemonicbox}

\end{document}
