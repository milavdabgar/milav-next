\documentclass[10pt,a4paper]{article}
\input{../../../../../../latex-templates/gtu-solutions/preamble.tex}
\input{../../../../../../latex-templates/gtu-solutions/gujarati-boxes.tex}

\begin{document}

\begin{center}
{\Huge\bfseries\color{headcolor} Subject Name (Gujarati)}\\[5pt]
{\LARGE 4351601 -- Summer 2024}\\[3pt]
{\large Semester 1 Study Material}\\[3pt]
{\normalsize\textit{Detailed Solutions and Explanations}}
\end{center}

\vspace{10pt}

\subsection*{પ્રશ્ન 1(અ) [3
ગુણ]}\label{uxaaauxab0uxab6uxaa8-1uxa85-3-uxa97uxaa3}

\textbf{Narrow AI અથવા Weak AI નો અર્થ શું છે?}

\begin{solutionbox}

\textbf{Narrow AI} અથવા \textbf{Weak AI} એ specific અને limited કાર્યો માટે
બનાવેલ artificial intelligence systems છે.

\textbf{ટેબલ: Narrow AI ની લાક્ષણિકતાઓ}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
પાસું & વર્ણન \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{વ્યાપ્તિ} & ફક્ત specific કાર્યો માટે \\
\textbf{બુદ્ધિમત્તા} & કાર્ય-વિશિષ્ટ કુશળતા \\
\textbf{ઉદાહરણો} & Siri, chess programs, recommendation systems \\
\textbf{શીખવાની પ્રક્રિયા} & Domain માં pattern recognition \\
\end{longtable}
}

\end{solutionbox}
\begin{mnemonicbox}
``Narrow = ફક્ત વિશિષ્ટ કાર્યો''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{પ્રશ્ન 1(બ) [4
ગુણ]}\label{uxaaauxab0uxab6uxaa8-1uxaac-4-uxa97uxaa3}

\textbf{વ્યાખ્યાયિત કરો: વર્ગીકરણ, રીગ્રેસન, ક્લસ્ટરિંગ, એસોસિએશન વિશ્લેષણ.}

\begin{solutionbox}

\textbf{ટેબલ: Machine Learning ની તકનીકો}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2647}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2647}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2059}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2647}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
તકનીક
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
વ્યાખ્યા
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
પ્રકાર
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ઉદાહરણ
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{વર્ગીકરણ (Classification)} & Discrete categories/classes predict
કરે છે & Supervised & Email spam detection \\
\textbf{રીગ્રેસન (Regression)} & Continuous numerical values predict કરે છે
& Supervised & House price prediction \\
\textbf{ક્લસ્ટરિંગ (Clustering)} & Similar data points ને group કરે છે &
Unsupervised & Customer segmentation \\
\textbf{એસોસિએશન વિશ્લેષણ} & Variables વચ્ચે relationships શોધે છે &
Unsupervised & Market basket analysis \\
\end{longtable}
}

\end{solutionbox}
\begin{mnemonicbox}
``CRCA - Categories, Real numbers, Clusters,
Associations''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{પ્રશ્ન 1(ક) [7
ગુણ]}\label{uxaaauxab0uxab6uxaa8-1uxa95-7-uxa97uxaa3}

\textbf{ન્યુરોનના ત્રણ મુખ્ય ઘટકોને પ્રકાશિત કરો.}

\begin{solutionbox}

Biological neuron ના ત્રણ મુખ્ય ઘટકો જે artificial neural networks ને inspire
કરે છે:

\textbf{ડાયાગ્રામ:}

\begin{verbatim}
    Dendrites     Cell Body      Axon
        |            |           |
        v            v           v
    [Inputs] {-{-} [Processing] {-}{-} [Output]}
        |            |           |
    Receives     Integrates    Transmits
    signals      signals       signals
\end{verbatim}

\textbf{ટેબલ: ન્યુરોન ઘટકો}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2083}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5417}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
ઘટક
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
કાર્ય
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI માં સમકક્ષ
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Dendrites} & અન્ય neurons થી input signals receive કરે છે & Input
layer/weights \\
\textbf{Cell Body (Soma)} & Signals ને process અને integrate કરે છે &
Activation function \\
\textbf{Axon} & અન્ય neurons ને output signals transmit કરે છે & Output
connections \\
\end{longtable}
}

\textbf{મુખ્ય મુદ્દાઓ:}

\begin{itemize}
\tightlist
\item
  \textbf{Dendrites}: વિવિધ connection strengths સાથે input receivers
  તરીકે કામ કરે છે
\item
  \textbf{Cell Body}: Inputs ને sum કરે છે અને threshold function apply કરે છે
\item
  \textbf{Axon}: Processed signal ને આગળના neurons સુધી પહોંચાડે છે
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``DCA - Dendrites Collect, Cell-body Calculates,
Axon Announces''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{પ્રશ્ન 1(ક) OR [7
ગુણ]}\label{uxaaauxab0uxab6uxaa8-1uxa95-or-7-uxa97uxaa3}

\textbf{Artificial Neural Network માં back propagation પદ્ધતિ સમજાવો.}

\begin{solutionbox}

\textbf{Back Propagation} એ supervised learning algorithm છે જે gradient
descent દ્વારા error minimize કરીને multi-layer neural networks ને train કરે
છે.

\textbf{ફ્લોચાર્ટ:}

\begin{center}
\textbf{Mermaid Diagram (Code)}
\begin{verbatim}
{Shaded}
{Highlighting}[]
graph LR
    A[Forward Pass] {-{-}{} B[Output Calculate કરો]}
    B {-{-}{} C[Error Calculate કરો]}
    C {-{-}{} D[Backward Pass]}
    D {-{-}{} E[Gradients Calculate કરો]}
    E {-{-}{} F[Weights Update કરો]}
    F {-{-}{} G\{Error સ્વીકાર્ય છે?\}}
    G {-{-}{}|ના| A}
    G {-{-}{}|હા| H[Training Complete]}
{Highlighting}
{Shaded}
\end{verbatim}
\end{center}

\textbf{ટેબલ: Back Propagation Steps}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2400}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3600}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
સ્ટેપ
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
પ્રક્રિયા
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ફોર્મ્યુલા
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Forward Pass} & Layer દ્વારા layer outputs calculate કરો & y =
f(Σ(wi*xi + b)) \\
\textbf{Error Calculation} & Loss function compute કરો & E = ½(target -
output)^{2} \\
\textbf{Backward Pass} & Error gradients calculate કરો & δ = \partialE/\partialw \\
\textbf{Weight Update} & Learning rate વાપરીને weights adjust કરો &
w\_new = w\_old - η*δ \\
\end{longtable}
}

\textbf{મુખ્ય લાક્ષણિકતાઓ:}

\begin{itemize}
\tightlist
\item
  \textbf{Gradient Descent}: Minimum error શોધવા માટે calculus વાપરે છે
\item
  \textbf{Chain Rule}: Layers દ્વારા error ને backward propagate કરે છે
\item
  \textbf{Learning Rate}: Weight updates ની speed control કરે છે
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``FEBU - Forward, Error, Backward, Update''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{પ્રશ્ન 2(અ) [3
ગુણ]}\label{uxaaauxab0uxab6uxaa8-2uxa85-3-uxa97uxaa3}

\textbf{Machine Learning માં ઉપયોગમાં લેવાતા કોઈપણ પાંચ લોકપ્રિય algorithms
ની સૂચિ બનાવો.}

\begin{solutionbox}

\textbf{ટેબલ: લોકપ્રિય ML Algorithms}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Algorithm & પ્રકાર & Application \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Linear Regression} & Supervised & Continuous values નું
prediction \\
\textbf{Decision Tree} & Supervised & Classification અને regression \\
\textbf{K-Means Clustering} & Unsupervised & Data grouping \\
\textbf{Support Vector Machine} & Supervised & Margins સાથે
classification \\
\textbf{Random Forest} & Supervised & Ensemble learning \\
\end{longtable}
}

\end{solutionbox}
\begin{mnemonicbox}
``LDKSR - Learn Data, Keep Samples, Run''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{પ્રશ્ન 2(બ) [4
ગુણ]}\label{uxaaauxab0uxab6uxaa8-2uxaac-4-uxa97uxaa3}

\textbf{નિષ્ણાત સિસ્ટમ શું છે? તેની મર્યાદાઓ અને applications ની યાદી બનાવો.}

\begin{solutionbox}

\textbf{Expert System} એ AI program છે જે specific domains માં complex
problems solve કરવા માટે human expert knowledge ને mimic કરે છે.

\textbf{ટેબલ: Expert System Overview}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
પાસું & વિગતો \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{વ્યાખ્યા} & Domain-specific expertise સાથે AI system \\
\textbf{ઘટકો} & Knowledge base, inference engine, user interface \\
\end{longtable}
}

\textbf{Applications:}

\begin{itemize}
\tightlist
\item
  \textbf{Medical Diagnosis}: રોગ identification systems
\item
  \textbf{Financial Planning}: Investment advisory systems
\item
  \textbf{Fault Diagnosis}: Equipment troubleshooting
\end{itemize}

\textbf{મર્યાદાઓ:}

\begin{itemize}
\tightlist
\item
  \textbf{Limited Domain}: ફક્ત specific areas માં કામ કરે છે
\item
  \textbf{Knowledge Acquisition}: Expert knowledge extract કરવું મુશ્કેલ
\item
  \textbf{Maintenance}: Rules update અને modify કરવા મુશ્કેલ
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``EXPERT - Explains Problems, Executes Rules,
Tests''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{પ્રશ્ન 2(ક) [7
ગુણ]}\label{uxaaauxab0uxab6uxaa8-2uxa95-7-uxa97uxaa3}

\textbf{ટોકનાઇઝેશન શું છે? યોગ્ય ઉદાહરણ સાથે સમજાવો.}

\begin{solutionbox}

\textbf{Tokenization} એ text ને smaller units (tokens) માં break down
કરવાની process છે NLP processing માટે.

\textbf{ટેબલ: Tokenization ના પ્રકારો}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2727}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3182}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4091}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
પ્રકાર
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
વર્ણન
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ઉદાહરણ
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Word Tokenization} & Words દ્વારા split કરે છે & ``Hello world'' \rightarrow
[``Hello'', ``world''] \\
\textbf{Sentence Tokenization} & Sentences દ્વારા split કરે છે & ``Hi. How
are you?'' \rightarrow [``Hi.'', ``How are you?''] \\
\textbf{Subword Tokenization} & Subwords માં split કરે છે & ``unhappy'' \rightarrow
[``un'', ``happy''] \\
\end{longtable}
}

\textbf{Code ઉદાહરણ:}

\begin{verbatim}
import nltk
text = "Natural Language Processing is amazing!"
tokens = nltk.word\_tokenize(text)
\# Output: [{Natural, Language, Processing, is, amazing, !]}
\end{verbatim}

\textbf{Process Flow:}

\begin{center}
\textbf{Mermaid Diagram (Code)}
\begin{verbatim}
{Shaded}
{Highlighting}[]
graph LR
    A[Raw Text] {-{-}{} B[Tokenization]}
    B {-{-}{} C[Clean Tokens]}
    C {-{-}{} D[Further Processing]}
{Highlighting}
{Shaded}
\end{verbatim}
\end{center}

\textbf{મુખ્ય ફાયદા:}

\begin{itemize}
\tightlist
\item
  \textbf{Standardization}: Text ને uniform format માં convert કરે છે
\item
  \textbf{Analysis Ready}: ML algorithms માટે text prepare કરે છે
\item
  \textbf{Feature Extraction}: Statistical analysis enable કરે છે
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``TOKEN - Text Operations Keep Everything
Normalized''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{પ્રશ્ન 2(અ) OR [3
ગુણ]}\label{uxaaauxab0uxab6uxaa8-2uxa85-or-3-uxa97uxaa3}

\textbf{સુપરવાઇઝ્ડ અને અનસુપરવાઇઝ્ડ લર્નિંગની સરખામણી કરો.}

\begin{solutionbox}

\textbf{ટેબલ: Supervised vs Unsupervised Learning}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1304}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4130}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4565}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
પાસું
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Supervised Learning
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Unsupervised Learning
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Training Data} & Target outputs સાથે labeled data & Targets વિના
unlabeled data \\
\textbf{લક્ષ્ય} & Specific outcomes predict કરવા & Hidden patterns
discover કરવા \\
\textbf{ઉદાહરણો} & Classification, Regression & Clustering, Association
rules \\
\textbf{મૂલ્યાંકન} & Accuracy, precision, recall & Silhouette score, elbow
method \\
\textbf{Applications} & Email spam, price prediction & Customer
segmentation, anomaly detection \\
\end{longtable}
}

\end{solutionbox}
\begin{mnemonicbox}
``SU - Supervised Uses labels, Unsupervised Uncovers
patterns''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{પ્રશ્ન 2(બ) OR [4
ગુણ]}\label{uxaaauxab0uxab6uxaa8-2uxaac-or-4-uxa97uxaa3}

\textbf{હેલ્થકેર, ફાઇનાન્સ અને મેન્યુફેક્ચરિંગમાં AI applications વિશે બધું સમજાવો.}

\begin{solutionbox}

\textbf{ટેબલ: Industry પ્રમાણે AI Applications}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Industry
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Applications
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ફાયદા
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Healthcare} & Medical imaging, drug discovery, diagnosis &
Improved accuracy, faster treatment \\
\textbf{Finance} & Fraud detection, algorithmic trading, credit scoring
& Risk reduction, automated decisions \\
\textbf{Manufacturing} & Quality control, predictive maintenance,
robotics & Efficiency, cost reduction \\
\end{longtable}
}

\textbf{Healthcare ઉદાહરણો:}

\begin{itemize}
\tightlist
\item
  \textbf{Medical Imaging}: X-rays અને MRIs માં AI cancer detect કરે છે
\item
  \textbf{Drug Discovery}: AI નવી medicine development ને accelerate કરે છે
\end{itemize}

\textbf{Finance ઉદાહરણો:}

\begin{itemize}
\tightlist
\item
  \textbf{Fraud Detection}: Real-time transaction monitoring
\item
  \textbf{Robo-advisors}: Automated investment management
\end{itemize}

\textbf{Manufacturing ઉદાહરણો:}

\begin{itemize}
\tightlist
\item
  \textbf{Quality Control}: Automated defect detection
\item
  \textbf{Predictive Maintenance}: Equipment failure prediction
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``HFM - Health, Finance, Manufacturing benefit from
AI''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{પ્રશ્ન 2(ક) OR [7
ગુણ]}\label{uxaaauxab0uxab6uxaa8-2uxa95-or-7-uxa97uxaa3}

\textbf{સિન્ટેક્ટિક વિશ્લેષણ શું છે અને તે લેક્સિકલ વિશ્લેષણથી કેવી રીતે અલગ છે?}

\begin{solutionbox}

\textbf{Syntactic Analysis} sentences ના grammatical structure ને examine
કરે છે, જ્યારે \textbf{Lexical Analysis} text ને meaningful tokens માં break
કરે છે.

\textbf{ટેબલ: Lexical vs Syntactic Analysis}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4048}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4524}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
પાસું
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Lexical Analysis
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Syntactic Analysis
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{હેતુ} & Text ને words માં tokenize કરવા & Grammatical structure
parse કરવા \\
\textbf{Input} & Raw text & Lexical analysis થી tokens \\
\textbf{Output} & Tokens, part-of-speech tags & Parse trees, grammar
rules \\
\textbf{ધ્યાન} & Individual words & Sentence structure \\
\textbf{ઉદાહરણ} & ``The cat runs'' \rightarrow [The, cat, runs] & Noun-verb
relationship દર્શાવતું parse tree બનાવે છે \\
\end{longtable}
}

\textbf{Process Flow:}

\begin{center}
\textbf{Mermaid Diagram (Code)}
\begin{verbatim}
{Shaded}
{Highlighting}[]
graph LR
    A[Raw Text] {-{-}{} B[Lexical Analysis]}
    B {-{-}{} C[Tokens]}
    C {-{-}{} D[Syntactic Analysis]}
    D {-{-}{} E[Parse Tree]}
{Highlighting}
{Shaded}
\end{verbatim}
\end{center}

\textbf{ઉદાહરણ:}

\begin{itemize}
\tightlist
\item
  \textbf{Lexical}: ``She reads books'' \rightarrow [``She'', ``reads'',
  ``books'']
\item
  \textbf{Syntactic}: ``She'' ને subject, ``reads'' ને verb, ``books'' ને
  object તરીકે identify કરે છે
\end{itemize}

\textbf{મુખ્ય તફાવતો:}

\begin{itemize}
\tightlist
\item
  \textbf{Scope}: Lexical words પર કામ કરે છે, Syntactic sentence
  structure પર
\item
  \textbf{જટિલતા}: Syntactic analysis lexical કરતાં વધુ complex છે
\item
  \textbf{Dependencies}: Syntactic analysis lexical analysis પર depend
  કરે છે
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``LEX-SYN: LEXical extracts, SYNtactic structures''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{પ્રશ્ન 3(અ) [3
ગુણ]}\label{uxaaauxab0uxab6uxaa8-3uxa85-3-uxa97uxaa3}

\textbf{પ્રતિક્રિયાશીલ મશીનોની વિવિધ લાક્ષણિકતાઓની યાદી બનાવો.}

\begin{solutionbox}

\textbf{ટેબલ: Reactive Machines ની લાક્ષણિકતાઓ}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
લાક્ષણિકતા & વર્ણન \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{કોઈ મેમરી નથી} & Past experiences store કરી શકતા નથી \\
\textbf{વર્તમાન-કેન્દ્રિત} & ફક્ત current input ને respond કરે છે \\
\textbf{નિર્ધારિત} & Same input માટે same output આપે છે \\
\textbf{કાર્ય-વિશિષ્ટ} & Particular functions માટે design કરેલ \\
\textbf{કોઈ શીખવું નથી} & Experience થી improve કરી શકતા નથી \\
\end{longtable}
}

\textbf{ઉદાહરણો:}

\begin{itemize}
\tightlist
\item
  \textbf{Deep Blue}: IBM નું chess computer
\item
  \textbf{Game AI}: Tic-tac-toe programs
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``REACT - Responds Exactly, Always Consistent
Tasks''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{પ્રશ્ન 3(બ) [4
ગુણ]}\label{uxaaauxab0uxab6uxaa8-3uxaac-4-uxa97uxaa3}

\textbf{તફાવત કરો: હકારાત્મક મજબૂતીકરણ v/s નકારાત્મક મજબૂતીકરણ}

\begin{solutionbox}

\textbf{ટેબલ: Positive vs Negative Reinforcement}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1200}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4400}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4400}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
પાસું
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Positive Reinforcement
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Negative Reinforcement
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{વ્યાખ્યા} & Good behavior માટે reward add કરવું & Good behavior માટે
penalty remove કરવું \\
\textbf{Action} & કંઈક desirable આપવું & કંઈક undesirable દૂર કરવું \\
\textbf{લક્ષ્ય} & Desired behavior increase કરવું & Desired behavior
increase કરવું \\
\textbf{ઉદાહરણ} & Correct answer માટે treat આપવું & Good performance માટે
extra work દૂર કરવું \\
\end{longtable}
}

\textbf{ડાયાગ્રામ:}

\begin{verbatim}
Positive Reinforcement:     Negative Reinforcement:
Good Behavior               Good Behavior
     +                           +
Add Reward                 Remove Penalty
     =                           =
Behavior Increases         Behavior Increases
\end{verbatim}

\textbf{મુખ્ય મુદ્દાઓ:}

\begin{itemize}
\tightlist
\item
  \textbf{બંને behavior increase કરે છે} પરંતુ વિવિધ mechanisms દ્વારા
\item
  \textbf{Positive કંઈક pleasant add કરે છે}
\item
  \textbf{Negative કંઈક unpleasant remove કરે છે}
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``PN - Positive adds Nice things, Negative removes
Nasty things''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{પ્રશ્ન 3(ક) [7
ગુણ]}\label{uxaaauxab0uxab6uxaa8-3uxa95-7-uxa97uxaa3}

\textbf{ટર્મ-ફ્રીક્વન્સી-ઇનવર્સ ડોક્યુમેન્ટ ફ્રીક્વન્સી (TF-IDF) word embedding
technique વિશે બધું સમજાવો.}

\begin{solutionbox}

\textbf{TF-IDF} એ numerical statistic છે જે documents ના collection માં કોઈ
document માટે word કેટલું important છે તે reflect કરે છે.

\textbf{ફોર્મ્યુલા:}

\begin{verbatim}
TF-IDF = TF(t,d) \times IDF(t)
જ્યાં:
TF(t,d) = (Document d માં term t કેટલી વાર આવે છે) / (Document d માં total terms)
IDF(t) = log((Total documents) / (Term t ધરાવતા documents))
\end{verbatim}

\textbf{ટેબલ: TF-IDF ઘટકો}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2381}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5238}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2381}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
ઘટક
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ફોર્મ્યુલા
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
હેતુ
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Term Frequency (TF)} & tf(t,d) = count(t,d) / & d \\
\textbf{Inverse Document Frequency (IDF)} & idf(t) = log(N / df(t)) &
Corpus માં word importance measure કરે છે \\
\textbf{TF-IDF Score} & tf-idf(t,d) = tf(t,d) \times idf(t) & Final word
importance score \\
\end{longtable}
}

\textbf{ઉદાહરણ Calculation:}

\begin{itemize}
\tightlist
\item
  Document: ``cat sat on mat''
\item
  Term: ``cat''
\item
  TF = 1/4 = 0.25
\item
  જો ``cat'' 10 માંથી 2 documents માં આવે છે: IDF = log(10/2) = 0.699
\item
  TF-IDF = 0.25 \times 0.699 = 0.175
\end{itemize}

\textbf{Applications:}

\begin{itemize}
\tightlist
\item
  \textbf{Information Retrieval}: Search engines
\item
  \textbf{Text Mining}: Document similarity
\item
  \textbf{Feature Extraction}: ML preprocessing
\end{itemize}

\textbf{ફાયદા:}

\begin{itemize}
\tightlist
\item
  \textbf{Common words ને low scores મળે છે} (the, and, is)
\item
  \textbf{Rare પરંતુ important words ને high scores મળે છે}
\item
  \textbf{સરળ અને અસરકારક} text analysis માટે
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``TF-IDF - Term Frequency \times Inverse Document
Frequency''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{પ્રશ્ન 3(અ) OR [3
ગુણ]}\label{uxaaauxab0uxab6uxaa8-3uxa85-or-3-uxa97uxaa3}

\textbf{ફઝી લોજિક સિસ્ટમ્સ વ્યાખ્યાયિત કરો. તેના મુખ્ય ઘટકોની ચર્ચા કરો.}

\begin{solutionbox}

\textbf{Fuzzy Logic Systems} uncertainty અને partial truth handle કરે છે,
completely true અને completely false વચ્ચે values allow કરે છે.

\textbf{ટેબલ: Fuzzy Logic ઘટકો}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4500}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
ઘટક
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
કાર્ય
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ઉદાહરણ
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Fuzzifier} & Crisp inputs ને fuzzy sets માં convert કરે છે &
Temperature 75^\circF \rightarrow ``Warm'' (0.7) \\
\textbf{Rule Base} & If-then fuzzy rules ધરાવે છે & IF temp is warm THEN
fan is medium \\
\textbf{Inference Engine} & Inputs પર fuzzy rules apply કરે છે & Multiple
rules combine કરે છે \\
\textbf{Defuzzifier} & Fuzzy output ને crisp value માં convert કરે છે &
``Medium speed'' \rightarrow 60\% fan speed \\
\end{longtable}
}

\textbf{મુખ્ય લાક્ષણિકતાઓ:}

\begin{itemize}
\tightlist
\item
  \textbf{Membership Functions}: Belonging ની degree (0 થી 1)
\item
  \textbf{Linguistic Variables}: Human-like terms (hot, cold, warm)
\item
  \textbf{Fuzzy Rules}: Fuzzy conditions સાથે IF-THEN statements
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``FRID - Fuzzifier, Rules, Inference, Defuzzifier''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{પ્રશ્ન 3(બ) OR [4
ગુણ]}\label{uxaaauxab0uxab6uxaa8-3uxaac-or-4-uxa97uxaa3}

\textbf{મજબૂતીકરણ શિક્ષણના ઘટકો સમજાવો: નીતિ, પુરસ્કાર સંકેત, મૂલ્ય કાર્ય, મોડેલ}

\begin{solutionbox}

\textbf{ટેબલ: Reinforcement Learning ઘટકો}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2632}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4737}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2632}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
ઘટક
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
વ્યાખ્યા
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
હેતુ
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Policy (નીતિ)} & Actions select કરવાની strategy & Agent ના
behavior ને define કરે છે \\
\textbf{Reward Signal (પુરસ્કાર સંકેત)} & Environment તરફથી feedback &
Good/bad actions indicate કરે છે \\
\textbf{Value Function (મૂલ્ય કાર્ય)} & Expected future rewards & Long-term
benefit estimate કરે છે \\
\textbf{Model (મોડેલ)} & Environment નું agent representation & Next state
અને reward predict કરે છે \\
\end{longtable}
}

\textbf{વિગતવાર સમજૂતી:}

\textbf{Policy (π):}

\begin{itemize}
\tightlist
\item
  \textbf{Deterministic}: π(s) = a (એક state માટે એક action)
\item
  \textbf{Stochastic}: π(a\textbar s) = state s માં action a ની
  probability
\end{itemize}

\textbf{Reward Signal (R):}

\begin{itemize}
\tightlist
\item
  Environment તરફથી \textbf{immediate feedback}
\item
  Good actions માટે \textbf{positive}, bad actions માટે \textbf{negative}
\end{itemize}

\textbf{Value Function (V):}

\begin{itemize}
\tightlist
\item
  \textbf{State Value}: V(s) = state s થી expected return
\item
  \textbf{Action Value}: Q(s,a) = state s માં action a થી expected return
\end{itemize}

\textbf{Model:}

\begin{itemize}
\tightlist
\item
  \textbf{Transition Model}: P(s'\textbar s,a) = next state ની
  probability
\item
  \textbf{Reward Model}: R(s,a,s') = expected reward
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``PRVM - Policy chooses, Reward judges, Value
estimates, Model predicts''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{પ્રશ્ન 3(ક) OR [7
ગુણ]}\label{uxaaauxab0uxab6uxaa8-3uxa95-or-7-uxa97uxaa3}

\textbf{તફાવત કરો: આવૃત્તિ-આધારિત v/s આગાહી-આધારિત word embedding તકનીકો.}

\begin{solutionbox}

\textbf{ટેબલ: Frequency-based vs Prediction-based Word Embeddings}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4500}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
પાસું
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Frequency-based
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Prediction-based
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Approach} & Count-based statistics & Neural network
prediction \\
\textbf{ઉદાહરણો} & TF-IDF, Co-occurrence Matrix & Word2Vec, GloVe \\
\textbf{Computation} & Matrix factorization & Gradient descent \\
\textbf{Context} & Global statistics & Local context windows \\
\textbf{Scalability} & Matrix size દ્વારા limited & Vocabulary સાથે
scales \\
\textbf{Quality} & Basic semantic relationships & Rich semantic
relationships \\
\end{longtable}
}

\textbf{Frequency-based Methods:}

\begin{itemize}
\tightlist
\item
  \textbf{TF-IDF}: Term frequency \times Inverse document frequency
\item
  \textbf{Co-occurrence Matrix}: Word pair frequency counts
\item
  \textbf{LSA}: SVD વાપરીને Latent Semantic Analysis
\end{itemize}

\textbf{Prediction-based Methods:}

\begin{itemize}
\tightlist
\item
  \textbf{Word2Vec}: Skip-gram અને CBOW models
\item
  \textbf{GloVe}: Global Vectors for Word Representation
\item
  \textbf{FastText}: Subword information inclusion
\end{itemize}

\textbf{Code Comparison:}

\begin{verbatim}
\# Frequency{-based (TF{-}IDF)}
from sklearn.feature\_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()
tfidf\_matrix = vectorizer.fit\_transform(documents)

\# Prediction{-based (Word2Vec)}
from gensim.models import Word2Vec
model = Word2Vec(sentences, vector\_size=100, window=5)
\end{verbatim}

\textbf{ફાયદા:}

\textbf{Frequency-based:}

\begin{itemize}
\tightlist
\item
  \textbf{સરળ} અને interpretable
\item
  Small datasets માટે \textbf{ઝડપી} computation
\item
  Basic similarity tasks માટે \textbf{સારું}
\end{itemize}

\textbf{Prediction-based:}

\begin{itemize}
\tightlist
\item
  \textbf{Dense} vector representations
\item
  \textbf{બહેતર} semantic relationships
\item
  Large vocabularies માટે \textbf{scalable}
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``FP - Frequency counts, Prediction learns''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{પ્રશ્ન 4(અ) [3
ગુણ]}\label{uxaaauxab0uxab6uxaa8-4uxa85-3-uxa97uxaa3}

\textbf{પ્રતિક્રિયાશીલ મશીનની મુખ્ય લાક્ષણિકતાઓની યાદી બનાવો.}

\begin{solutionbox}

\textbf{ટેબલ: Reactive Machine મુખ્ય લાક્ષણિકતાઓ}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
લાક્ષણિકતા & વર્ણન \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Stateless} & Past interactions ની કોઈ memory નથી \\
\textbf{Reactive} & ફક્ત current inputs ને respond કરે છે \\
\textbf{Deterministic} & Same inputs માટે consistent outputs \\
\textbf{Specialized} & Specific tasks માટે designed \\
\textbf{Real-time} & Stimuli ને immediate response \\
\end{longtable}
}

\textbf{ઉદાહરણો:}

\begin{itemize}
\tightlist
\item
  \textbf{Deep Blue}: Chess-playing computer
\item
  \textbf{Google AlphaGo}: Go-playing system (early version)
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``SRDSR - Stateless, Reactive, Deterministic,
Specialized, Real-time''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{પ્રશ્ન 4(બ) [4
ગુણ]}\label{uxaaauxab0uxab6uxaa8-4uxaac-4-uxa97uxaa3}

\textbf{વિવિધ પૂર્વ-પ્રોસેસિંગ તકનીકોની સૂચિ બનાવો. તેમાંથી કોઈપણ એકને python
code વડે સમજાવો.}

\begin{solutionbox}

\textbf{ટેબલ: Text Pre-processing તકનીકો}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3913}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2174}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3913}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
તકનીક
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
હેતુ
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ઉદાહરણ
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Tokenization} & Text ને words માં split કરવું & ``Hello world'' \rightarrow
[``Hello'', ``world''] \\
\textbf{Stop Word Removal} & Common words remove કરવા & ``the'',
``and'', ``is'' remove કરવા \\
\textbf{Stemming} & Words ને root form માં reduce કરવા & ``running'' \rightarrow
``run'' \\
\textbf{Lemmatization} & Dictionary form માં convert કરવા & ``better'' \rightarrow
``good'' \\
\end{longtable}
}

\textbf{Stemming સમજૂતી:} Stemming suffixes remove કરીને words ને root form
માં reduce કરે છે.

\textbf{Stemming માટે Python Code:}

\begin{verbatim}
import nltk
from nltk.stem import PorterStemmer

\# Stemmer initialize કરો
stemmer = PorterStemmer()

\# Example words
words = ["running", "flies", "dogs", "churches", "studying"]

\# Stemming apply કરો
stemmed\_words = [stemmer.stem(word) for word in words]
print(stemmed\_words)
\# Output: [{run, fli, dog, church, studi]}
\end{verbatim}

\textbf{Stemming ના ફાયદા:}

\begin{itemize}
\tightlist
\item
  ML models માટે \textbf{vocabulary size reduce કરે છે}
\item
  Related words ને \textbf{together group કરે છે}
\item
  Text analysis efficiency \textbf{improve કરે છે}
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``TSSL - Tokenize, Stop-words, Stem, Lemmatize''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{પ્રશ્ન 4(ક) [7
ગુણ]}\label{uxaaauxab0uxab6uxaa8-4uxa95-7-uxa97uxaa3}

\textbf{Word2vec તકનીકને વિગતવાર પ્રકાશિત કરો.}

\begin{solutionbox}

\textbf{Word2Vec} એ neural network-based તકનીક છે જે context predict કરીને
words ના dense vector representations શીખે છે.

\textbf{ટેબલ: Word2Vec Architectures}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3421}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2632}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1842}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2105}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Architecture
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Approach
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Input
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Output
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Skip-gram} & Center word થી context predict કરે છે & Center word &
Context words \\
\textbf{CBOW} & Context થી center word predict કરે છે & Context words &
Center word \\
\end{longtable}
}

\textbf{Skip-gram Model:}

\begin{center}
\textbf{Mermaid Diagram (Code)}
\begin{verbatim}
{Shaded}
{Highlighting}[]
graph LR
    A[Input: Center Word] {-{-}{} B[Hidden Layer]}
    B {-{-}{} C[Output: Context Words]}
    C {-{-}{} D[Softmax Layer]}
    D {-{-}{} E[Probability Distribution]}
{Highlighting}
{Shaded}
\end{verbatim}
\end{center}

\textbf{Training Process:}

\begin{enumerate}
\tightlist
\item
  \textbf{Sliding Window}: Text પર window move કરો
\item
  \textbf{Word Pairs}: (center, context) pairs બનાવો
\item
  \textbf{Neural Network}: Context predict કરવા માટે train કરો
\item
  \textbf{Weight Matrix}: Word vectors extract કરો
\end{enumerate}

\textbf{મુખ્ય લાક્ષણિકતાઓ:}

\begin{itemize}
\tightlist
\item
  \textbf{Vector Size}: સામાન્ય રીતે 100-300 dimensions
\item
  \textbf{Window Size}: Context range (સામાન્ય રીતે 5-10 words)
\item
  \textbf{Negative Sampling}: Efficient training method
\item
  \textbf{Hierarchical Softmax}: Softmax નો alternative
\end{itemize}

\textbf{Mathematical Concept:}

\begin{verbatim}
Objective = max Σ log P(context|center)
જ્યાં P(context|center) = exp(v_context · v_center) / Σ exp(v_w · v_center)
\end{verbatim}

\textbf{Applications:}

\begin{itemize}
\tightlist
\item
  \textbf{Similarity}: Similar words શોધવા
\item
  \textbf{Analogies}: King - Man + Woman = Queen
\item
  \textbf{Clustering}: Semantic categories group કરવા
\item
  \textbf{Feature Engineering}: ML input features
\end{itemize}

\textbf{ફાયદા:}

\begin{itemize}
\tightlist
\item
  \textbf{Dense Representations}: Rich semantic information
\item
  \textbf{Semantic Relationships}: Word meanings capture કરે છે
\item
  \textbf{Arithmetic Properties}: Vector operations make sense
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``W2V - Words to Vectors via neural networks''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{પ્રશ્ન 4(અ) OR [3
ગુણ]}\label{uxaaauxab0uxab6uxaa8-4uxa85-or-3-uxa97uxaa3}

\textbf{નેચરલ લેંગ્વેજ પ્રોસેસિંગની કોઈપણ ચાર applications ની યાદી બનાવો. સ્પામ
શોધને વિગતવાર સમજાવો.}

\begin{solutionbox}

\textbf{ટેબલ: NLP Applications}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Application & વર્ણન \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Spam Detection} & Unwanted emails identify કરવા \\
\textbf{Sentiment Analysis} & Emotional tone determine કરવા \\
\textbf{Machine Translation} & Languages વચ્ચે translate કરવા \\
\textbf{Chatbots} & Automated conversation systems \\
\end{longtable}
}

\textbf{Spam Detection વિગતો:}

\textbf{Process:}

\begin{enumerate}
\tightlist
\item
  \textbf{Feature Extraction}: Email text ને numerical features માં
  convert કરો
\item
  \textbf{Classification}: ML algorithms વાપરીને classify કરો
\item
  \textbf{Decision}: Spam અથવા legitimate તરીકે mark કરો
\end{enumerate}

\textbf{વપરાયેલા Features:}

\begin{itemize}
\tightlist
\item
  \textbf{Word Frequency}: Spam keywords count
\item
  \textbf{Email Headers}: Sender information
\item
  \textbf{URL Analysis}: Suspicious links
\item
  \textbf{Text Patterns}: ALL CAPS, excessive punctuation
\end{itemize}

\textbf{Machine Learning Approach:}

\begin{verbatim}
\# Simplified spam detection
from sklearn.feature\_extraction.text import TfidfVectorizer
from sklearn.naive\_bayes import MultinomialNB

\# Emails ને features માં convert કરો
vectorizer = TfidfVectorizer()
X = vectorizer.fit\_transform(email\_texts)

\# Classifier train કરો
classifier = MultinomialNB()
classifier.fit(X, labels)  \# labels: 0=legitimate, 1=spam
\end{verbatim}

\end{solutionbox}
\begin{mnemonicbox}
``SMTP - Spam, Machine Translation, Sentiment,
Phishing detection''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{પ્રશ્ન 4(બ) OR [4
ગુણ]}\label{uxaaauxab0uxab6uxaa8-4uxaac-or-4-uxa97uxaa3}

\textbf{પ્રવચન સંકલન અને વ્યવહારિક વિશ્લેષણ વિશે સમજાવો.}

\begin{solutionbox}

\textbf{ટેબલ: Discourse Integration vs Pragmatic Analysis}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1304}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4565}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4130}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
પાસું
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Discourse Integration
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Pragmatic Analysis
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{ધ્યાન} & Text coherence અને structure & Context અને intention \\
\textbf{વ્યાપ્તિ} & Multiple sentences/paragraphs & Speaker નો intended
meaning \\
\textbf{ઘટકો} & Anaphora, cataphora, connectives & Implicature, speech
acts \\
\textbf{લક્ષ્ય} & Text flow understand કરવું & Real meaning understand
કરવું \\
\end{longtable}
}

\textbf{Discourse Integration:}

\begin{itemize}
\tightlist
\item
  \textbf{Anaphora Resolution}: ``John went to store. He bought milk.''
  (He = John)
\item
  \textbf{Cataphora}: ``Before he left, John locked the door.''
\item
  \textbf{Coherence}: Sentences વચ્ચે logical flow
\item
  \textbf{Cohesion}: Grammatical connections
\end{itemize}

\textbf{Pragmatic Analysis:}

\begin{itemize}
\tightlist
\item
  \textbf{Speech Acts}: Commands, requests, promises
\item
  \textbf{Implicature}: Literal કરતાં implied meanings
\item
  \textbf{Context Dependency}: Same words, different meanings
\item
  \textbf{Intention Recognition}: Speaker ખરેખર શું mean કરે છે
\end{itemize}

\textbf{ઉદાહરણો:}

\textbf{Discourse Integration:}

\begin{verbatim}
Text: "Mary owns a car. The vehicle is red."
Resolution: "vehicle" refers to "car"
\end{verbatim}

\textbf{Pragmatic Analysis:}

\begin{verbatim}
Statement: "Can you pass the salt?"
Literal: Ability વિશે question
Pragmatic: Salt pass કરવાની request
\end{verbatim}

\end{solutionbox}
\begin{mnemonicbox}
``DP - Discourse connects, Pragmatics interprets
context''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{પ્રશ્ન 4(ક) OR [7
ગુણ]}\label{uxaaauxab0uxab6uxaa8-4uxa95-or-7-uxa97uxaa3}

\textbf{બેગ ઓફ વર્ડ્સ word embedding technique વિશે વિગતવાર ચર્ચા કરો.}

\begin{solutionbox}

\textbf{Bag of Words (BoW)} એ simple text representation method છે જે
documents ને unordered collections of words તરીકે treat કરે છે.

\textbf{ટેબલ: BoW Process}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2727}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3182}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4091}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
સ્ટેપ
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
વર્ણન
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ઉદાહરણ
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Vocabulary Creation} & બધા unique words collect કરો &
[``cat'', ``sat'', ``mat'', ``dog''] \\
\textbf{Vector Creation} & Word occurrences count કરો & [1, 1, 1, 0]
for ``cat sat mat'' \\
\textbf{Document Representation} & દરેક document vector બને છે & Multiple
documents \rightarrow Matrix \\
\end{longtable}
}

\textbf{ઉદાહરણ:}

\begin{verbatim}
Documents:
1. "The cat sat on the mat"
2. "The dog ran in the park"

Vocabulary: [the, cat, sat, on, mat, dog, ran, in, park]

Document Vectors:
Doc1: [2, 1, 1, 1, 1, 0, 0, 0, 0]
Doc2: [2, 0, 0, 0, 0, 1, 1, 1, 1]
\end{verbatim}

\textbf{Python Implementation:}

\begin{verbatim}
from sklearn.feature\_extraction.text import CountVectorizer

documents = [
    "The cat sat on the mat",
    "The dog ran in the park"
]

vectorizer = CountVectorizer()
bow\_matrix = vectorizer.fit\_transform(documents)
vocab = vectorizer.get\_feature\_names\_out()

print("Vocabulary:", vocab)
print("BoW Matrix:", bow\_matrix.toarray())
\end{verbatim}

\textbf{ફાયદા:}

\begin{itemize}
\tightlist
\item
  \textbf{સરળતા}: Understand અને implement કરવા માટે સરળ
\item
  \textbf{Interpretability}: Clear word-count relationship
\item
  \textbf{અસરકારકતા}: ઘણા tasks માટે સારું કામ કરે છે
\end{itemize}

\textbf{ગેરફાયદા:}

\begin{itemize}
\tightlist
\item
  \textbf{કોઈ Word Order નથી}: ``cat sat mat'' = ``mat sat cat''
\item
  \textbf{Sparse Vectors}: Large vocabularies માં ઘણા zeros
\item
  \textbf{કોઈ Semantics નથી}: Word meanings ની કોઈ understanding નથી
\item
  \textbf{High Dimensionality}: Vocabulary size સાથે scales
\end{itemize}

\textbf{વિવિધતાઓ:}

\begin{itemize}
\tightlist
\item
  \textbf{Binary BoW}: Word present હોય તો 1, absent હોય તો 0
\item
  \textbf{TF-IDF BoW}: Term frequency \times Inverse document frequency
\item
  \textbf{N-gram BoW}: Word sequences consider કરે છે
\end{itemize}

\textbf{Applications:}

\begin{itemize}
\tightlist
\item
  \textbf{Document Classification}: Spam detection
\item
  \textbf{Information Retrieval}: Search engines
\item
  \textbf{Text Clustering}: Similar documents group કરવા
\item
  \textbf{Feature Engineering}: ML models માટે input
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``BOW - Bag Of Words counts occurrences''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{પ્રશ્ન 5(અ) [3
ગુણ]}\label{uxaaauxab0uxab6uxaa8-5uxa85-3-uxa97uxaa3}

\textbf{ન્યુરલ નેટવર્કમાં સક્રિયકરણ કાર્યોની ભૂમિકા શું છે?}

\begin{solutionbox}

\textbf{ટેબલ: Activation Function ભૂમિકાઓ}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
ભૂમિકા & વર્ણન \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{બિન-રેખીયતા (Non-linearity)} & Complex patterns શીખવાને enable કરે
છે \\
\textbf{આઉટપુટ નિયંત્રણ} & Neuron firing threshold determine કરે છે \\
\textbf{Gradient Flow} & Backpropagation efficiency ને affect કરે છે \\
\textbf{રેન્જ મર્યાદા} & Output values ને bounds કરે છે \\
\end{longtable}
}

\textbf{મુખ્ય કાર્યો:}

\begin{itemize}
\tightlist
\item
  \textbf{Decision Making}: Neuron activate થવો જોઈએ કે નહીં
\item
  \textbf{Pattern Recognition}: Complex decision boundaries enable કરે છે
\item
  \textbf{Signal Processing}: Weighted inputs ને transform કરે છે
\end{itemize}

\textbf{સામાન્ય Activation Functions:}

\begin{itemize}
\tightlist
\item
  \textbf{ReLU}: f(x) = max(0, x) - સરળ અને efficient
\item
  \textbf{Sigmoid}: f(x) = 1/(1 + e\^{}-x) - Smooth probability output
\item
  \textbf{Tanh}: f(x) = (e\^{}x - e\textsuperscript{-x)/(e}x + e\^{}-x)
  - Zero-centered
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``NOGL - Non-linearity, Output control, Gradient
flow, Limiting range''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{પ્રશ્ન 5(બ) [4
ગુણ]}\label{uxaaauxab0uxab6uxaa8-5uxaac-4-uxa97uxaa3}

\textbf{ન્યુરલ નેટવર્કના આર્કિટેક્ચરનું વિગતવાર વર્ણન કરો.}

\begin{solutionbox}

\textbf{ટેબલ: Neural Network Architecture ઘટકો}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4500}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
ઘટક
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
કાર્ય
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ઉદાહરણ
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Input Layer} & Input data receive કરે છે & Features/pixels \\
\textbf{Hidden Layers} & Information process કરે છે & Pattern
recognition \\
\textbf{Output Layer} & Final result produce કરે છે &
Classification/prediction \\
\textbf{Connections} & Layers વચ્ચે neurons ને link કરે છે & Weighted
edges \\
\end{longtable}
}

\textbf{Architecture ડાયાગ્રામ:}

\begin{center}
\textbf{Mermaid Diagram (Code)}
\begin{verbatim}
{Shaded}
{Highlighting}[]
graph LR
    A[Input Layer] {-{-}{} B[Hidden Layer 1]}
    B {-{-}{} C[Hidden Layer 2]  }
    C {-{-}{} D[Output Layer]}
    
    A1[X1] {-{-}{} B1[H1]}
    A2[X2] {-{-}{} B1}
    A1 {-{-}{} B2[H2]}
    A2 {-{-}{} B2}
    
    B1 {-{-}{} D1[Y1]}
    B2 {-{-}{} D1}
{Highlighting}
{Shaded}
\end{verbatim}
\end{center}

\textbf{Layer વિગતો:}

\begin{itemize}
\tightlist
\item
  \textbf{Input Layer}: Neurons ની સંખ્યા = features ની સંખ્યા
\item
  \textbf{Hidden Layers}: Variable neurons, complexity માટે multiple
  layers
\item
  \textbf{Output Layer}: Neurons ની સંખ્યા = classes/outputs ની સંખ્યા
\end{itemize}

\textbf{Information Flow:}

\begin{enumerate}
\tightlist
\item
  \textbf{Forward Pass}: Input \rightarrow Hidden \rightarrow Output
\item
  \textbf{Weighted Sum}: Σ(wi \times xi + bias)
\item
  \textbf{Activation}: Activation function apply કરો
\item
  \textbf{Output}: Final prediction/classification
\end{enumerate}

\end{solutionbox}
\begin{mnemonicbox}
``IHOC - Input, Hidden, Output, Connections''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{પ્રશ્ન 5(ક) [7
ગુણ]}\label{uxaaauxab0uxab6uxaa8-5uxa95-7-uxa97uxaa3}

\textbf{નેચરલ લેંગ્વેજ પ્રોસેસિંગમાં અસ્પષ્ટતાના પ્રકારોની યાદી બનાવો અને સમજાવો.}

\begin{solutionbox}

\textbf{Ambiguity} NLP માં ત્યારે થાય છે જ્યારે text ના multiple possible
interpretations હોય છે, જે automatic understanding ને challenging બનાવે છે.

\textbf{ટેબલ: NLP Ambiguities ના પ્રકારો}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
પ્રકાર
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
વ્યાખ્યા
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ઉદાહરણ
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ઉકેલ
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Lexical} & Word ના multiple meanings & ``Bank''
(river/financial) & Context analysis \\
\textbf{Syntactic} & Multiple parse structures & ``I saw her duck'' &
Grammar rules \\
\textbf{Semantic} & Multiple sentence meanings & ``Visiting relatives
can be boring'' & Semantic analysis \\
\textbf{Pragmatic} & Context-dependent meaning & ``Can you pass salt?''
& Intent recognition \\
\textbf{Referential} & Unclear pronoun reference & ``John told Bill he
was late'' & Anaphora resolution \\
\end{longtable}
}

\textbf{વિગતવાર સમજૂતીઓ:}

\textbf{Lexical Ambiguity:}

\begin{itemize}
\tightlist
\item
  \textbf{Homonyms}: Same spelling, different meanings
\item
  ઉદાહરણ: ``I went to the bank'' (financial institution vs.~river bank)
\item
  \textbf{ઉકેલ}: Context વાપરીને word sense disambiguation
\end{itemize}

\textbf{Syntactic Ambiguity:}

\begin{itemize}
\tightlist
\item
  \textbf{Multiple Parse Trees}: Same sentence, different structures
\item
  ઉદાહરણ: ``I saw the man with the telescope''

  \begin{itemize}
  \tightlist
  \item
    મેં telescope વાપરીને man જોયો
  \item
    મેં telescope વાળા man ને જોયો
  \end{itemize}
\item
  \textbf{ઉકેલ}: Statistical parsing, grammar preferences
\end{itemize}

\textbf{Semantic Ambiguity:}

\begin{itemize}
\tightlist
\item
  \textbf{Multiple Interpretations}: Same structure, different meanings
\item
  ઉદાહરણ: ``Visiting relatives can be boring''

  \begin{itemize}
  \tightlist
  \item
    Relatives ને visit કરવા જવું boring છે
  \item
    Visit કરવા આવતા relatives boring છે
  \end{itemize}
\item
  \textbf{ઉકેલ}: Semantic role labeling
\end{itemize}

\textbf{Pragmatic Ambiguity:}

\begin{itemize}
\tightlist
\item
  \textbf{Context-dependent}: Situation પર meaning depend કરે છે
\item
  ઉદાહરણ: ``It's cold here'' (statement vs.~window બંધ કરવાની request)
\item
  \textbf{ઉકેલ}: Dialogue systems, context modeling
\end{itemize}

\textbf{Referential Ambiguity:}

\begin{itemize}
\tightlist
\item
  \textbf{Unclear References}: Multiple possible antecedents સાથે
  pronouns
\item
  ઉદાહરણ: ``John told Bill that he was promoted'' (કોને promotion મળ્યો?)
\item
  \textbf{ઉકેલ}: Coreference resolution algorithms
\end{itemize}

\textbf{Resolution Strategies:}

\begin{center}
\textbf{Mermaid Diagram (Code)}
\begin{verbatim}
{Shaded}
{Highlighting}[]
graph LR
    A[Ambiguous Text] {-{-}{} B[Context Analysis]}
    A {-{-}{} C[Statistical Models]}
    A {-{-}{} D[Knowledge Bases]}
    B {-{-}{} E[Disambiguation]}
    C {-{-}{} E}
    D {-{-}{} E}
    E {-{-}{} F[Clear Interpretation]}
{Highlighting}
{Shaded}
\end{verbatim}
\end{center}

\textbf{NLP Systems પર Impact:}

\begin{itemize}
\tightlist
\item
  \textbf{Machine Translation}: ખોટા word choices
\item
  \textbf{Information Retrieval}: Irrelevant results
\item
  \textbf{Question Answering}: Incorrect responses
\item
  \textbf{Chatbots}: Misunderstood queries
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``LSSPR - Lexical, Syntactic, Semantic, Pragmatic,
Referential''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{પ્રશ્ન 5(અ) OR [3
ગુણ]}\label{uxaaauxab0uxab6uxaa8-5uxa85-or-3-uxa97uxaa3}

\textbf{ન્યુરલ નેટવર્કમાં ઉપયોગમાં લેવાતા કેટલાક લોકપ્રિય સક્રિયકરણ કાર્યોના નામોની
સૂચિ બનાવો.}

\begin{solutionbox}

\textbf{ટેબલ: લોકપ્રિય Activation Functions}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2778}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1944}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1944}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Function
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ફોર્મ્યુલા
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Range
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
વપરાશ
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{ReLU} & f(x) = max(0, x) & [0, \infty) & Hidden layers \\
\textbf{Sigmoid} & f(x) = 1/(1 + e\^{}-x) & (0, 1) & Binary
classification \\
\textbf{Tanh} & f(x) = (e\^{}x - e\textsuperscript{-x)/(e}x + e\^{}-x) &
(-1, 1) & Hidden layers \\
\textbf{Softmax} & f(xi) = e\^{}xi / Σe\^{}xj & (0, 1) & Multi-class
output \\
\textbf{Leaky ReLU} & f(x) = max(0.01x, x) & (-\infty, \infty) & Dead neurons
solve કરવા \\
\end{longtable}
}

\textbf{લોકપ્રિય Functions:}

\begin{itemize}
\tightlist
\item
  \textbf{ReLU}: Hidden layers માં સૌથી વધુ વપરાતું
\item
  \textbf{Sigmoid}: Binary problems માટે traditional choice
\item
  \textbf{Tanh}: Sigmoid નો zero-centered alternative
\item
  \textbf{Softmax}: Multi-class classification માટે standard
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``RSTSL - ReLU, Sigmoid, Tanh, Softmax, Leaky ReLU''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{પ્રશ્ન 5(બ) OR [4
ગુણ]}\label{uxaaauxab0uxab6uxaa8-5uxaac-or-4-uxa97uxaa3}

\textbf{કૃત્રિમ ન્યુરલ નેટવર્કમાં શીખવાની પ્રક્રિયા સમજાવો.}

\begin{solutionbox}

\textbf{Learning Process} neural networks માં iterative training દ્વારા
error minimize કરવા માટે weights અને biases ને adjust કરવાનો સમાવેશ કરે છે.

\textbf{ટેબલ: Learning Process Steps}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2727}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4091}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3182}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
સ્ટેપ
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
પ્રક્રિયા
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
વર્ણન
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Initialize} & Random weights & Small random values સાથે start
કરો \\
\textbf{Forward Pass} & Output calculate કરો & Network દ્વારા input
propagate કરો \\
\textbf{Calculate Error} & Target સાથે compare કરો & Loss function
વાપરો \\
\textbf{Backward Pass} & Gradients calculate કરો & Backpropagation
વાપરો \\
\textbf{Update Weights} & Parameters adjust કરો & Gradient descent apply
કરો \\
\textbf{Repeat} & Process iterate કરો & Convergence સુધી \\
\end{longtable}
}

\textbf{Learning Algorithm Flow:}

\begin{center}
\textbf{Mermaid Diagram (Code)}
\begin{verbatim}
{Shaded}
{Highlighting}[]
graph LR
    A[Weights Initialize કરો] {-{-}{} B[Forward Pass]}
    B {-{-}{} C[Loss Calculate કરો]}
    C {-{-}{} D[Backward Pass]}
    D {-{-}{} E[Weights Update કરો]}
    E {-{-}{} F\{Converged?\}}
    F {-{-}{}|ના| B}
    F {-{-}{}|હા| G[Training Complete]}
{Highlighting}
{Shaded}
\end{verbatim}
\end{center}

\textbf{Mathematical Foundation:}

\begin{itemize}
\tightlist
\item
  \textbf{Loss Function}: L = ½(target - output)^{2}
\item
  \textbf{Gradient}: \partialL/\partialw = error \times input
\item
  \textbf{Weight Update}: w\_new = w\_old - η \times gradient
\item
  \textbf{Learning Rate}: η update step size control કરે છે
\end{itemize}

\textbf{Learning ના પ્રકારો:}

\begin{itemize}
\tightlist
\item
  \textbf{Supervised}: Labeled examples થી શીખવું
\item
  \textbf{Batch Learning}: બધા samples પછી update
\item
  \textbf{Online Learning}: દરેક sample પછી update
\item
  \textbf{Mini-batch}: Small batches પછી update
\end{itemize}

\textbf{મુખ્ય વિભાવનાઓ:}

\begin{itemize}
\tightlist
\item
  \textbf{Epoch}: Training data દ્વારા એક complete pass
\item
  \textbf{Convergence}: જ્યારે error ઘટવાનું બંધ થાય
\item
  \textbf{Overfitting}: Training data memorize કરવું
\item
  \textbf{Regularization}: Overfitting prevent કરવાની techniques
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``IFCBU - Initialize, Forward, Calculate, Backward,
Update''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{પ્રશ્ન 5(ક) OR [7
ગુણ]}\label{uxaaauxab0uxab6uxaa8-5uxa95-or-7-uxa97uxaa3}

\textbf{નેચરલ લેંગ્વેજ પ્રોસેસિંગના વિવિધ ફાયદા અને ગેરફાયદાની યાદી બનાવો.}

\begin{solutionbox}

\textbf{ટેબલ: NLP ફાયદા અને ગેરફાયદા}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
ફાયદા & ગેરફાયદા \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{સ્વચાલિત ટેક્સ્ટ વિશ્લેષણ} & \textbf{અસ્પષ્ટતા હેન્ડલિંગ} \\
\textbf{ભાષા અનુવાદ} & \textbf{સંદર્ભ સમજ} \\
\textbf{માનવ-કમ્પ્યુટર ક્રિયાપ્રતિક્રિયા} & \textbf{સાંસ્કૃતિક સૂત્રધારતા} \\
\textbf{માહિતી નિષ્કર્ષણ} & \textbf{કોમ્પ્યુટેશનલ જટિલતા} \\
\textbf{ભાવના વિશ્લેષણ} & \textbf{ડેટા આવશ્યકતાઓ} \\
\end{longtable}
}

\textbf{વિગતવાર ફાયદા:}

\textbf{બિઝનેસ ફાયદા:}

\begin{itemize}
\tightlist
\item
  \textbf{કસ્ટમર સેવા}: Automated chatbots અને support
\item
  \textbf{કન્ટેન્ટ વિશ્લેષણ}: Social media monitoring
\item
  \textbf{ડોક્યુમેન્ટ પ્રોસેસિંગ}: Automated summarization
\item
  \textbf{સર્ચ એન્હાન્સમેન્ટ}: બહેતર information retrieval
\end{itemize}

\textbf{તકનીકી ફાયદા:}

\begin{itemize}
\tightlist
\item
  \textbf{Scalability}: મોટા text volumes process કરી શકે છે
\item
  \textbf{સુસંગતતા}: Documents પર uniform analysis
\item
  \textbf{ઝડપ}: Human text processing કરતાં વધુ ઝડપી
\item
  \textbf{Integration}: Existing systems સાથે કામ કરે છે
\end{itemize}

\textbf{વિગતવાર ગેરફાયદા:}

\textbf{તકનીકી પડકારો:}

\begin{itemize}
\tightlist
\item
  \textbf{અસ્પષ્ટતા}: Text ના multiple interpretations
\item
  \textbf{Context Dependency}: Situation સાથે meaning બદલાય છે
\item
  \textbf{Sarcasm/Irony}: Automatically detect કરવું મુશ્કેલ
\item
  \textbf{Domain Specificity}: નવા domains માટે models ને retraining જરૂરી
\end{itemize}

\textbf{રિસોર્સ આવશ્યકતાઓ:}

\begin{itemize}
\tightlist
\item
  \textbf{મોટા Datasets}: લાખો text samples જરૂરી
\item
  \textbf{કોમ્પ્યુટેશનલ પાવર}: Complex models ને GPUs જરૂરી
\item
  \textbf{એક્સપર્ટ નોલેજ}: Linguistics અને ML expertise જરૂરી
\item
  \textbf{Maintenance}: Models ને નિયમિત updates જરૂરી
\end{itemize}

\textbf{ગુણવત્તાની સમસ્યાઓ:}

\begin{itemize}
\tightlist
\item
  \textbf{ચોકસાઈ મર્યાદાઓ}: 100\% accurate નથી
\item
  \textbf{Bias સમસ્યાઓ}: Training data biases reflect કરે છે
\item
  \textbf{ભાષા અવરોધો}: કેટલીક languages માટે વધુ સારું કામ કરે છે
\item
  \textbf{Error Propagation}: Pipelines માં mistakes compound થાય છે
\end{itemize}

\textbf{Applications vs Challenges:}

\begin{center}
\textbf{Mermaid Diagram (Code)}
\begin{verbatim}
{Shaded}
{Highlighting}[]
graph TD
    A[NLP Applications] {-{-}{} B[Machine Translation]}
    A {-{-}{} C[Sentiment Analysis]  }
    A {-{-}{} D[Information Extraction]}
    
    E[NLP Challenges] {-{-}{} F[Ambiguity]}
    E {-{-}{} G[Context Understanding]}
    E {-{-}{} H[Cultural Nuances]}
{Highlighting}
{Shaded}
\end{verbatim}
\end{center}

\textbf{ભવિષ્યના સુધારાઓ:}

\begin{itemize}
\tightlist
\item
  \textbf{બહેતર Context Models}: Transformer architectures
\item
  \textbf{Multilingual Support}: Cross-language understanding
\item
  \textbf{Few-shot Learning}: ઓછા data requirements
\item
  \textbf{Explainable AI}: Model decisions ની understanding
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``ALICE vs ACHDR - Automated, Language, Interaction,
Content, Extraction vs Ambiguity, Context, Human-nuances, Data,
Resources''

\end{mnemonicbox}

\end{document}
