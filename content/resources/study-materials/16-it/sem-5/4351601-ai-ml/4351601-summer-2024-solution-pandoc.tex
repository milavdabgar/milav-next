\documentclass[10pt,a4paper]{article}
\input{../../../../../../latex-templates/gtu-solutions/preamble.tex}
\input{../../../../../../latex-templates/gtu-solutions/english-boxes.tex}

\begin{document}

\begin{center}
{\Huge\bfseries\color{headcolor} Subject Name Solutions}\\[5pt]
{\LARGE 4351601 -- Summer 2024}\\[3pt]
{\large Semester 1 Study Material}\\[3pt]
{\normalsize\textit{Detailed Solutions and Explanations}}
\end{center}

\vspace{10pt}

\subsection*{Question 1(a) [3 marks]}\label{q1a}

\textbf{What do you mean by Narrow AI or Weak AI?}

\begin{solutionbox}

\textbf{Narrow AI} or \textbf{Weak AI} refers to artificial intelligence
systems designed to perform specific, limited tasks within a narrow
domain.


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Narrow AI Characteristics}
\vspace{-10pt}
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Aspect & Description \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Scope} & Limited to specific tasks \\
\textbf{Intelligence} & Task-specific expertise \\
\textbf{Examples} & Siri, chess programs, recommendation systems \\
\textbf{Learning} & Pattern recognition within domain \\
\end{longtable}
}

\end{solutionbox}
\begin{mnemonicbox}
``Narrow = Specific Tasks Only''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{Question 1(b) [4 marks]}\label{q1b}

\textbf{Define: Classification, Regression, Clustering, Association
Analysis.}

\begin{solutionbox}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Machine Learning Techniques}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2895}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3158}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1579}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2368}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Technique
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Classification} & Predicts discrete categories/classes &
Supervised & Email spam detection \\
\textbf{Regression} & Predicts continuous numerical values & Supervised
& House price prediction \\
\textbf{Clustering} & Groups similar data points & Unsupervised &
Customer segmentation \\
\textbf{Association Analysis} & Finds relationships between variables &
Unsupervised & Market basket analysis \\
\end{longtable}
}

\end{solutionbox}
\begin{mnemonicbox}
``CRCA - Categories, Real-numbers, Clusters,
Associations''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{Question 1(c) [7 marks]}\label{q1c}

\textbf{Illuminate the three main components of neuron.}

\begin{solutionbox}

The three main components of a biological neuron that inspire artificial
neural networks are:

\textbf{Diagram:}

\begin{verbatim}
    Dendrites     Cell Body      Axon
        |            |           |
        v            v           v
    [Inputs] {-{-} [Processing] {-}{-} [Output]}
        |            |           |
    Receives     Integrates    Transmits
    signals      signals       signals
\end{verbatim}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Neuron Components}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3056}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2778}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4167}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Function
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AI Equivalent
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Dendrites} & Receive input signals from other neurons & Input
layer/weights \\
\textbf{Cell Body (Soma)} & Processes and integrates signals &
Activation function \\
\textbf{Axon} & Transmits output signals to other neurons & Output
connections \\
\end{longtable}
}

\textbf{Key Points:}

\begin{itemize}
\tightlist
\item
  \textbf{Dendrites}: Act as input receivers with varying connection
  strengths
\item
  \textbf{Cell Body}: Sums inputs and applies threshold function
\item
  \textbf{Axon}: Carries processed signal to next neurons
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``DCA - Dendrites Collect, Cell-body Calculates, Axon
Announces''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{Question 1(c) OR [7
marks]}\label{q1c}

\textbf{Explicate back propagation method in Artificial Neural Network.}

\begin{solutionbox}

\textbf{Back Propagation} is a supervised learning algorithm used to
train multi-layer neural networks by minimizing error through gradient
descent.

\textbf{Flowchart:}

\begin{center}
\textbf{Mermaid Diagram (Code)}
\begin{verbatim}
{Shaded}
{Highlighting}[]
graph LR
    A[Forward Pass] {-{-}{} B[Calculate Output]}
    B {-{-}{} C[Calculate Error]}
    C {-{-}{} D[Backward Pass]}
    D {-{-}{} E[Calculate Gradients]}
    E {-{-}{} F[Update Weights]}
    F {-{-}{} G\{Error Acceptable?\}}
    G {-{-}{}|No| A}
    G {-{-}{}|Yes| H[Training Complete]}
{Highlighting}
{Shaded}
\end{verbatim}
\end{center}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Back Propagation Steps}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3750}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3750}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Step
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Process
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formula
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Forward Pass} & Calculate outputs layer by layer & y = f(Σ(wi*xi
+ b)) \\
\textbf{Error Calculation} & Compute loss function & E = ½(target -
output)^{2} \\
\textbf{Backward Pass} & Calculate error gradients & δ = \partialE/\partialw \\
\textbf{Weight Update} & Adjust weights using learning rate & w\_new =
w\_old - η*δ \\
\end{longtable}
}

\textbf{Key Features:}

\begin{itemize}
\tightlist
\item
  \textbf{Gradient Descent}: Uses calculus to find minimum error
\item
  \textbf{Chain Rule}: Propagates error backward through layers
\item
  \textbf{Learning Rate}: Controls speed of weight updates
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``FEBU - Forward, Error, Backward, Update''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{Question 2(a) [3 marks]}\label{q2a}

\textbf{List out any five popular algorithms used in Machine Learning.}

\begin{solutionbox}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Popular ML Algorithms}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3667}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Algorithm
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Application
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Linear Regression} & Supervised & Prediction of continuous
values \\
\textbf{Decision Tree} & Supervised & Classification and regression \\
\textbf{K-Means Clustering} & Unsupervised & Data grouping \\
\textbf{Support Vector Machine} & Supervised & Classification with
margins \\
\textbf{Random Forest} & Supervised & Ensemble learning \\
\end{longtable}
}

\end{solutionbox}
\begin{mnemonicbox}
``LDKSR - Learn Data, Keep Samples, Run''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{Question 2(b) [4 marks]}\label{q2b}

\textbf{What is Expert System? List out its limitations and
applications.}

\begin{solutionbox}

\textbf{Expert System} is an AI program that mimics human expert
knowledge to solve complex problems in specific domains.


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Expert System Overview}
\vspace{-10pt}
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Aspect & Details \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Definition} & AI system with domain-specific expertise \\
\textbf{Components} & Knowledge base, inference engine, user
interface \\
\end{longtable}
}

\textbf{Applications:}

\begin{itemize}
\tightlist
\item
  \textbf{Medical Diagnosis}: Disease identification systems
\item
  \textbf{Financial Planning}: Investment advisory systems
\item
  \textbf{Fault Diagnosis}: Equipment troubleshooting
\end{itemize}

\textbf{Limitations:}

\begin{itemize}
\tightlist
\item
  \textbf{Limited Domain}: Works only in specific areas
\item
  \textbf{Knowledge Acquisition}: Difficult to extract expert knowledge
\item
  \textbf{Maintenance}: Hard to update and modify rules
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``EXPERT - Explains Problems, Executes Rules, Tests''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{Question 2(c) [7 marks]}\label{q2c}

\textbf{What is tokenization? Explain with suitable example.}

\begin{solutionbox}

\textbf{Tokenization} is the process of breaking down text into smaller
units called tokens (words, phrases, symbols) for NLP processing.


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Tokenization Types}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2143}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4643}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3214}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Word Tokenization} & Split by words & ``Hello world'' \rightarrow
[``Hello'', ``world''] \\
\textbf{Sentence Tokenization} & Split by sentences & ``Hi. How are
you?'' \rightarrow [``Hi.'', ``How are you?''] \\
\textbf{Subword Tokenization} & Split into subwords & ``unhappy'' \rightarrow
[``un'', ``happy''] \\
\end{longtable}
}

\textbf{Code Example:}

\begin{verbatim}
import nltk
text = "Natural Language Processing is amazing!"
tokens = nltk.word\_tokenize(text)
\# Output: [{Natural, Language, Processing, is, amazing, !]}
\end{verbatim}

\textbf{Process Flow:}

\begin{center}
\textbf{Mermaid Diagram (Code)}
\begin{verbatim}
{Shaded}
{Highlighting}[]
graph LR
    A[Raw Text] {-{-}{} B[Tokenization]}
    B {-{-}{} C[Clean Tokens]}
    C {-{-}{} D[Further Processing]}
{Highlighting}
{Shaded}
\end{verbatim}
\end{center}

\textbf{Key Benefits:}

\begin{itemize}
\tightlist
\item
  \textbf{Standardization}: Converts text to uniform format
\item
  \textbf{Analysis Ready}: Prepares text for ML algorithms
\item
  \textbf{Feature Extraction}: Enables statistical analysis
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``TOKEN - Text Operations Keep Everything
Normalized''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{Question 2(a) OR [3
marks]}\label{q2a}

\textbf{Compare Supervised and Unsupervised Learning.}

\begin{solutionbox}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Supervised vs Unsupervised Learning}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3958}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4375}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Supervised Learning
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Unsupervised Learning
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Training Data} & Labeled data with target outputs & Unlabeled
data without targets \\
\textbf{Goal} & Predict specific outcomes & Discover hidden patterns \\
\textbf{Examples} & Classification, Regression & Clustering, Association
rules \\
\textbf{Evaluation} & Accuracy, precision, recall & Silhouette score,
elbow method \\
\textbf{Applications} & Email spam, price prediction & Customer
segmentation, anomaly detection \\
\end{longtable}
}

\end{solutionbox}
\begin{mnemonicbox}
``SU - Supervised Uses labels, Unsupervised Uncovers
patterns''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{Question 2(b) OR [4
marks]}\label{q2b}

\textbf{Explain all about AI applications in Healthcare, Finance and
Manufacturing.}

\begin{solutionbox}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{AI Applications by Industry}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3030}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3939}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3030}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Industry
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Applications
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Benefits
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Healthcare} & Medical imaging, drug discovery, diagnosis &
Improved accuracy, faster treatment \\
\textbf{Finance} & Fraud detection, algorithmic trading, credit scoring
& Risk reduction, automated decisions \\
\textbf{Manufacturing} & Quality control, predictive maintenance,
robotics & Efficiency, cost reduction \\
\end{longtable}
}

\textbf{Healthcare Examples:}

\begin{itemize}
\tightlist
\item
  \textbf{Medical Imaging}: AI detects cancer in X-rays and MRIs
\item
  \textbf{Drug Discovery}: AI accelerates new medicine development
\end{itemize}

\textbf{Finance Examples:}

\begin{itemize}
\tightlist
\item
  \textbf{Fraud Detection}: Real-time transaction monitoring
\item
  \textbf{Robo-advisors}: Automated investment management
\end{itemize}

\textbf{Manufacturing Examples:}

\begin{itemize}
\tightlist
\item
  \textbf{Quality Control}: Automated defect detection
\item
  \textbf{Predictive Maintenance}: Equipment failure prediction
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``HFM - Health, Finance, Manufacturing benefit from
AI''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{Question 2(c) OR [7
marks]}\label{q2c}

\textbf{What is syntactic analysis and how it is differ from lexical
analysis?}

\begin{solutionbox}

\textbf{Syntactic Analysis} examines the grammatical structure of
sentences, while \textbf{Lexical Analysis} breaks text into meaningful
tokens.


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Lexical vs Syntactic Analysis}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1818}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3864}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4318}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Lexical Analysis
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Syntactic Analysis
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Purpose} & Tokenize text into words & Parse grammatical
structure \\
\textbf{Input} & Raw text & Tokens from lexical analysis \\
\textbf{Output} & Tokens, part-of-speech tags & Parse trees, grammar
rules \\
\textbf{Focus} & Individual words & Sentence structure \\
\textbf{Example} & ``The cat runs'' \rightarrow [The, cat, runs] & Creates
parse tree showing noun-verb relationship \\
\end{longtable}
}

\textbf{Process Flow:}

\begin{center}
\textbf{Mermaid Diagram (Code)}
\begin{verbatim}
{Shaded}
{Highlighting}[]
graph LR
    A[Raw Text] {-{-}{} B[Lexical Analysis]}
    B {-{-}{} C[Tokens]}
    C {-{-}{} D[Syntactic Analysis]}
    D {-{-}{} E[Parse Tree]}
{Highlighting}
{Shaded}
\end{verbatim}
\end{center}

\textbf{Example:}

\begin{itemize}
\tightlist
\item
  \textbf{Lexical}: ``She reads books'' \rightarrow [``She'', ``reads'',
  ``books'']
\item
  \textbf{Syntactic}: Identifies ``She'' as subject, ``reads'' as verb,
  ``books'' as object
\end{itemize}

\textbf{Key Differences:}

\begin{itemize}
\tightlist
\item
  \textbf{Scope}: Lexical works on words, Syntactic on sentence
  structure
\item
  \textbf{Complexity}: Syntactic analysis is more complex than lexical
\item
  \textbf{Dependencies}: Syntactic analysis depends on lexical analysis
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``LEX-SYN: LEXical extracts, SYNtactic structures''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{Question 3(a) [3 marks]}\label{q3a}

\textbf{List out various characteristics of Reactive machines.}

\begin{solutionbox}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Reactive Machines Characteristics}
\vspace{-10pt}
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Characteristic & Description \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{No Memory} & Cannot store past experiences \\
\textbf{Present-focused} & Responds only to current input \\
\textbf{Deterministic} & Same input produces same output \\
\textbf{Task-specific} & Designed for particular functions \\
\textbf{No Learning} & Cannot improve from experience \\
\end{longtable}
}

\textbf{Examples:}

\begin{itemize}
\tightlist
\item
  \textbf{Deep Blue}: IBM's chess computer
\item
  \textbf{Game AI}: Tic-tac-toe programs
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``REACT - Responds Exactly, Always Consistent Tasks''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{Question 3(b) [4 marks]}\label{q3b}

\textbf{Differentiate: Positive Reinforcement v/s Negative
Reinforcement.}

\begin{solutionbox}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Positive vs Negative Reinforcement}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1538}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4231}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4231}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Positive Reinforcement
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Negative Reinforcement
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Definition} & Adding reward for good behavior & Removing penalty
for good behavior \\
\textbf{Action} & Give something desirable & Take away something
undesirable \\
\textbf{Goal} & Increase desired behavior & Increase desired behavior \\
\textbf{Example} & Give treat for correct answer & Remove extra work for
good performance \\
\end{longtable}
}

\textbf{Diagram:}

\begin{verbatim}
Positive Reinforcement:     Negative Reinforcement:
Good Behavior               Good Behavior
     +                           +
Add Reward                 Remove Penalty
     =                           =
Behavior Increases         Behavior Increases
\end{verbatim}

\textbf{Key Points:}

\begin{itemize}
\tightlist
\item
  \textbf{Both increase behavior} but through different mechanisms
\item
  \textbf{Positive adds} something pleasant
\item
  \textbf{Negative removes} something unpleasant
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``PN - Positive adds Nice things, Negative removes
Nasty things''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{Question 3(c) [7 marks]}\label{q3c}

\textbf{Explain all about Term-Frequency-Inverse Document
Frequency(TF-IDF) word embedding technique.}

\begin{solutionbox}

\textbf{TF-IDF} is a numerical statistic that reflects how important a
word is to a document in a collection of documents.

\textbf{Formula:}

\begin{verbatim}
TF-IDF = TF(t,d) \times IDF(t)
Where:
TF(t,d) = (Number of times term t appears in document d) / (Total terms in document d)
IDF(t) = log((Total documents) / (Documents containing term t))
\end{verbatim}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{TF-IDF Components}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3793}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3103}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3103}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formula
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Term Frequency (TF)} & tf(t,d) = count(t,d) / & d \\
\textbf{Inverse Document Frequency (IDF)} & idf(t) = log(N / df(t)) &
Measures word importance across corpus \\
\textbf{TF-IDF Score} & tf-idf(t,d) = tf(t,d) \times idf(t) & Final word
importance score \\
\end{longtable}
}

\textbf{Example Calculation:}

\begin{itemize}
\tightlist
\item
  Document: ``cat sat on mat''
\item
  Term: ``cat''
\item
  TF = 1/4 = 0.25
\item
  If ``cat'' appears in 2 out of 10 documents: IDF = log(10/2) = 0.699
\item
  TF-IDF = 0.25 \times 0.699 = 0.175
\end{itemize}

\textbf{Applications:}

\begin{itemize}
\tightlist
\item
  \textbf{Information Retrieval}: Search engines
\item
  \textbf{Text Mining}: Document similarity
\item
  \textbf{Feature Extraction}: ML preprocessing
\end{itemize}

\textbf{Advantages:}

\begin{itemize}
\tightlist
\item
  \textbf{Common words get low scores} (the, and, is)
\item
  \textbf{Rare but important words get high scores}
\item
  \textbf{Simple and effective} for text analysis
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``TF-IDF - Term Frequency \times Inverse Document
Frequency''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{Question 3(a) OR [3
marks]}\label{q3a}

\textbf{Define Fuzzy Logic Systems. Discuss its key components.}

\begin{solutionbox}

\textbf{Fuzzy Logic Systems} handle uncertainty and partial truth,
allowing values between completely true and completely false.


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Fuzzy Logic Components}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3667}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Function
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Fuzzifier} & Converts crisp inputs to fuzzy sets & Temperature
75^\circF \rightarrow ``Warm'' (0.7) \\
\textbf{Rule Base} & Contains if-then fuzzy rules & IF temp is warm THEN
fan is medium \\
\textbf{Inference Engine} & Applies fuzzy rules to inputs & Combines
multiple rules \\
\textbf{Defuzzifier} & Converts fuzzy output to crisp value & ``Medium
speed'' \rightarrow 60\% fan speed \\
\end{longtable}
}

\textbf{Key Features:}

\begin{itemize}
\tightlist
\item
  \textbf{Membership Functions}: Degree of belonging (0 to 1)
\item
  \textbf{Linguistic Variables}: Human-like terms (hot, cold, warm)
\item
  \textbf{Fuzzy Rules}: IF-THEN statements with fuzzy conditions
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``FRID - Fuzzifier, Rules, Inference, Defuzzifier''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{Question 3(b) OR [4
marks]}\label{q3b}

\textbf{Explain elements of reinforcement learning: Policy, Reward
Signal, Value Function, Model}

\begin{solutionbox}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Reinforcement Learning Elements}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Element
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Policy} & Strategy for selecting actions & Defines agent's
behavior \\
\textbf{Reward Signal} & Feedback from environment & Indicates good/bad
actions \\
\textbf{Value Function} & Expected future rewards & Estimates long-term
benefit \\
\textbf{Model} & Agent's representation of environment & Predicts next
state and reward \\
\end{longtable}
}

\textbf{Detailed Explanation:}

\textbf{Policy (π):}

\begin{itemize}
\tightlist
\item
  \textbf{Deterministic}: π(s) = a (one action per state)
\item
  \textbf{Stochastic}: π(a\textbar s) = probability of action a in state
  s
\end{itemize}

\textbf{Reward Signal (R):}

\begin{itemize}
\tightlist
\item
  \textbf{Immediate feedback} from environment
\item
  \textbf{Positive} for good actions, \textbf{negative} for bad actions
\end{itemize}

\textbf{Value Function (V):}

\begin{itemize}
\tightlist
\item
  \textbf{State Value}: V(s) = expected return from state s
\item
  \textbf{Action Value}: Q(s,a) = expected return from action a in state
  s
\end{itemize}

\textbf{Model:}

\begin{itemize}
\tightlist
\item
  \textbf{Transition Model}: P(s'\textbar s,a) = probability of next
  state
\item
  \textbf{Reward Model}: R(s,a,s') = expected reward
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``PRVM - Policy chooses, Reward judges, Value
estimates, Model predicts''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{Question 3(c) OR [7
marks]}\label{q3c}

\textbf{Differentiate: frequency-based v/s prediction-based word
embedding techniques.}

\begin{solutionbox}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Frequency-based vs Prediction-based Word Embeddings}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1905}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3810}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4286}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Frequency-based
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Prediction-based
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Approach} & Count-based statistics & Neural network
prediction \\
\textbf{Examples} & TF-IDF, Co-occurrence Matrix & Word2Vec, GloVe \\
\textbf{Computation} & Matrix factorization & Gradient descent \\
\textbf{Context} & Global statistics & Local context windows \\
\textbf{Scalability} & Limited by matrix size & Scales with
vocabulary \\
\textbf{Quality} & Basic semantic relationships & Rich semantic
relationships \\
\end{longtable}
}

\textbf{Frequency-based Methods:}

\begin{itemize}
\tightlist
\item
  \textbf{TF-IDF}: Term frequency \times Inverse document frequency
\item
  \textbf{Co-occurrence Matrix}: Word pair frequency counts
\item
  \textbf{LSA}: Latent Semantic Analysis using SVD
\end{itemize}

\textbf{Prediction-based Methods:}

\begin{itemize}
\tightlist
\item
  \textbf{Word2Vec}: Skip-gram and CBOW models
\item
  \textbf{GloVe}: Global Vectors for Word Representation
\item
  \textbf{FastText}: Subword information inclusion
\end{itemize}

\textbf{Code Comparison:}

\begin{verbatim}
\# Frequency{-based (TF{-}IDF)}
from sklearn.feature\_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()
tfidf\_matrix = vectorizer.fit\_transform(documents)

\# Prediction{-based (Word2Vec)}
from gensim.models import Word2Vec
model = Word2Vec(sentences, vector\_size=100, window=5)
\end{verbatim}

\textbf{Advantages:}

\textbf{Frequency-based:}

\begin{itemize}
\tightlist
\item
  \textbf{Simple} and interpretable
\item
  \textbf{Fast} computation for small datasets
\item
  \textbf{Good} for basic similarity tasks
\end{itemize}

\textbf{Prediction-based:}

\begin{itemize}
\tightlist
\item
  \textbf{Dense} vector representations
\item
  \textbf{Better} semantic relationships
\item
  \textbf{Scalable} to large vocabularies
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``FP - Frequency counts, Prediction learns''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{Question 4(a) [3 marks]}\label{q4a}

\textbf{List out the key characteristics of reactive machine.}

\begin{solutionbox}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Reactive Machine Key Characteristics}
\vspace{-10pt}
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Characteristic & Description \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Stateless} & No memory of past interactions \\
\textbf{Reactive} & Responds only to current inputs \\
\textbf{Deterministic} & Consistent outputs for same inputs \\
\textbf{Specialized} & Designed for specific tasks \\
\textbf{Real-time} & Immediate response to stimuli \\
\end{longtable}
}

\textbf{Examples:}

\begin{itemize}
\tightlist
\item
  \textbf{Deep Blue}: Chess-playing computer
\item
  \textbf{Google AlphaGo}: Go-playing system (early version)
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``SRDSR - Stateless, Reactive, Deterministic,
Specialized, Real-time''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{Question 4(b) [4 marks]}\label{q4b}

\textbf{List out various pre-processing techniques. Explain any one of
them with python code.}

\begin{solutionbox}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Text Pre-processing Techniques}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3793}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3103}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3103}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Technique
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Tokenization} & Split text into words & ``Hello world'' \rightarrow
[``Hello'', ``world''] \\
\textbf{Stop Word Removal} & Remove common words & Remove ``the'',
``and'', ``is'' \\
\textbf{Stemming} & Reduce words to root form & ``running'' \rightarrow ``run'' \\
\textbf{Lemmatization} & Convert to dictionary form & ``better'' \rightarrow
``good'' \\
\end{longtable}
}

\textbf{Stemming Explanation:} Stemming reduces words to their root form
by removing suffixes.

\textbf{Python Code for Stemming:}

\begin{verbatim}
import nltk
from nltk.stem import PorterStemmer

\# Initialize stemmer
stemmer = PorterStemmer()

\# Example words
words = ["running", "flies", "dogs", "churches", "studying"]

\# Apply stemming
stemmed\_words = [stemmer.stem(word) for word in words]
print(stemmed\_words)
\# Output: [{run, fli, dog, church, studi]}
\end{verbatim}

\textbf{Benefits of Stemming:}

\begin{itemize}
\tightlist
\item
  \textbf{Reduces vocabulary size} for ML models\\
\item
  \textbf{Groups related words} together
\item
  \textbf{Improves} text analysis efficiency
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``TSSL - Tokenize, Stop-words, Stem, Lemmatize''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{Question 4(c) [7 marks]}\label{q4c}

\textbf{Illuminate the Word2vec technique in detail.}

\begin{solutionbox}

\textbf{Word2Vec} is a neural network-based technique that learns dense
vector representations of words by predicting context.


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Word2Vec Architectures}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3421}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2632}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1842}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2105}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Architecture
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Approach
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Input
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Output
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Skip-gram} & Predict context from center word & Center word &
Context words \\
\textbf{CBOW} & Predict center word from context & Context words &
Center word \\
\end{longtable}
}

\textbf{Skip-gram Model:}

\begin{center}
\textbf{Mermaid Diagram (Code)}
\begin{verbatim}
{Shaded}
{Highlighting}[]
graph LR
    A[Input: Center Word] {-{-}{} B[Hidden Layer]}
    B {-{-}{} C[Output: Context Words]}
    C {-{-}{} D[Softmax Layer]}
    D {-{-}{} E[Probability Distribution]}
{Highlighting}
{Shaded}
\end{verbatim}
\end{center}

\textbf{Training Process:}

\begin{enumerate}
\tightlist
\item
  \textbf{Sliding Window}: Move window across text
\item
  \textbf{Word Pairs}: Create (center, context) pairs\\
\item
  \textbf{Neural Network}: Train to predict context
\item
  \textbf{Weight Matrix}: Extract word vectors
\end{enumerate}

\textbf{Key Features:}

\begin{itemize}
\tightlist
\item
  \textbf{Vector Size}: Typically 100-300 dimensions
\item
  \textbf{Window Size}: Context range (usually 5-10 words)
\item
  \textbf{Negative Sampling}: Efficient training method
\item
  \textbf{Hierarchical Softmax}: Alternative to softmax
\end{itemize}

\textbf{Mathematical Concept:}

\begin{verbatim}
Objective = max Σ log P(context|center)
Where P(context|center) = exp(v_context · v_center) / Σ exp(v_w · v_center)
\end{verbatim}

\textbf{Applications:}

\begin{itemize}
\tightlist
\item
  \textbf{Similarity}: Find similar words
\item
  \textbf{Analogies}: King - Man + Woman = Queen
\item
  \textbf{Clustering}: Group semantic categories
\item
  \textbf{Feature Engineering}: ML input features
\end{itemize}

\textbf{Advantages:}

\begin{itemize}
\tightlist
\item
  \textbf{Dense Representations}: Rich semantic information
\item
  \textbf{Semantic Relationships}: Captures word meanings
\item
  \textbf{Arithmetic Properties}: Vector operations make sense
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``W2V - Words to Vectors via neural networks''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{Question 4(a) OR [3
marks]}\label{q4a}

\textbf{List out any four applications of Natural Language Processing.
Explain spam detection in detail.}

\begin{solutionbox}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{NLP Applications}
\vspace{-10pt}
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Application & Description \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Spam Detection} & Identify unwanted emails \\
\textbf{Sentiment Analysis} & Determine emotional tone \\
\textbf{Machine Translation} & Translate between languages \\
\textbf{Chatbots} & Automated conversation systems \\
\end{longtable}
}

\textbf{Spam Detection Details:}

\textbf{Process:}

\begin{enumerate}
\tightlist
\item
  \textbf{Feature Extraction}: Convert email text to numerical features
\item
  \textbf{Classification}: Use ML algorithms to classify
\item
  \textbf{Decision}: Mark as spam or legitimate
\end{enumerate}

\textbf{Features Used:}

\begin{itemize}
\tightlist
\item
  \textbf{Word Frequency}: Spam keywords count
\item
  \textbf{Email Headers}: Sender information
\item
  \textbf{URL Analysis}: Suspicious links
\item
  \textbf{Text Patterns}: ALL CAPS, excessive punctuation
\end{itemize}

\textbf{Machine Learning Approach:}

\begin{verbatim}
\# Simplified spam detection
from sklearn.feature\_extraction.text import TfidfVectorizer
from sklearn.naive\_bayes import MultinomialNB

\# Convert emails to features
vectorizer = TfidfVectorizer()
X = vectorizer.fit\_transform(email\_texts)

\# Train classifier
classifier = MultinomialNB()
classifier.fit(X, labels)  \# labels: 0=legitimate, 1=spam
\end{verbatim}

\end{solutionbox}
\begin{mnemonicbox}
``SMTP - Spam, Machine Translation, Sentiment,
Phishing detection''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{Question 4(b) OR [4
marks]}\label{q4b}

\textbf{Explain about discourse integration and pragmatic analysis.}

\begin{solutionbox}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Discourse Integration vs Pragmatic Analysis}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4375}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3958}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Discourse Integration
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Pragmatic Analysis
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Focus} & Text coherence and structure & Context and intention \\
\textbf{Scope} & Multiple sentences/paragraphs & Speaker's intended
meaning \\
\textbf{Elements} & Anaphora, cataphora, connectives & Implicature,
speech acts \\
\textbf{Goal} & Understand text flow & Understand real meaning \\
\end{longtable}
}

\textbf{Discourse Integration:}

\begin{itemize}
\tightlist
\item
  \textbf{Anaphora Resolution}: ``John went to store. He bought milk.''
  (He = John)
\item
  \textbf{Cataphora}: ``Before he left, John locked the door.''
\item
  \textbf{Coherence}: Logical flow between sentences
\item
  \textbf{Cohesion}: Grammatical connections
\end{itemize}

\textbf{Pragmatic Analysis:}

\begin{itemize}
\tightlist
\item
  \textbf{Speech Acts}: Commands, requests, promises
\item
  \textbf{Implicature}: Implied meanings beyond literal
\item
  \textbf{Context Dependency}: Same words, different meanings
\item
  \textbf{Intention Recognition}: What speaker really means
\end{itemize}

\textbf{Examples:}

\textbf{Discourse Integration:}

\begin{verbatim}
Text: "Mary owns a car. The vehicle is red."
Resolution: "vehicle" refers to "car"
\end{verbatim}

\textbf{Pragmatic Analysis:}

\begin{verbatim}
Statement: "Can you pass the salt?"
Literal: Question about ability
Pragmatic: Request to pass salt
\end{verbatim}

\end{solutionbox}
\begin{mnemonicbox}
``DP - Discourse connects, Pragmatics interprets
context''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{Question 4(c) OR [7
marks]}\label{q4c}

\textbf{Discuss about the Bag of Words word embedding technique in
detail.}

\begin{solutionbox}

\textbf{Bag of Words (BoW)} is a simple text representation method that
treats documents as unordered collections of words.


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{BoW Process}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2143}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4643}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3214}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Step
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Vocabulary Creation} & Collect all unique words & [``cat'',
``sat'', ``mat'', ``dog''] \\
\textbf{Vector Creation} & Count word occurrences & [1, 1, 1, 0] for
``cat sat mat'' \\
\textbf{Document Representation} & Each document becomes a vector &
Multiple documents \rightarrow Matrix \\
\end{longtable}
}

\textbf{Example:}

\begin{verbatim}
Documents:
1. "The cat sat on the mat"
2. "The dog ran in the park"

Vocabulary: [the, cat, sat, on, mat, dog, ran, in, park]

Document Vectors:
Doc1: [2, 1, 1, 1, 1, 0, 0, 0, 0]
Doc2: [2, 0, 0, 0, 0, 1, 1, 1, 1]
\end{verbatim}

\textbf{Python Implementation:}

\begin{verbatim}
from sklearn.feature\_extraction.text import CountVectorizer

documents = [
    "The cat sat on the mat",
    "The dog ran in the park"
]

vectorizer = CountVectorizer()
bow\_matrix = vectorizer.fit\_transform(documents)
vocab = vectorizer.get\_feature\_names\_out()

print("Vocabulary:", vocab)
print("BoW Matrix:", bow\_matrix.toarray())
\end{verbatim}

\textbf{Advantages:}

\begin{itemize}
\tightlist
\item
  \textbf{Simplicity}: Easy to understand and implement
\item
  \textbf{Interpretability}: Clear word-count relationship
\item
  \textbf{Effectiveness}: Works well for many tasks
\end{itemize}

\textbf{Disadvantages:}

\begin{itemize}
\tightlist
\item
  \textbf{No Word Order}: ``cat sat mat'' = ``mat sat cat''
\item
  \textbf{Sparse Vectors}: Many zeros in large vocabularies
\item
  \textbf{No Semantics}: No understanding of word meanings
\item
  \textbf{High Dimensionality}: Scales with vocabulary size
\end{itemize}

\textbf{Variations:}

\begin{itemize}
\tightlist
\item
  \textbf{Binary BoW}: 1 if word present, 0 if absent
\item
  \textbf{TF-IDF BoW}: Term frequency \times Inverse document frequency
\item
  \textbf{N-gram BoW}: Consider word sequences
\end{itemize}

\textbf{Applications:}

\begin{itemize}
\tightlist
\item
  \textbf{Document Classification}: Spam detection
\item
  \textbf{Information Retrieval}: Search engines
\item
  \textbf{Text Clustering}: Group similar documents
\item
  \textbf{Feature Engineering}: Input for ML models
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``BOW - Bag Of Words counts occurrences''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{Question 5(a) [3 marks]}\label{q5a}

\textbf{What is the role of activation functions in Neural Network?}

\begin{solutionbox}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Activation Function Roles}
\vspace{-10pt}
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Role & Description \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Non-linearity} & Enables learning complex patterns \\
\textbf{Output Control} & Determines neuron firing threshold \\
\textbf{Gradient Flow} & Affects backpropagation efficiency \\
\textbf{Range Limiting} & Bounds output values \\
\end{longtable}
}

\textbf{Key Functions:}

\begin{itemize}
\tightlist
\item
  \textbf{Decision Making}: Whether neuron should activate
\item
  \textbf{Pattern Recognition}: Enables complex decision boundaries\\
\item
  \textbf{Signal Processing}: Transforms weighted inputs
\end{itemize}

\textbf{Common Activation Functions:}

\begin{itemize}
\tightlist
\item
  \textbf{ReLU}: f(x) = max(0, x) - Simple and efficient
\item
  \textbf{Sigmoid}: f(x) = 1/(1 + e\^{}-x) - Smooth probability output
\item
  \textbf{Tanh}: f(x) = (e\^{}x - e\textsuperscript{-x)/(e}x + e\^{}-x)
  - Zero-centered
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``NOGL - Non-linearity, Output control, Gradient
flow, Limiting range''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{Question 5(b) [4 marks]}\label{q5b}

\textbf{Describe architecture of Neural Network in detail.}

\begin{solutionbox}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Neural Network Architecture Components}
\vspace{-10pt}
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Component & Function & Example \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Input Layer} & Receives input data & Features/pixels \\
\textbf{Hidden Layers} & Process information & Pattern recognition \\
\textbf{Output Layer} & Produces final result &
Classification/prediction \\
\textbf{Connections} & Link neurons between layers & Weighted edges \\
\end{longtable}
}

\textbf{Architecture Diagram:}

\begin{center}
\textbf{Mermaid Diagram (Code)}
\begin{verbatim}
{Shaded}
{Highlighting}[]
graph LR
    A[Input Layer] {-{-}{} B[Hidden Layer 1]}
    B {-{-}{} C[Hidden Layer 2]  }
    C {-{-}{} D[Output Layer]}
    
    A1[X1] {-{-}{} B1[H1]}
    A2[X2] {-{-}{} B1}
    A1 {-{-}{} B2[H2]}
    A2 {-{-}{} B2}
    
    B1 {-{-}{} D1[Y1]}
    B2 {-{-}{} D1}
{Highlighting}
{Shaded}
\end{verbatim}
\end{center}

\textbf{Layer Details:}

\begin{itemize}
\tightlist
\item
  \textbf{Input Layer}: Number of neurons = number of features
\item
  \textbf{Hidden Layers}: Variable neurons, multiple layers for
  complexity
\item
  \textbf{Output Layer}: Number of neurons = number of classes/outputs
\end{itemize}

\textbf{Information Flow:}

\begin{enumerate}
\tightlist
\item
  \textbf{Forward Pass}: Input \rightarrow Hidden \rightarrow Output
\item
  \textbf{Weighted Sum}: Σ(wi \times xi + bias)\\
\item
  \textbf{Activation}: Apply activation function
\item
  \textbf{Output}: Final prediction/classification
\end{enumerate}

\end{solutionbox}
\begin{mnemonicbox}
``IHOC - Input, Hidden, Output, Connections''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{Question 5(c) [7 marks]}\label{q5c}

\textbf{List out and explain types of ambiguities in Natural Language
Processing.}

\begin{solutionbox}

\textbf{Ambiguity} in NLP occurs when text has multiple possible
interpretations, making automatic understanding challenging.


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Types of NLP Ambiguities}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1579}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3158}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2368}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2895}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Resolution
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Lexical} & Word has multiple meanings & ``Bank''
(river/financial) & Context analysis \\
\textbf{Syntactic} & Multiple parse structures & ``I saw her duck'' &
Grammar rules \\
\textbf{Semantic} & Multiple sentence meanings & ``Visiting relatives
can be boring'' & Semantic analysis \\
\textbf{Pragmatic} & Context-dependent meaning & ``Can you pass salt?''
& Intent recognition \\
\textbf{Referential} & Unclear pronoun reference & ``John told Bill he
was late'' & Anaphora resolution \\
\end{longtable}
}

\textbf{Detailed Explanations:}

\textbf{Lexical Ambiguity:}

\begin{itemize}
\tightlist
\item
  \textbf{Homonyms}: Same spelling, different meanings
\item
  Example: ``I went to the bank'' (financial institution vs.~river bank)
\item
  \textbf{Solution}: Word sense disambiguation using context
\end{itemize}

\textbf{Syntactic Ambiguity:}

\begin{itemize}
\tightlist
\item
  \textbf{Multiple Parse Trees}: Same sentence, different structures
\item
  Example: ``I saw the man with the telescope''

  \begin{itemize}
  \tightlist
  \item
    I used telescope to see man
  \item
    I saw man who had telescope
  \end{itemize}
\item
  \textbf{Solution}: Statistical parsing, grammar preferences
\end{itemize}

\textbf{Semantic Ambiguity:}

\begin{itemize}
\tightlist
\item
  \textbf{Multiple Interpretations}: Same structure, different meanings
\item
  Example: ``Visiting relatives can be boring''

  \begin{itemize}
  \tightlist
  \item
    Going to visit relatives is boring
  \item
    Relatives who visit are boring
  \end{itemize}
\item
  \textbf{Solution}: Semantic role labeling
\end{itemize}

\textbf{Pragmatic Ambiguity:}

\begin{itemize}
\tightlist
\item
  \textbf{Context-dependent}: Meaning depends on situation
\item
  Example: ``It's cold here'' (statement vs.~request to close window)
\item
  \textbf{Solution}: Dialogue systems, context modeling
\end{itemize}

\textbf{Referential Ambiguity:}

\begin{itemize}
\tightlist
\item
  \textbf{Unclear References}: Pronouns with multiple possible
  antecedents
\item
  Example: ``John told Bill that he was promoted'' (who got promoted?)
\item
  \textbf{Solution}: Coreference resolution algorithms
\end{itemize}

\textbf{Resolution Strategies:}

\begin{center}
\textbf{Mermaid Diagram (Code)}
\begin{verbatim}
{Shaded}
{Highlighting}[]
graph LR
    A[Ambiguous Text] {-{-}{} B[Context Analysis]}
    A {-{-}{} C[Statistical Models]}
    A {-{-}{} D[Knowledge Bases]}
    B {-{-}{} E[Disambiguation]}
    C {-{-}{} E}
    D {-{-}{} E}
    E {-{-}{} F[Clear Interpretation]}
{Highlighting}
{Shaded}
\end{verbatim}
\end{center}

\textbf{Impact on NLP Systems:}

\begin{itemize}
\tightlist
\item
  \textbf{Machine Translation}: Wrong word choices
\item
  \textbf{Information Retrieval}: Irrelevant results
\item
  \textbf{Question Answering}: Incorrect responses
\item
  \textbf{Chatbots}: Misunderstood queries
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``LSSPR - Lexical, Syntactic, Semantic, Pragmatic,
Referential''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{Question 5(a) OR [3
marks]}\label{q5a}

\textbf{List down the names of some popular activation functions used in
Neural Network.}

\begin{solutionbox}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Popular Activation Functions}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3030}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2727}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2121}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2121}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Function
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formula
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Range
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Usage
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{ReLU} & f(x) = max(0, x) & [0, \infty) & Hidden layers \\
\textbf{Sigmoid} & f(x) = 1/(1 + e\^{}-x) & (0, 1) & Binary
classification \\
\textbf{Tanh} & f(x) = (e\^{}x - e\textsuperscript{-x)/(e}x + e\^{}-x) &
(-1, 1) & Hidden layers \\
\textbf{Softmax} & f(xi) = e\^{}xi / Σe\^{}xj & (0, 1) & Multi-class
output \\
\textbf{Leaky ReLU} & f(x) = max(0.01x, x) & (-\infty, \infty) & Solving dead
neurons \\
\end{longtable}
}

\textbf{Popular Functions:}

\begin{itemize}
\tightlist
\item
  \textbf{ReLU}: Most commonly used in hidden layers
\item
  \textbf{Sigmoid}: Traditional choice for binary problems
\item
  \textbf{Tanh}: Zero-centered alternative to sigmoid
\item
  \textbf{Softmax}: Standard for multi-class classification
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``RSTSL - ReLU, Sigmoid, Tanh, Softmax, Leaky ReLU''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{Question 5(b) OR [4
marks]}\label{q5b}

\textbf{Explain Learning process in artificial Neural Network.}

\begin{solutionbox}

\textbf{Learning Process} in neural networks involves adjusting weights
and biases to minimize error through iterative training.


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{Learning Process Steps}
\vspace{-10pt}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2143}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3214}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4643}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Step
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Process
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Initialize} & Random weights & Start with small random values \\
\textbf{Forward Pass} & Calculate output & Propagate input through
network \\
\textbf{Calculate Error} & Compare with target & Use loss function \\
\textbf{Backward Pass} & Calculate gradients & Use backpropagation \\
\textbf{Update Weights} & Adjust parameters & Apply gradient descent \\
\textbf{Repeat} & Iterate process & Until convergence \\
\end{longtable}
}

\textbf{Learning Algorithm Flow:}

\begin{center}
\textbf{Mermaid Diagram (Code)}
\begin{verbatim}
{Shaded}
{Highlighting}[]
graph LR
    A[Initialize Weights] {-{-}{} B[Forward Pass]}
    B {-{-}{} C[Calculate Loss]}
    C {-{-}{} D[Backward Pass]}
    D {-{-}{} E[Update Weights]}
    E {-{-}{} F\{Converged?\}}
    F {-{-}{}|No| B}
    F {-{-}{}|Yes| G[Training Complete]}
{Highlighting}
{Shaded}
\end{verbatim}
\end{center}

\textbf{Mathematical Foundation:}

\begin{itemize}
\tightlist
\item
  \textbf{Loss Function}: L = ½(target - output)^{2}
\item
  \textbf{Gradient}: \partialL/\partialw = error \times input
\item
  \textbf{Weight Update}: w\_new = w\_old - η \times gradient
\item
  \textbf{Learning Rate}: η controls update step size
\end{itemize}

\textbf{Types of Learning:}

\begin{itemize}
\tightlist
\item
  \textbf{Supervised}: Learn from labeled examples
\item
  \textbf{Batch Learning}: Update after all samples
\item
  \textbf{Online Learning}: Update after each sample
\item
  \textbf{Mini-batch}: Update after small batches
\end{itemize}

\textbf{Key Concepts:}

\begin{itemize}
\tightlist
\item
  \textbf{Epoch}: One complete pass through training data
\item
  \textbf{Convergence}: When error stops decreasing
\item
  \textbf{Overfitting}: Memorizing training data
\item
  \textbf{Regularization}: Techniques to prevent overfitting
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``IFCBU - Initialize, Forward, Calculate, Backward,
Update''

\end{mnemonicbox}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{Question 5(c) OR [7
marks]}\label{q5c}

\textbf{List out various advantages and disadvantages of Natural
Language Processing.}

\begin{solutionbox}


{\def\LTcaptype{none} % do not increment counter
\vspace{-5pt}
\captionof{table}{NLP Advantages and Disadvantages}
\vspace{-10pt}
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Advantages & Disadvantages \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Automated Text Analysis} & \textbf{Ambiguity Handling} \\
\textbf{Language Translation} & \textbf{Context Understanding} \\
\textbf{Human-Computer Interaction} & \textbf{Cultural Nuances} \\
\textbf{Information Extraction} & \textbf{Computational Complexity} \\
\textbf{Sentiment Analysis} & \textbf{Data Requirements} \\
\end{longtable}
}

\textbf{Detailed Advantages:}

\textbf{Business Benefits:}

\begin{itemize}
\tightlist
\item
  \textbf{Customer Service}: Automated chatbots and support
\item
  \textbf{Content Analysis}: Social media monitoring
\item
  \textbf{Document Processing}: Automated summarization
\item
  \textbf{Search Enhancement}: Better information retrieval
\end{itemize}

\textbf{Technical Advantages:}

\begin{itemize}
\tightlist
\item
  \textbf{Scalability}: Process large text volumes
\item
  \textbf{Consistency}: Uniform analysis across documents
\item
  \textbf{Speed}: Faster than human text processing
\item
  \textbf{Integration}: Works with existing systems
\end{itemize}

\textbf{Detailed Disadvantages:}

\textbf{Technical Challenges:}

\begin{itemize}
\tightlist
\item
  \textbf{Ambiguity}: Multiple interpretations of text
\item
  \textbf{Context Dependency}: Meaning changes with situation
\item
  \textbf{Sarcasm/Irony}: Difficult to detect automatically
\item
  \textbf{Domain Specificity}: Models need retraining for new domains
\end{itemize}

\textbf{Resource Requirements:}

\begin{itemize}
\tightlist
\item
  \textbf{Large Datasets}: Need millions of text samples
\item
  \textbf{Computational Power}: Complex models require GPUs
\item
  \textbf{Expert Knowledge}: Requires linguistics and ML expertise
\item
  \textbf{Maintenance}: Models need regular updates
\end{itemize}

\textbf{Quality Issues:}

\begin{itemize}
\tightlist
\item
  \textbf{Accuracy Limitations}: Not 100\% accurate
\item
  \textbf{Bias Problems}: Reflects training data biases
\item
  \textbf{Language Barriers}: Works better for some languages
\item
  \textbf{Error Propagation}: Mistakes compound in pipelines
\end{itemize}

\textbf{Applications vs Challenges:}

\begin{center}
\textbf{Mermaid Diagram (Code)}
\begin{verbatim}
{Shaded}
{Highlighting}[]
graph TD
    A[NLP Applications] {-{-}{} B[Machine Translation]}
    A {-{-}{} C[Sentiment Analysis]  }
    A {-{-}{} D[Information Extraction]}
    
    E[NLP Challenges] {-{-}{} F[Ambiguity]}
    E {-{-}{} G[Context Understanding]}
    E {-{-}{} H[Cultural Nuances]}
{Highlighting}
{Shaded}
\end{verbatim}
\end{center}

\textbf{Future Improvements:}

\begin{itemize}
\tightlist
\item
  \textbf{Better Context Models}: Transformer architectures
\item
  \textbf{Multilingual Support}: Cross-language understanding
\item
  \textbf{Few-shot Learning}: Less data requirements
\item
  \textbf{Explainable AI}: Understanding model decisions
\end{itemize}

\end{solutionbox}
\begin{mnemonicbox}
``ALICE vs ACHDR - Automated, Language, Interaction,
Content, Extraction vs Ambiguity, Context, Human-nuances, Data,
Resources''

\end{mnemonicbox}

\end{document}
